{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0196ca2b",
   "metadata": {},
   "source": [
    "# Atividade: Preparação e Análise de Dados para Redes Neurais\n",
    "\n",
    "Esta atividade consiste em gerar conjuntos de dados sintéticos, investigar sua separabilidade e preparar um conjunto de dados real para uso em redes neurais. O objetivo é praticar técnicas de geração, visualização e pré‑processamento de dados, com foco na criação de entradas adequadas para modelos com funções de ativação como `tanh`.\n",
    "\n",
    "As instruções e parâmetros utilizados aqui seguem o enunciado da disciplina. Optou‑se pelo uso de um número reduzido de bibliotecas: apenas `numpy`, `pandas` e `matplotlib`, suficientes para manipular dados numéricos, visualizar gráficos e ler arquivos CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e08b783",
   "metadata": {},
   "source": [
    "## Como executar este notebook\n",
    "\n",
    "Para rodar este notebook, recomenda‑se criar um ambiente virtual e instalar apenas as bibliotecas necessárias:\n",
    "\n",
    "```bash\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # Windows: .venv\\Scripts\u0007ctivate\n",
    "pip install numpy pandas matplotlib\n",
    "```\n",
    "\n",
    "Em seguida, coloque este arquivo (`ex1_pedrotpc_pt.ipynb`) em uma pasta de trabalho e, se desejar executar o Exercício 3, baixe o arquivo `train.csv` do conjunto **Spaceship Titanic** (Kaggle) e salve na mesma pasta.\n",
    "\n",
    "Abra o Jupyter Notebook com `jupyter notebook` ou `jupyter lab` e execute as células na ordem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b2daf",
   "metadata": {},
   "source": [
    "## Exercício 1 – Explorar a separabilidade das classes em 2D\n",
    "\n",
    "Neste exercício vamos gerar um conjunto de dados bidimensional com quatro classes distintas. Cada classe será modelada por uma distribuição normal (gaussiana) com média e desvio padrão definidos pelo enunciado:\n",
    "\n",
    "- **Classe 0**: média = [2, 3], desvio padrão = [0,8, 2,5]\n",
    "- **Classe 1**: média = [5, 6], desvio padrão = [1,2, 1,9]\n",
    "- **Classe 2**: média = [8, 1], desvio padrão = [0,9, 0,9]\n",
    "- **Classe 3**: média = [15, 4], desvio padrão = [0,5, 2,0]\n",
    "\n",
    "Geraremos 100 amostras por classe, totalizando 400 pontos. O objetivo é visualizar os pontos em um gráfico de dispersão (scatter plot) e discutir visualmente se um classificador linear seria capaz de separar todas as classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício 1 – Gerar e visualizar o conjunto 2D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Semente para reprodutibilidade\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definição das médias e desvios para cada classe\n",
    "medias = np.array([[2, 3], [5, 6], [8, 1], [15, 4]], dtype=float)\n",
    "desvios = np.array([[0.8, 2.5], [1.2, 1.9], [0.9, 0.9], [0.5, 2.0]], dtype=float)\n",
    "\n",
    "# Geração das amostras\n",
    "n_por_classe = 100\n",
    "X_parts = []\n",
    "y_parts = []\n",
    "for i, (mu, sigma) in enumerate(zip(medias, desvios)):\n",
    "    # cada dimensão tem seu próprio desvio; multiplicamos pelo desvio e somamos a média\n",
    "    pts = np.random.randn(n_por_classe, 2) * sigma + mu\n",
    "    labels = np.full(n_por_classe, i)\n",
    "    X_parts.append(pts)\n",
    "    y_parts.append(labels)\n",
    "\n",
    "# Concatenar todas as classes\n",
    "X1 = np.vstack(X_parts)\n",
    "y1 = np.concatenate(y_parts)\n",
    "\n",
    "print(f\"Formato do conjunto: {X1.shape}, rótulos: {y1.shape}\")\n",
    "\n",
    "# Plotar os pontos\n",
    "cores = ['red', 'green', 'blue', 'orange']\n",
    "plt.figure(figsize=(6, 5))\n",
    "for i in range(4):\n",
    "    plt.scatter(X1[y1 == i, 0], X1[y1 == i, 1], color=cores[i], label=f\"Classe {i}\", alpha=0.6)\n",
    "\n",
    "plt.xlabel('x₁')\n",
    "plt.ylabel('x₂')\n",
    "plt.title('Exercício 1: classes gaussianas em 2D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf0adc",
   "metadata": {},
   "source": [
    "### Análise do Exercício 1\n",
    "\n",
    "O gráfico mostra quatro agrupamentos distintos de pontos. Nota‑se que as três primeiras classes (0, 1 e 2) ficam relativamente próximas, com sobreposição parcial entre classes 0 e 1 e entre classes 1 e 2. A classe 3 está afastada, à direita.\n",
    "\n",
    "Devido a essa sobreposição entre as classes próximas, um classificador linear (como um perceptron simples) não seria capaz de traçar uma única linha reta que separe todas as classes ao mesmo tempo. Em vez disso, seriam necessárias regiões de decisão curvas ou múltiplas fronteiras. Redes neurais com funções de ativação não lineares podem aprender essas fronteiras complexas, ajustando‑se ao contorno dos agrupamentos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e129a",
   "metadata": {},
   "source": [
    "## Exercício 2 – Não linearidade em dimensões mais altas\n",
    "\n",
    "Aqui vamos estudar um caso em que os dados são originalmente de cinco dimensões (5D) e não são linearmente separáveis. Geraremos dois grupos:\n",
    "\n",
    "- **Classe A**: 500 amostras de uma normal multivariada com média \\(\\mu_A = [0, 0, 0, 0, 0]\\) e matriz de covariância \\(\\Sigma_A\\) definida pelo enunciado:\n",
    "\n",
    "\n",
    "\\(\\Sigma_A = \begin{pmatrix}\n",
    "1{,}0 & 0{,}8 & 0{,}1 & 0{,}0 & 0{,}0\\\n",
    "0{,}8 & 1{,}0 & 0{,}3 & 0{,}0 & 0{,}0\\\n",
    "0{,}1 & 0{,}3 & 1{,}0 & 0{,}5 & 0{,}0\\\n",
    "0{,}0 & 0{,}0 & 0{,}5 & 1{,}0 & 0{,}2\\\n",
    "0{,}0 & 0{,}0 & 0{,}0 & 0{,}2 & 1{,}0\n",
    "\\end{pmatrix}\\)\n",
    "\n",
    "- **Classe B**: 500 amostras com média \\(\\mu_B = [1{,}5, 1{,}5, 1{,}5, 1{,}5, 1{,}5]\\) e matriz de covariância\n",
    "\n",
    "\\(\\Sigma_B = \begin{pmatrix}\n",
    "1{,}5 & -0{,}7 & 0{,}2 & 0{,}0 & 0{,}0\\\n",
    "-0{,}7 & 1{,}5 & 0{,}4 & 0{,}0 & 0{,}0\\\n",
    "0{,}2 & 0{,}4 & 1{,}5 & 0{,}6 & 0{,}0\\\n",
    "0{,}0 & 0{,}0 & 0{,}6 & 1{,}5 & 0{,}3\\\n",
    "0{,}0 & 0{,}0 & 0{,}0 & 0{,}3 & 1{,}5\n",
    "\\end{pmatrix}\\)\n",
    "\n",
    "Após gerar os dados, aplicaremos **Análise de Componentes Principais (PCA)** para reduzir de 5D para 2D e, assim, visualizar os dados em um scatter plot. Para evitar dependência de bibliotecas externas, a PCA é implementada manualmente: centralizamos os dados, calculamos a matriz de covariância, extraímos os autovalores e autovetores com `numpy.linalg.eigh` e projetamos nos dois autovetores correspondentes aos maiores autovalores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13ef475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício 2 – Gerar dados 5D e reduzir para 2D com PCA manual\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Definir médias e covariâncias\n",
    "mu_A = np.array([0, 0, 0, 0, 0], dtype=float)\n",
    "Sigma_A = np.array([[1.0, 0.8, 0.1, 0.0, 0.0],\n",
    "                    [0.8, 1.0, 0.3, 0.0, 0.0],\n",
    "                    [0.1, 0.3, 1.0, 0.5, 0.0],\n",
    "                    [0.0, 0.0, 0.5, 1.0, 0.2],\n",
    "                    [0.0, 0.0, 0.0, 0.2, 1.0]], dtype=float)\n",
    "\n",
    "mu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5], dtype=float)\n",
    "Sigma_B = np.array([[1.5, -0.7, 0.2, 0.0, 0.0],\n",
    "                    [-0.7, 1.5, 0.4, 0.0, 0.0],\n",
    "                    [0.2, 0.4, 1.5, 0.6, 0.0],\n",
    "                    [0.0, 0.0, 0.6, 1.5, 0.3],\n",
    "                    [0.0, 0.0, 0.0, 0.3, 1.5]], dtype=float)\n",
    "\n",
    "# Número de amostras por classe\n",
    "n_amostras = 500\n",
    "\n",
    "# Gerar amostras\n",
    "A = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=n_amostras)\n",
    "B = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=n_amostras)\n",
    "\n",
    "# Concatenação e rótulos\n",
    "X = np.vstack([A, B])\n",
    "y = np.array([0]*n_amostras + [1]*n_amostras)\n",
    "\n",
    "print(f\"Formato dos dados: {X.shape}\")\n",
    "\n",
    "# PCA manual\n",
    "# 1. Centralizar as variáveis\n",
    "X_c = X - X.mean(axis=0)\n",
    "\n",
    "# 2. Matriz de covariância\n",
    "cov_mat = np.cov(X_c, rowvar=False)\n",
    "\n",
    "# 3. Autovalores e autovetores\n",
    "eigvals, eigvecs = np.linalg.eigh(cov_mat)\n",
    "\n",
    "# 4. Ordenar em ordem decrescente\n",
    "idx = np.argsort(eigvals)[::-1]\n",
    "eigvals_sorted = eigvals[idx]\n",
    "eigvecs_sorted = eigvecs[:, idx]\n",
    "\n",
    "# 5. Selecionar dois maiores\n",
    "W = eigvecs_sorted[:, :2]  # matriz 5x2\n",
    "\n",
    "# 6. Projetar\n",
    "X_proj = X_c @ W\n",
    "\n",
    "# Mostrar variância explicada\n",
    "var_total = eigvals.sum()\n",
    "exp_var = eigvals_sorted[:2] / var_total\n",
    "print(f\"Variância explicada pelos dois componentes: {exp_var}\")\n",
    "\n",
    "# Plotar projeção\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_proj[y == 0, 0], X_proj[y == 0, 1], s=12, alpha=0.6, color='blue', label='Classe A')\n",
    "plt.scatter(X_proj[y == 1, 0], X_proj[y == 1, 1], s=12, alpha=0.6, color='red', label='Classe B')\n",
    "plt.xlabel('Componente 1')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.title('Exercício 2: Projeção PCA (manual) de 5D para 2D')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49581ab1",
   "metadata": {},
   "source": [
    "### Análise do Exercício 2\n",
    "\n",
    "A projeção em duas dimensões evidencia que as classes A e B apresentam sobreposição significativa. Mesmo reduzidas para 2D, as amostras se intercalam em várias regiões do gráfico, sugerindo que a distribuição original em 5D não é linearmente separável. A variância explicada pelos dois maiores componentes principais mostra quanto da informação total é preservada na projeção.\n",
    "\n",
    "Modelos lineares simples não conseguem separar dados tão entrelaçados; técnicas mais sofisticadas — como redes neurais com camadas ocultas e funções de ativação não lineares — são capazes de aprender fronteiras complexas e capturar dependências não lineares entre variáveis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1ae27",
   "metadata": {},
   "source": [
    "## Exercício 3 – Preparar o conjunto de dados Spaceship Titanic\n",
    "\n",
    "Utilizaremos o dataset **Spaceship Titanic**, disponível na plataforma Kaggle, para praticar pré‑processamento de dados reais. O conjunto é dividido em `train.csv` (registros de aproximadamente dois terços dos passageiros) e `test.csv` (restante). A coluna alvo `Transported` indica se o passageiro foi transportado para outra dimensão (valor booleano).\n",
    "\n",
    "### Principais campos em `train.csv`\n",
    "\n",
    "- **PassengerId** – identificador único do passageiro (formato `gggg_pp`, em que `gggg` identifica o grupo e `pp` a posição dentro do grupo).\n",
    "- **HomePlanet** – planeta de origem do passageiro.\n",
    "- **CryoSleep** – se o passageiro optou por animação suspensa durante a viagem.\n",
    "- **Cabin** – cabine do passageiro (`deck/num/side`).\n",
    "- **Destination** – planeta de destino.\n",
    "- **Age** – idade.\n",
    "- **VIP** – se pagou por serviço VIP.\n",
    "- **RoomService, FoodCourt, ShoppingMall, Spa, VRDeck** – gastos em diferentes amenidades do navio.\n",
    "- **Name** – nome completo.\n",
    "- **Transported** – variável alvo (True/False).\n",
    "\n",
    "### Tarefas de pré‑processamento\n",
    "\n",
    "1. **Ler o arquivo** `train.csv` usando `pandas`.\n",
    "2. **Explorar dados**: exibir as primeiras linhas (`head()`), e contar valores ausentes por coluna.\n",
    "3. **Tratar valores ausentes**:\n",
    "   - Para colunas numéricas (`Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`), substituir `NaN` pela mediana da coluna.\n",
    "   - Para colunas categóricas (`HomePlanet`, `CryoSleep`, `Cabin`, `Destination`, `VIP`), substituir `NaN` por `'Unknown'`.\n",
    "4. **Codificar categorias**: aplicar *one‑hot encoding* com `pandas.get_dummies`.\n",
    "5. **Normalizar atributos numéricos**: ajustar para média 0 e desvio 1, calculando `(valor - média)/desvio`.\n",
    "6. **Visualizar**: plotar histogramas de uma ou duas colunas numéricas (por exemplo, `Age` e `FoodCourt`) antes e depois da normalização para perceber o efeito da escala.\n",
    "\n",
    "O código a seguir executa essas etapas e imprime estatísticas e gráficos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício 3 – Pré‑processar o dataset Spaceship Titanic (se disponível)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tentar ler o arquivo train.csv\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print(f\"Dataset carregado com sucesso. Formato: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print('Arquivo train.csv não encontrado. Coloque o arquivo na mesma pasta deste notebook para executar o pré‑processamento.')\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Exibir cabeçalho\n",
    "    display(df.head())\n",
    "    print('\n",
    "Informações gerais:')\n",
    "    print(df.info())\n",
    "\n",
    "    # Definir colunas numéricas e categóricas\n",
    "    col_numericas = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    col_categoricas = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n",
    "\n",
    "    # Contar valores ausentes\n",
    "    print('\n",
    "Valores ausentes por coluna:')\n",
    "    print(df.isna().sum()[df.isna().sum() > 0])\n",
    "\n",
    "    # Plotar histogramas antes da normalização\n",
    "    for col in ['Age', 'FoodCourt']:\n",
    "        if col in df.columns:\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            df[col].dropna().hist(bins=40, edgecolor='k')\n",
    "            plt.title(f'{col} antes do tratamento')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequência')\n",
    "            plt.show()\n",
    "\n",
    "    # Copiar dados para processamento\n",
    "    df_proc = df.copy()\n",
    "\n",
    "    # Tratar valores ausentes nas numéricas (mediana)\n",
    "    for c in col_numericas:\n",
    "        if c in df_proc.columns:\n",
    "            mediana = df_proc[c].median()\n",
    "            df_proc[c] = df_proc[c].fillna(mediana)\n",
    "\n",
    "    # Tratar valores ausentes nas categóricas (Unknown)\n",
    "    for c in col_categoricas:\n",
    "        if c in df_proc.columns:\n",
    "            df_proc[c] = df_proc[c].fillna('Unknown')\n",
    "\n",
    "    # One-hot encoding\n",
    "    df_proc = pd.get_dummies(df_proc, columns=[c for c in col_categoricas if c in df_proc.columns], drop_first=False)\n",
    "\n",
    "    # Remover identificadores se presentes\n",
    "    for col in ['PassengerId', 'Name']:\n",
    "        if col in df_proc.columns:\n",
    "            df_proc = df_proc.drop(columns=col)\n",
    "\n",
    "    # Normalizar colunas numéricas manualmente: (valor - média)/desvio\n",
    "    for c in col_numericas:\n",
    "        if c in df_proc.columns:\n",
    "            media = df_proc[c].mean()\n",
    "            desvio = df_proc[c].std()\n",
    "            if desvio != 0:\n",
    "                df_proc[c] = (df_proc[c] - media) / desvio\n",
    "            else:\n",
    "                df_proc[c] = 0\n",
    "\n",
    "    # Plotar histogramas após a normalização\n",
    "    for col in ['Age', 'FoodCourt']:\n",
    "        if col in df_proc.columns:\n",
    "            plt.figure(figsize=(5, 3))\n",
    "            df_proc[col].hist(bins=40, edgecolor='k')\n",
    "            plt.title(f'{col} após normalização')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequência')\n",
    "            plt.show()\n",
    "\n",
    "    print('\n",
    "Pré‑processamento concluído. Formato final:', df_proc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7fa96",
   "metadata": {},
   "source": [
    "## Resumo e próximos passos\n",
    "\n",
    "Neste notebook trabalhamos três tarefas principais:\n",
    "\n",
    "- **Exercício 1:** geramos quatro conjuntos de pontos gaussianos em 2D, visualizamos sua distribuição e concluímos que há sobreposição significativa entre algumas classes, exigindo fronteiras de decisão não lineares para classificá‑las adequadamente.\n",
    "\n",
    "- **Exercício 2:** construímos duas classes de dados em 5D e realizamos uma redução de dimensionalidade usando PCA implementada manualmente. A projeção mostrou que as classes não são linearmente separáveis, o que reforça a necessidade de modelos não lineares.\n",
    "\n",
    "- **Exercício 3:** revisamos os campos do dataset Spaceship Titanic, implementamos um pipeline básico de pré‑processamento com tratamento de valores ausentes, codificação de variáveis categóricas e normalização manual de atributos numéricos. Também geramos histogramas para visualizar a distribuição antes e depois da normalização.\n",
    "\n",
    "### Próximos passos\n",
    "\n",
    "- Você pode ajustar os parâmetros das distribuições no Exercício 1 para testar diferentes configurações de separabilidade.\n",
    "- No Exercício 2, experimente projetar em mais componentes ou utilizar outras técnicas de redução (como t‑SNE) para observar o comportamento visual.\n",
    "- Para o Exercício 3, considere dividir os dados em conjuntos de treino/validação e treinar um modelo simples (por exemplo, regressão logística) para predizer a coluna `Transported`, sempre respeitando as regras de uso de dados do Kaggle.\n",
    "\n",
    "Para publicar este relatório como página estática, siga o passo a passo descrito anteriormente (converter o notebook para HTML com `nbconvert`, comitar em um repositório GitHub e habilitar o GitHub Pages).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
