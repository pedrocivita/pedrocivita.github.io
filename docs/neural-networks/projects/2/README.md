# Projeto de RegressÃ£o: PrevisÃ£o de Demanda de Bicicletas Compartilhadas

**Disciplina:** Redes Neurais e Deep Learning
**Projeto:** Regression Task com Multi-Layer Perceptron (MLP)
**Deadline:** 19.out.2025

---

## ğŸ“‹ DescriÃ§Ã£o

Este projeto implementa uma rede neural **Multi-Layer Perceptron (MLP) do zero** usando apenas **NumPy** para resolver um problema de regressÃ£o real-world: prever a demanda de bicicletas compartilhadas com base em condiÃ§Ãµes climÃ¡ticas, temporais e sazonais.

## ğŸ¯ Objetivos

- Implementar MLP completo do zero (forward/backward propagation)
- Aplicar tÃ©cnicas de feature engineering para dados temporais
- Treinar modelo com regularizaÃ§Ã£o L2 e early stopping
- Avaliar performance com mÃºltiplas mÃ©tricas de regressÃ£o
- Analisar resultados e comparar com baseline

## ğŸ“Š Dataset

**Bike Sharing Demand Dataset** do UCI Machine Learning Repository

- **Fonte:** [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)
- **Alternativa:** [Kaggle Competition](https://www.kaggle.com/c/bike-sharing-demand)
- **Tamanho:** 17,379 amostras (registros horÃ¡rios de 2011-2012)
- **Features:** 16 variÃ¡veis (temporais, climÃ¡ticas, sazonais)
- **Target:** Contagem total de aluguÃ©is de bicicletas (variÃ¡vel contÃ­nua)

### Por que este dataset?

âœ… RelevÃ¢ncia prÃ¡tica (sistemas de bike-sharing em cidades inteligentes)
âœ… Complexidade adequada (relaÃ§Ãµes nÃ£o-lineares, sazonalidade)
âœ… Evita datasets clÃ¡ssicos (Boston/California Housing)
âœ… Possibilidade de participaÃ§Ã£o em competiÃ§Ã£o Kaggle (bonus)

## ğŸš€ Como Executar

### PrÃ©-requisitos

```bash
# Bibliotecas necessÃ¡rias
pip install numpy pandas matplotlib seaborn jupyter
```

### Passo 1: Download do Dataset

**OpÃ§Ã£o 1: UCI Repository (Recomendado)**

1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
2. Baixe `Bike-Sharing-Dataset.zip`
3. Extraia e copie o arquivo `hour.csv` para esta pasta

**OpÃ§Ã£o 2: Kaggle API**

```bash
# Instale o Kaggle CLI
pip install kaggle

# Configure suas credenciais Kaggle
# Depois execute:
kaggle competitions download -c bike-sharing-demand
```

### Passo 2: Executar o Notebook

```bash
# Inicie o Jupyter Notebook
jupyter notebook regression.ipynb

# Ou use Jupyter Lab
jupyter lab regression.ipynb
```

### Passo 3: Executar CÃ©lulas

Execute as cÃ©lulas sequencialmente de cima para baixo. O notebook estÃ¡ organizado em seÃ§Ãµes numeradas.

## ğŸ“ Estrutura do Projeto

```
regression.ipynb          # Notebook principal (49 cÃ©lulas)
README.md                 # Este arquivo
hour.csv                  # Dataset (baixar separadamente)
create_regression_notebook.py   # Script auxiliar (geraÃ§Ã£o do notebook)
add_remaining_cells.py    # Script auxiliar
add_mlp_and_training.py   # Script auxiliar
add_final_sections.py     # Script auxiliar
add_conclusion.py         # Script auxiliar
```

## ğŸ“– ConteÃºdo do Notebook

### SeÃ§Ãµes Principais

1. **SeleÃ§Ã£o e ExplicaÃ§Ã£o do Dataset**
   - Justificativa da escolha
   - DescriÃ§Ã£o detalhada das features
   - Conhecimento de domÃ­nio

2. **AnÃ¡lise ExploratÃ³ria**
   - EstatÃ­sticas descritivas
   - DetecÃ§Ã£o de valores ausentes e outliers
   - VisualizaÃ§Ãµes (distribuiÃ§Ãµes, correlaÃ§Ãµes, padrÃµes temporais)

3. **PrÃ©-processamento**
   - Limpeza de dados
   - Feature engineering (variÃ¡veis cÃ­clicas com sin/cos)
   - NormalizaÃ§Ã£o (z-score standardization)
   - DivisÃ£o Train/Val/Test (70/15/15)

4. **ImplementaÃ§Ã£o do MLP**
   - Arquitetura: [15 â†’ 64 â†’ 32 â†’ 16 â†’ 1]
   - Forward propagation
   - Backpropagation
   - He initialization
   - ReLU activation
   - MSE loss + L2 regularization

5. **Treinamento**
   - Mini-batch gradient descent (batch=64)
   - Early stopping (patience=15)
   - Curvas de convergÃªncia
   - AnÃ¡lise de overfitting/underfitting

6. **AvaliaÃ§Ã£o**
   - MÃ©tricas: MAE, MSE, RMSE, RÂ², MAPE
   - ComparaÃ§Ã£o com baseline (mÃ©dia)
   - VisualizaÃ§Ãµes (predito vs real, resÃ­duos)
   - AnÃ¡lise estatÃ­stica dos erros

7. **DiscussÃ£o e ConclusÃ£o**
   - Pontos fortes e limitaÃ§Ãµes
   - Melhorias futuras
   - Aprendizados principais

## ğŸ”§ HiperparÃ¢metros

| ParÃ¢metro | Valor | Justificativa |
|-----------|-------|---------------|
| **Arquitetura** | [15 â†’ 64 â†’ 32 â†’ 16 â†’ 1] | ReduÃ§Ã£o progressiva para hierarquia de features |
| **Learning Rate** | 0.001 | ConvergÃªncia estÃ¡vel sem oscilaÃ§Ãµes |
| **Batch Size** | 64 | EquilÃ­brio entre velocidade e estabilidade |
| **Ã‰pocas (mÃ¡x)** | 200 | Suficiente para convergÃªncia |
| **Early Stopping** | 15 Ã©pocas | Previne overfitting |
| **L2 Lambda** | 0.001 | RegularizaÃ§Ã£o moderada |

## ğŸ“Š MÃ©tricas de AvaliaÃ§Ã£o

- **MAE (Mean Absolute Error)**: Erro mÃ©dio em nÃºmero de aluguÃ©is
- **MSE (Mean Squared Error)**: Penaliza erros grandes
- **RMSE (Root MSE)**: Erro na mesma unidade do target
- **RÂ² (Coef. DeterminaÃ§Ã£o)**: VariÃ¢ncia explicada pelo modelo
- **MAPE (Mean Absolute % Error)**: Erro percentual mÃ©dio

## ğŸ“ Conceitos Implementados

### Redes Neurais
- [x] Forward propagation
- [x] Backpropagation
- [x] Gradient descent (mini-batch)
- [x] He initialization
- [x] ReLU activation
- [x] Linear output (regressÃ£o)

### RegularizaÃ§Ã£o
- [x] L2 regularization (Ridge)
- [x] Early stopping
- [x] Train/val/test split

### Feature Engineering
- [x] TransformaÃ§Ã£o cÃ­clica (sin/cos)
- [x] NormalizaÃ§Ã£o (z-score)
- [x] RemoÃ§Ã£o de features irrelevantes

### AvaliaÃ§Ã£o
- [x] MÃºltiplas mÃ©tricas de regressÃ£o
- [x] AnÃ¡lise de resÃ­duos
- [x] ComparaÃ§Ã£o com baseline
- [x] VisualizaÃ§Ãµes de performance

## âš ï¸ Requisitos do Projeto

âœ… Dataset real com >1,000 amostras e >5 features
âœ… Evita datasets clÃ¡ssicos (Boston/California Housing)
âœ… MLP implementado do zero (NumPy apenas)
âœ… Todas as etapas explicadas e justificadas
âœ… Train/val/test split adequado
âœ… Early stopping implementado
âœ… MÃºltiplas mÃ©tricas de avaliaÃ§Ã£o
âœ… AnÃ¡lise de curvas de erro
âœ… ComparaÃ§Ã£o com baseline
âœ… ConclusÃ£o e limitaÃ§Ãµes discutidas
âœ… ReferÃªncias citadas

## ğŸ“š ReferÃªncias Principais

1. **Dataset:** Fanaee-T & Gama (2013) - UCI ML Repository
2. **Deep Learning:** Goodfellow, Bengio & Courville (2016)
3. **He Initialization:** He et al. (2015) - ICCV
4. **Material do Curso:**
   - https://caioboa.github.io/DeepLearningPages/
   - https://insper.github.io/ann-dl/versions/2025.2/projects/regression/

## ğŸ¤ Ferramentas de IA Utilizadas

- **Claude Code (Anthropic)**: AssistÃªncia na estruturaÃ§Ã£o do cÃ³digo e documentaÃ§Ã£o
- Todas as implementaÃ§Ãµes foram **compreendidas e validadas manualmente**

## ğŸ† Oportunidade de Bonus

Este dataset estÃ¡ disponÃ­vel em **competiÃ§Ã£o no Kaggle**:
- SubmissÃ£o vÃ¡lida: +0.5 pontos
- Top 50% do leaderboard: +0.5 pontos adicionais

Link: https://www.kaggle.com/c/bike-sharing-demand

## ğŸ“ Notas Importantes

1. **ExecuÃ§Ã£o Sequencial**: Execute as cÃ©lulas na ordem (top-down)
2. **Tempo de Treinamento**: ~2-5 minutos (depende do hardware)
3. **Reprodutibilidade**: Seed=42 garante mesmos resultados
4. **Dados Normalizados**: PrediÃ§Ãµes sÃ£o desnormalizadas para interpretaÃ§Ã£o
5. **VisualizaÃ§Ãµes**: Requerem matplotlib e seaborn

## ğŸ” PrÃ³ximos Passos (Melhorias Futuras)

1. **Otimizadores avanÃ§ados**: Adam, RMSprop
2. **Arquiteturas**: LSTM/GRU para capturar dependÃªncia temporal
3. **RegularizaÃ§Ã£o adicional**: Dropout, Batch Normalization
4. **Features**: Lags temporais, interaÃ§Ãµes
5. **Ensemble**: Combinar MLP com XGBoost/Random Forest
6. **Hyperparameter tuning**: Grid search ou Bayesian optimization
7. **Deployment**: API com FastAPI/Flask

## ğŸ“§ Contato

Para dÃºvidas sobre o projeto, consulte:
- Material do curso
- Office hours do professor
- DocumentaÃ§Ã£o oficial das bibliotecas

---

**Data de criaÃ§Ã£o:** Outubro 2025
**VersÃ£o:** 1.0
**Status:** âœ… Completo e pronto para execuÃ§Ã£o
