# üöÄ Guia R√°pido de In√≠cio

**Projeto de Regress√£o - Bike Sharing Demand**

---

## ‚ö° In√≠cio R√°pido (3 passos)

### 1Ô∏è‚É£ Instalar Depend√™ncias

```bash
pip install numpy pandas matplotlib seaborn jupyter
```

### 2Ô∏è‚É£ Baixar Dataset

**Op√ß√£o A - Autom√°tico (recomendado):**
```bash
python download_dataset.py
```

**Op√ß√£o B - Manual:**
1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset
2. Baixe `Bike-Sharing-Dataset.zip`
3. Extraia e copie `hour.csv` para esta pasta

### 3Ô∏è‚É£ Executar Notebook

```bash
jupyter notebook regression.ipynb
```

---

## üìä O que esperar

### Estrutura do Notebook (49 c√©lulas)

```
üìå Se√ß√µes Principais:
‚îú‚îÄ‚îÄ 1-4:   Introdu√ß√£o e Dataset
‚îú‚îÄ‚îÄ 5-9:   Explora√ß√£o de Dados
‚îú‚îÄ‚îÄ 10-14: Pr√©-processamento
‚îú‚îÄ‚îÄ 15-20: Implementa√ß√£o MLP
‚îú‚îÄ‚îÄ 21-30: Treinamento
‚îú‚îÄ‚îÄ 31-40: Avalia√ß√£o
‚îî‚îÄ‚îÄ 41-49: An√°lise e Conclus√£o
```

### Tempo Estimado

- ‚è±Ô∏è **Execu√ß√£o completa:** ~5-10 minutos
- ‚è±Ô∏è **Treinamento do modelo:** ~2-3 minutos
- ‚è±Ô∏è **Leitura e an√°lise:** ~30-45 minutos

---

## üéØ Checklist de Execu√ß√£o

### Antes de Executar

- [ ] Python 3.8+ instalado
- [ ] Bibliotecas instaladas (numpy, pandas, matplotlib, seaborn)
- [ ] Arquivo `hour.csv` na pasta do notebook
- [ ] Jupyter Notebook funcionando

### Durante a Execu√ß√£o

- [ ] Executar c√©lulas sequencialmente (n√£o pular c√©lulas)
- [ ] Verificar se dataset foi carregado (c√©lula 4)
- [ ] Aguardar treinamento completo (~100-150 √©pocas)
- [ ] Verificar converg√™ncia nas curvas de loss
- [ ] Analisar m√©tricas finais (R¬≤, RMSE, MAE)

### Ap√≥s Execu√ß√£o

- [ ] R¬≤ > 0.3 no test set (m√≠nimo aceit√°vel)
- [ ] Gap train-val < 20% (sem overfitting severo)
- [ ] Res√≠duos centrados em zero
- [ ] Visualiza√ß√µes geradas corretamente

---

## üîß Hiperpar√¢metros Principais

| Par√¢metro | Valor Padr√£o | Onde Modificar |
|-----------|--------------|----------------|
| Learning Rate | 0.001 | C√©lula 30 (`LEARNING_RATE`) |
| Batch Size | 64 | C√©lula 30 (`BATCH_SIZE`) |
| √âpocas M√°x | 200 | C√©lula 30 (`EPOCHS`) |
| Arquitetura | [64,32,16] | C√©lula 30 (`HIDDEN_SIZES`) |
| L2 Lambda | 0.001 | C√©lula 30 (`REG_LAMBDA`) |

**üí° Dica:** Para experimentar, modifique os valores e re-execute a c√©lula 30 em diante.

---

## üìà Resultados Esperados

### M√©tricas T√≠picas (ap√≥s treinamento)

| M√©trica | Valor Esperado | Interpreta√ß√£o |
|---------|----------------|---------------|
| **R¬≤** | 0.40 - 0.70 | Quanto da vari√¢ncia √© explicada |
| **RMSE** | 60 - 100 | Erro m√©dio em n¬∫ de bikes |
| **MAE** | 40 - 70 | Erro absoluto m√©dio |
| **MAPE** | 20% - 40% | Erro percentual |

### Curva de Converg√™ncia

```
‚úì Esperado:
  - Loss decrescente nas primeiras 50 √©pocas
  - Plateau ou oscila√ß√£o m√≠nima depois
  - Train e Val loss pr√≥ximos (gap < 20%)

‚ùå Problemas:
  - Loss crescente: learning rate muito alto
  - N√£o converge: learning rate muito baixo
  - Val >> Train: overfitting
```

---

## üêõ Troubleshooting

### Erro: "File 'hour.csv' not found"
```bash
# Solu√ß√£o:
python download_dataset.py
```

### Erro: "ModuleNotFoundError: No module named 'numpy'"
```bash
# Solu√ß√£o:
pip install numpy pandas matplotlib seaborn
```

### Erro: "Kernel died"
```bash
# Poss√≠veis causas:
# 1. Mem√≥ria insuficiente
# 2. Conflito de vers√µes

# Solu√ß√£o:
# Reinicie o kernel: Kernel ‚Üí Restart
# Se persistir, reduza BATCH_SIZE para 32
```

### Warning: "RuntimeWarning: overflow encountered"
```bash
# Causa: Learning rate muito alto ou pesos explodindo
# Solu√ß√£o:
# 1. Reduza LEARNING_RATE para 0.0005
# 2. Aumente REG_LAMBDA para 0.01
# 3. Re-execute o treinamento
```

### Loss n√£o converge (fica oscilando)
```bash
# Solu√ß√µes:
# 1. Reduza LEARNING_RATE (ex: 0.0005)
# 2. Aumente BATCH_SIZE (ex: 128)
# 3. Aumente L2 regularization (REG_LAMBDA = 0.01)
```

### Overfitting (Val >> Train loss)
```bash
# Solu√ß√µes:
# 1. Aumente REG_LAMBDA (ex: 0.01)
# 2. Reduza arquitetura (ex: [32, 16, 8])
# 3. Reduza EARLY_STOPPING_PATIENCE (ex: 10)
```

---

## üìö Se√ß√µes Importantes

### **C√©lulas Essenciais (n√£o pule!)**

- **C√©lula 3:** Carregamento do dataset
- **C√©lula 7:** An√°lise de outliers
- **C√©lula 14:** Normaliza√ß√£o (salva y_mean e y_std)
- **C√©lula 16:** Split train/val/test
- **C√©lulas 18-20:** Implementa√ß√£o MLP (classe completa)
- **C√©lula 21:** Treinamento (pode demorar 2-3 min)
- **C√©lula 28:** Desnormaliza√ß√£o (cr√≠tico para m√©tricas corretas)

### **C√©lulas Opcionais (podem pular se tiver pressa)**

- C√©lulas de visualiza√ß√£o explorat√≥ria (5, 6, 8, 9)
- An√°lise detalhada de res√≠duos (37, 38)
- Exemplos de predi√ß√µes (39)

---

## üí° Dicas de Otimiza√ß√£o

### Para Resultados Melhores

1. **Feature Engineering:**
   - J√° implementado: transforma√ß√£o c√≠clica (sin/cos)
   - Experimente: adicionar lags temporais, intera√ß√µes

2. **Arquitetura:**
   - Teste diferentes profundidades: [128,64,32], [32,16]
   - Cuidado com underfitting (muito simples) ou overfitting (muito complexo)

3. **Hiperpar√¢metros:**
   - Learning rate: teste [0.0001, 0.0005, 0.001, 0.005]
   - Batch size: teste [32, 64, 128, 256]
   - L2 lambda: teste [0.0001, 0.001, 0.01]

4. **Treinamento:**
   - Aumente EPOCHS para 300-500 se converg√™ncia lenta
   - EARLY_STOPPING_PATIENCE pode ir at√© 20-30

### Para Execu√ß√£o Mais R√°pida

1. Reduza EPOCHS para 50-100
2. Aumente BATCH_SIZE para 128
3. Reduza arquitetura: [32, 16]
4. Pule c√©lulas de visualiza√ß√£o

---

## üéì Conceitos Aprendidos

Ao final deste projeto, voc√™ ter√° implementado:

- ‚úÖ Forward propagation
- ‚úÖ Backpropagation
- ‚úÖ Gradient descent (mini-batch)
- ‚úÖ He initialization
- ‚úÖ ReLU activation
- ‚úÖ L2 regularization
- ‚úÖ Early stopping
- ‚úÖ Feature engineering (sin/cos)
- ‚úÖ M√©tricas de regress√£o
- ‚úÖ An√°lise de res√≠duos

---

## üìû Suporte

### Documenta√ß√£o Completa

Leia [README.md](README.md) para documenta√ß√£o detalhada.

### Material de Apoio

- https://caioboa.github.io/DeepLearningPages/
- https://insper.github.io/ann-dl/versions/2025.2/projects/regression/

### D√∫vidas Comuns

Consulte a se√ß√£o **Discuss√£o e An√°lise** (c√©lula 44) do notebook.

---

## üéâ Pr√≥ximos Passos

Ap√≥s executar com sucesso:

1. **An√°lise:** Interprete os resultados nas c√©lulas 40-45
2. **Experimenta√ß√£o:** Modifique hiperpar√¢metros e re-treine
3. **Compara√ß√£o:** Compare com baseline (c√©lula 32)
4. **Melhorias:** Implemente sugest√µes da se√ß√£o "Melhorias Futuras"
5. **Bonus:** Submeta para competi√ß√£o Kaggle (+0.5 pts)

---

**Boa sorte! üöÄ**

Se encontrar problemas, revise o README ou consulte o material do curso.
