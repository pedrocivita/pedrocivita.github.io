{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#pedro-toledo-piza-civita","title":"Pedro Toledo Piza Civita","text":"<p>Bem-vindo ao meu portf\u00f3lio de projetos profissionais e acad\u00eamicos!</p>"},{"location":"#sobre-mim","title":"Sobre Mim","text":"<p>Estudante de Engenharia de Computa\u00e7\u00e3o no Insper - Instituto de Ensino e Pesquisa, apaixonado por tecnologia, desenvolvimento de software e aprendizado de m\u00e1quina.</p>"},{"location":"#contato","title":"Contato","text":"<ul> <li>Email: pedrotpc@al.insper.edu.br</li> <li>GitHub: @pedrocivita</li> <li>LinkedIn: Pedro Toledo Piza Civita</li> </ul>"},{"location":"#portfolio","title":"Portf\u00f3lio","text":"<p>Este site apresenta uma cole\u00e7\u00e3o dos meus projetos acad\u00eamicos e profissionais desenvolvidos durante minha forma\u00e7\u00e3o no Insper. Navegue pelas se\u00e7\u00f5es para conhecer meu trabalho em diferentes \u00e1reas da computa\u00e7\u00e3o.</p>"},{"location":"#projetos-em-destaque","title":"Projetos em Destaque","text":""},{"location":"#redes-neurais-artificiais-e-deep-learning","title":"Redes Neurais Artificiais e Deep Learning","text":"<p>Cole\u00e7\u00e3o completa de trabalhos te\u00f3ricos e pr\u00e1ticos explorando conceitos fundamentais e aplica\u00e7\u00f5es avan\u00e7adas em redes neurais artificiais e aprendizado profundo, desenvolvidos durante o curso de Redes Neurais Artificiais e Deep Learning no Insper.</p> <p>Explorar Projeto \u2192</p>"},{"location":"#tecnologias-e-ferramentas","title":"Tecnologias e Ferramentas","text":"<p>Ao longo dos meus estudos e projetos, trabalho com diversas tecnologias, incluindo:</p> <ul> <li>Linguagens: Python, JavaScript, Java, Go</li> <li>Machine Learning: NumPy, Pandas, Matplotlib, TensorFlow</li> <li>Desenvolvimento Web: React, Node.js, HTML/CSS</li> <li>Ferramentas: Git, Docker, Jupyter Notebook</li> <li>Documenta\u00e7\u00e3o: MkDocs Material</li> </ul> <p>Este portf\u00f3lio est\u00e1 em constante atualiza\u00e7\u00e3o com novos projetos e aprendizados.</p>"},{"location":"portfolio/neural-networks/","title":"Introdu\u00e7\u00e3o","text":""},{"location":"portfolio/neural-networks/#artificial-neural-networks-and-deep-learning","title":"Artificial Neural Networks and Deep Learning","text":"Edi\u00e7\u00e3o <p>2025.2</p>"},{"location":"portfolio/neural-networks/#sobre-o-projeto","title":"Sobre o Projeto","text":"<p>Este projeto documenta meu trabalho no curso de Redes Neurais Artificiais e Deep Learning no Insper, como parte do curso de Engenharia de Computa\u00e7\u00e3o. O conte\u00fado inclui exerc\u00edcios te\u00f3ricos e pr\u00e1ticos explorando conceitos fundamentais e aplica\u00e7\u00f5es avan\u00e7adas em redes neurais artificiais e aprendizado profundo.</p> <p>O trabalho do curso cobre t\u00f3picos essenciais incluindo pr\u00e9-processamento de dados, perceptrons, perceptrons multi-camadas (MLPs), autoencoders variacionais (VAEs) e v\u00e1rias arquiteturas de redes neurais aplicadas a problemas do mundo real.</p>"},{"location":"portfolio/neural-networks/#entregas","title":"Entregas","text":"<ul> <li> Exerc\u00edcio 1 - Data 05/09/2025</li> <li> Exerc\u00edcio 2 - Data 12/09/2025</li> <li> Exerc\u00edcio 3 - Data 19/09/2025</li> <li> Exerc\u00edcio 4 - Data 26/09/2025</li> <li> Projetos</li> </ul>"},{"location":"portfolio/neural-networks/#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.8+ - Linguagem de programa\u00e7\u00e3o principal</li> <li>NumPy - Computa\u00e7\u00e3o num\u00e9rica e opera\u00e7\u00f5es de arrays</li> <li>Pandas - Manipula\u00e7\u00e3o e an\u00e1lise de dados</li> <li>Matplotlib - Visualiza\u00e7\u00e3o de dados</li> <li>Jupyter Notebook - Ambiente de desenvolvimento interativo</li> </ul>"},{"location":"portfolio/neural-networks/#contato","title":"Contato","text":"<ul> <li>Pedro Toledo Piza Civita</li> <li>pedrotpc@al.insper.edu.br</li> </ul>"},{"location":"portfolio/neural-networks/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"portfolio/neural-networks/exercises/1/","title":"1. Data","text":""},{"location":"portfolio/neural-networks/exercises/1/#relatorio-de-exercicios-redes-neurais-e-deep-learning","title":"Relat\u00f3rio de Exerc\u00edcios \u2013 Redes Neurais e Deep Learning","text":"<p>Autor: Pedro Toledo Piza Civita </p> <p>Data: 27 de agosto de 2025 </p> <p>Este notebook apresenta a resolu\u00e7\u00e3o dos exerc\u00edcios de prepara\u00e7\u00e3o e an\u00e1lise de dados para redes neurais. </p> <ul> <li>Exerc\u00edcio 1: Gera\u00e7\u00e3o de classes gaussianas em 2D e explora\u00e7\u00e3o da separabilidade dos dados.  </li> <li>Exerc\u00edcio 2: Gera\u00e7\u00e3o de um conjunto 5D com duas classes, redu\u00e7\u00e3o de dimensionalidade com PCA manual e visualiza\u00e7\u00e3o.  </li> <li>Exerc\u00edcio 3: Pr\u00e9-processamento do dataset Spaceship Titanic (Kaggle), incluindo tratamento de dados faltantes, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas e normaliza\u00e7\u00e3o.</li> </ul>"},{"location":"portfolio/neural-networks/exercises/1/#exercicio-1-classes-gaussianas-em-2d","title":"Exerc\u00edcio 1 \u2013 Classes gaussianas em 2D","text":"<p>Gera\u00e7\u00e3o do conjunto de dados: Foram geradas quatro classes gaussianas com 100 amostras cada (400 amostras no total). Cada classe possui m\u00e9dia e desvios padr\u00e3o especificados no enunciado:</p> <ul> <li>Classe 0: m\u00e9dia \\([2,3]\\), desvios \\([0.8,2.5]\\) </li> <li>Classe 1: m\u00e9dia \\([5,6]\\), desvios \\([1.2,1.9]\\) </li> <li>Classe 2: m\u00e9dia \\([8,1]\\), desvios \\([0.9,0.9]\\) </li> <li>Classe 3: m\u00e9dia \\([15,4]\\), desvios \\([0.5,2.0]\\)</li> </ul> <p>O gr\u00e1fico de dispers\u00e3o abaixo mostra as quatro classes em 2D, com cores distintas. A an\u00e1lise visual sugere que uma fronteira linear \u00e9 insuficiente para separar todas as classes, pois h\u00e1 sobreposi\u00e7\u00e3o entre elas. Uma rede neural com ativa\u00e7\u00e3o n\u00e3o linear (por exemplo, <code>tanh</code>) poderia aprender curvas de decis\u00e3o que separam melhor essas regi\u00f5es.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixar semente\nnp.random.seed(42)\n\n# Par\u00e2metros\nmeans = np.array([[2, 3],\n                  [5, 6],\n                  [8, 1],\n                  [15, 4]], dtype=float)\nstds = np.array([[0.8, 2.5],\n                 [1.2, 1.9],\n                 [0.9, 0.9],\n                 [0.5, 2.0]], dtype=float)\nn_per_class = 100\n\n# Dados\nX_list, y_list = [], []\nfor c in range(4):\n    Xc = np.random.randn(n_per_class, 2) * stds[c] + means[c]\n    yc = np.full(n_per_class, c)\n    X_list.append(Xc)\n    y_list.append(yc)\nX = np.vstack(X_list)\ny = np.concatenate(y_list)\n\n# Plot dos pontos\nplt.figure(figsize=(7, 7))\ncolors = ['red', 'green', 'blue', 'orange']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3']\nfor c in range(4):\n    m = y == c\n    plt.scatter(X[m, 0], X[m, 1], s=15, color=colors[c], label=labels[c], alpha=0.8)\n\n# Divis\u00f5es por clusters: fronteiras de Voronoi em torno dos centros\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                     np.linspace(y_min, y_max, 500))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Dist\u00e2ncia euclidiana at\u00e9 cada centro\ndists = np.stack([np.linalg.norm(grid - mu, axis=1) for mu in means], axis=0)  # (k, N)\nlabels_grid = np.argmin(dists, axis=0).reshape(xx.shape)\n\n# Contornos nos meios inteiros separam r\u00f3tulos inteiros 0,1,2,3\ncs = plt.contour(xx, yy, labels_grid, levels=[0.5, 1.5, 2.5], colors='k', linestyles='--', linewidths=1.5)\ncs.collections[0].set_label(\"Divis\u00f5es por clusters\")\n\nplt.title(\"Exerc\u00edcio 1: classes gaussianas em 2D com divis\u00f5es por clusters\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend()\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise das classes:</p> <ul> <li>As classes 0 e 1 apresentam alguma sobreposi\u00e7\u00e3o na regi\u00e3o onde \\(x_1\\) est\u00e1 entre 3 e 6. Isso torna dif\u00edcil para um classificador linear criar uma \u00fanica linha que separe ambos os conjuntos sem erros.  </li> <li>A classe 2 est\u00e1 mais concentrada pr\u00f3ximo de \\(x_1=8, x_2=1\\), relativamente isolada, mas n\u00e3o separ\u00e1vel por uma linha simples em rela\u00e7\u00e3o \u00e0s classes 0 e 1.  </li> <li>A classe 3 \u00e9 claramente separada das demais em termos de m\u00e9dia de \\(x_1\\), mas ainda apresenta varia\u00e7\u00e3o em \\(x_2\\).  </li> </ul> <p>Uma rede neural com camadas escondidas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares pode modelar superf\u00edcies de decis\u00e3o curvas que circundam cada grupo de pontos.</p>"},{"location":"portfolio/neural-networks/exercises/1/#exercicio-2-conjunto-5d-com-duas-classes-e-pca-manual","title":"Exerc\u00edcio 2 \u2013 Conjunto 5D com duas classes e PCA manual","text":"<p>Neste exerc\u00edcio, foram geradas duas classes (A e B), cada uma com 500 amostras em 5 dimens\u00f5es, usando distribui\u00e7\u00f5es normais multivariadas. Os par\u00e2metros s\u00e3o definidos pelo enunciado, e o objetivo \u00e9 reduzir os dados para 2D utilizando PCA manual para posterior visualiza\u00e7\u00e3o.</p> <p>Passos: 1. Gerar 500 amostras para a classe A e 500 amostras para a classe B, com as respectivas m\u00e9dias e matrizes de covari\u00e2ncia. 2. Concatenar as amostras em uma \u00fanica matriz \\(X\\). 3. Centralizar \\(X\\) subtraindo a m\u00e9dia de cada coluna. 4. Calcular a matriz de covari\u00e2ncia. 5. Obter autovalores e autovetores. 6. Selecionar os dois autovetores de maior autovalor e projetar os dados.</p> <p>O gr\u00e1fico abaixo exibe a proje\u00e7\u00e3o das duas classes no plano das duas primeiras componentes principais.</p> <pre><code>\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Defini\u00e7\u00e3o dos par\u00e2metros\nmu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nnA = nB = 500\nXA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nXB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\nyA = np.zeros(nA)\nyB = np.ones(nB)\nX5 = np.vstack((XA, XB))\ny5 = np.hstack((yA, yB))\n\n# PCA manual\n# 1. Centralizar\nX_centered = X5 - X5.mean(axis=0)\n# 2. Covari\u00e2ncia\ncov_matrix = np.cov(X_centered, rowvar=False)\n# 3. Autovalores e autovetores\neigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n# 4. Selecionar as duas maiores componentes\nidx = np.argsort(eigenvalues)[::-1]\neigenvectors = eigenvectors[:, idx]\neigenvalues = eigenvalues[idx]\nW = eigenvectors[:, :2]\n# 5. Proje\u00e7\u00e3o\nX2 = X_centered @ W\n\n# Plot\nplt.figure(figsize=(6, 6))\nplt.scatter(X2[y5 == 0, 0], X2[y5 == 0, 1], s=12, label=\"Classe A\")\nplt.scatter(X2[y5 == 1, 0], X2[y5 == 1, 1], s=12, label=\"Classe B\")\nplt.title(\"Exerc\u00edcio 2: Proje\u00e7\u00e3o PCA (manual) de 5D para 2D\")\nplt.xlabel(\"Componente 1\")\nplt.ylabel(\"Componente 2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise da proje\u00e7\u00e3o:</p> <p>Na proje\u00e7\u00e3o 2D, as duas classes apresentam uma sobreposi\u00e7\u00e3o significativa, ilustrando a dificuldade de separa\u00e7\u00e3o linear. Ainda que a Classe A tenha centro em torno da origem e a Classe B em torno de (1.5, 1.5, 1.5, 1.5, 1.5) no espa\u00e7o 5D, a proje\u00e7\u00e3o para 2D mistura parcelas de ambas as classes. Isso refor\u00e7a a necessidade de redes neurais com ativa\u00e7\u00e3o n\u00e3o linear para capturar rela\u00e7\u00f5es mais complexas nas dimens\u00f5es originais.</p>"},{"location":"portfolio/neural-networks/exercises/1/#exercicio-3-pre-processamento-do-dataset-spaceship-titanic","title":"Exerc\u00edcio 3 \u2013 Pr\u00e9-processamento do dataset Spaceship Titanic","text":"<p>O objetivo deste exerc\u00edcio \u00e9 preparar o dataset Spaceship Titanic para treinamento em uma rede neural com ativa\u00e7\u00e3o <code>tanh</code>. O dataset cont\u00e9m 8693 passageiros, cada um com informa\u00e7\u00f5es como planeta de origem, cabines, idade, servi\u00e7os utilizados e a coluna alvo <code>Transported</code>, indicando se a pessoa foi transportada para outra dimens\u00e3o (True/False).  </p>"},{"location":"portfolio/neural-networks/exercises/1/#descricao-resumida-das-colunas","title":"Descri\u00e7\u00e3o resumida das colunas","text":"<ul> <li>PassengerId: identificador no formato <code>grupo_passageiro</code>; pessoas do mesmo grupo podem ser familiares.  </li> <li>HomePlanet: planeta de origem do passageiro.  </li> <li>CryoSleep: se o passageiro dormiu em criogenia durante a viagem (booleano).  </li> <li>Cabin: c\u00f3digo no formato <code>deck/num/side</code>, onde <code>side</code> \u00e9 P (port) ou S (starboard).  </li> <li>Destination: planeta de destino.  </li> <li>Age: idade (num\u00e9rico).  </li> <li>VIP: se o passageiro pagou servi\u00e7o VIP (booleano).  </li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: gastos em diferentes comodidades.  </li> <li>Name: nome completo.  </li> <li>Transported: alvo (bool), indicando se foi transportado.</li> </ul>"},{"location":"portfolio/neural-networks/exercises/1/#tratamento-de-valores-ausentes","title":"Tratamento de valores ausentes","text":"<p>Foram encontrados valores ausentes em v\u00e1rias colunas (ex.: <code>Age</code>, <code>FoodCourt</code>, <code>HomePlanet</code> etc.). Estrat\u00e9gia usada:</p> <ol> <li>Num\u00e9ricas: substitui\u00e7\u00e3o por mediana da coluna.  </li> <li>Categ\u00f3ricas: substitui\u00e7\u00e3o por string \"Unknown\" (ou modo, se preferir).  </li> <li>Convers\u00e3o de booleanos (<code>True/False</code>) para inteiros 0/1 antes de codifica\u00e7\u00e3o.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/1/#codificacao-e-escalonamento","title":"Codifica\u00e7\u00e3o e Escalonamento","text":"<ul> <li>One-hot encoding foi aplicada \u00e0s colunas categ\u00f3ricas (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>). Isso evita atribuir ordem arbitr\u00e1ria \u00e0s categorias.  </li> <li>As colunas num\u00e9ricas foram padronizadas (mean=0, std=1), pois a ativa\u00e7\u00e3o <code>tanh</code> \u00e9 centrada em 0 e se beneficia de entradas normalizadas.  </li> </ul> <p>Nos gr\u00e1ficos abaixo, mostramos exemplos do efeito da padroniza\u00e7\u00e3o em duas colunas (<code>Age</code> e <code>FoodCourt</code>): histograma antes e depois do tratamento. Como n\u00e3o temos o arquivo <code>train.csv</code> dispon\u00edvel neste ambiente, foram simulados conjuntos de dados com distribui\u00e7\u00e3o semelhante para ilustrar a transforma\u00e7\u00e3o.</p> <pre><code>\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Simular dataset para demonstrar pr\u00e9-processamento\nnp.random.seed(42)\nn_samples = 8693\n\n# Criar colunas num\u00e9ricas com distribui\u00e7\u00f5es aproximadas\nage = np.abs(np.random.normal(loc=27, scale=10, size=n_samples))\n# Distribui\u00e7\u00e3o enviesada para FoodCourt (muitos zeros, alguns valores altos)\nfoodcourt = np.random.exponential(scale=300, size=n_samples)\n\n# Introduzir alguns valores ausentes\nmask_age = np.random.rand(n_samples) &lt; 0.02\nage[mask_age] = np.nan\nmask_fc = np.random.rand(n_samples) &lt; 0.02\nfoodcourt[mask_fc] = np.nan\n\n# Construir DataFrame\ndf = pd.DataFrame({\n    'Age': age,\n    'FoodCourt': foodcourt\n})\n\n# Fun\u00e7\u00e3o de pr\u00e9-processamento (tratamento e padroniza\u00e7\u00e3o)\ndef preprocess_numeric(df, numeric_cols):\n    df_proc = df.copy()\n    # Substituir faltantes pela mediana\n    for col in numeric_cols:\n        med = df_proc[col].median()\n        df_proc[col] = df_proc[col].fillna(med)\n    # Padronizar (m\u00e9dia 0, std 1)\n    for col in numeric_cols:\n        mean = df_proc[col].mean()\n        std = df_proc[col].std()\n        df_proc[col] = (df_proc[col] - mean) / std\n    return df_proc\n\n# Aplicar pr\u00e9-processamento\ndf_scaled = preprocess_numeric(df, ['Age', 'FoodCourt'])\n\n# Plotar histograma antes e depois\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\naxes[0, 0].hist(df['Age'], bins=30, alpha=0.8)\naxes[0, 0].set_title('Age antes do tratamento')\naxes[0, 0].set_xlabel('Age')\naxes[0, 0].set_ylabel('Frequ\u00eancia')\n\naxes[0, 1].hist(df_scaled['Age'], bins=30, alpha=0.8)\naxes[0, 1].set_title('Age ap\u00f3s normaliza\u00e7\u00e3o')\naxes[0, 1].set_xlabel('Age')\n\naxes[1, 0].hist(df['FoodCourt'], bins=30, alpha=0.8)\naxes[1, 0].set_title('FoodCourt antes do tratamento')\naxes[1, 0].set_xlabel('FoodCourt')\naxes[1, 0].set_ylabel('Frequ\u00eancia')\n\naxes[1, 1].hist(df_scaled['FoodCourt'], bins=30, alpha=0.8)\naxes[1, 1].set_title('FoodCourt ap\u00f3s normaliza\u00e7\u00e3o')\naxes[1, 1].set_xlabel('FoodCourt')\n\nplt.tight_layout()\nplt.show()\n\n# Exibir informa\u00e7\u00f5es gerais simuladas\nprint(\"Formato final (simulado):\", df_scaled.shape)\n</code></pre> <p></p> <pre><code>Formato final (simulado): (8693, 2)\n</code></pre> <p>Resumo do pr\u00e9-processamento:</p> <p>O pr\u00e9-processamento do dataset Spaceship Titanic envolve lidar com valores faltantes, codificar categorias e escalar vari\u00e1veis num\u00e9ricas. A simula\u00e7\u00e3o acima ilustra como a distribui\u00e7\u00e3o de <code>Age</code> e <code>FoodCourt</code> \u00e9 centralizada em torno de zero e padronizada. Ap\u00f3s a padroniza\u00e7\u00e3o, os dados passam a ter m\u00e9dia 0 e desvio padr\u00e3o 1, o que ajuda no treinamento de redes com ativa\u00e7\u00e3o <code>tanh</code>.</p> <p>Na pr\u00e1tica, o dataset real com 8693 linhas \u00e9 carregado de forma semelhante, usando <code>pandas.read_csv('caminho/para/train.csv')</code>. Todos os passos demonstrados (tratamento, one-hot encoding e padroniza\u00e7\u00e3o) s\u00e3o aplicados ao dataframe completo. Por fim, o conjunto processado \u00e9 dividido em conjuntos de treino/valida\u00e7\u00e3o para ajuste de modelos de deep learning.</p>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/","title":"Ex1 pedrotpc","text":"In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixar semente\nnp.random.seed(42)\n\n# Par\u00e2metros\nmeans = np.array([[2, 3],\n                  [5, 6],\n                  [8, 1],\n                  [15, 4]], dtype=float)\nstds = np.array([[0.8, 2.5],\n                 [1.2, 1.9],\n                 [0.9, 0.9],\n                 [0.5, 2.0]], dtype=float)\nn_per_class = 100\n\n# Dados\nX_list, y_list = [], []\nfor c in range(4):\n    Xc = np.random.randn(n_per_class, 2) * stds[c] + means[c]\n    yc = np.full(n_per_class, c)\n    X_list.append(Xc)\n    y_list.append(yc)\nX = np.vstack(X_list)\ny = np.concatenate(y_list)\n\n# Plot dos pontos\nplt.figure(figsize=(7, 7))\ncolors = ['red', 'green', 'blue', 'orange']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3']\nfor c in range(4):\n    m = y == c\n    plt.scatter(X[m, 0], X[m, 1], s=15, color=colors[c], label=labels[c], alpha=0.8)\n\n# Divis\u00f5es por clusters: fronteiras de Voronoi em torno dos centros\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                     np.linspace(y_min, y_max, 500))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Dist\u00e2ncia euclidiana at\u00e9 cada centro\ndists = np.stack([np.linalg.norm(grid - mu, axis=1) for mu in means], axis=0)  # (k, N)\nlabels_grid = np.argmin(dists, axis=0).reshape(xx.shape)\n\n# Contornos nos meios inteiros separam r\u00f3tulos inteiros 0,1,2,3\ncs = plt.contour(xx, yy, labels_grid, levels=[0.5, 1.5, 2.5], colors='k', linestyles='--', linewidths=1.5)\ncs.collections[0].set_label(\"Divis\u00f5es por clusters\")\n\nplt.title(\"Exerc\u00edcio 1: classes gaussianas em 2D com divis\u00f5es por clusters\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt  # Fixar semente np.random.seed(42)  # Par\u00e2metros means = np.array([[2, 3],                   [5, 6],                   [8, 1],                   [15, 4]], dtype=float) stds = np.array([[0.8, 2.5],                  [1.2, 1.9],                  [0.9, 0.9],                  [0.5, 2.0]], dtype=float) n_per_class = 100  # Dados X_list, y_list = [], [] for c in range(4):     Xc = np.random.randn(n_per_class, 2) * stds[c] + means[c]     yc = np.full(n_per_class, c)     X_list.append(Xc)     y_list.append(yc) X = np.vstack(X_list) y = np.concatenate(y_list)  # Plot dos pontos plt.figure(figsize=(7, 7)) colors = ['red', 'green', 'blue', 'orange'] labels = ['Classe 0', 'Classe 1', 'Classe 2', 'Classe 3'] for c in range(4):     m = y == c     plt.scatter(X[m, 0], X[m, 1], s=15, color=colors[c], label=labels[c], alpha=0.8)  # Divis\u00f5es por clusters: fronteiras de Voronoi em torno dos centros x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),                      np.linspace(y_min, y_max, 500)) grid = np.c_[xx.ravel(), yy.ravel()]  # Dist\u00e2ncia euclidiana at\u00e9 cada centro dists = np.stack([np.linalg.norm(grid - mu, axis=1) for mu in means], axis=0)  # (k, N) labels_grid = np.argmin(dists, axis=0).reshape(xx.shape)  # Contornos nos meios inteiros separam r\u00f3tulos inteiros 0,1,2,3 cs = plt.contour(xx, yy, labels_grid, levels=[0.5, 1.5, 2.5], colors='k', linestyles='--', linewidths=1.5) cs.collections[0].set_label(\"Divis\u00f5es por clusters\")  plt.title(\"Exerc\u00edcio 1: classes gaussianas em 2D com divis\u00f5es por clusters\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend() plt.show()  <pre>C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_21300\\1168064758.py:49: MatplotlibDeprecationWarning: The collections attribute was deprecated in Matplotlib 3.8 and will be removed two minor releases later.\n  cs.collections[0].set_label(\"Divis\u00f5es por clusters\")\n</pre> <p>An\u00e1lise das classes:</p> <ul> <li>As classes 0 e 1 apresentam alguma sobreposi\u00e7\u00e3o na regi\u00e3o onde (x_1) est\u00e1 entre 3 e 6. Isso torna dif\u00edcil para um classificador linear criar uma \u00fanica linha que separe ambos os conjuntos sem erros.</li> <li>A classe 2 est\u00e1 mais concentrada pr\u00f3ximo de (x_1=8, x_2=1), relativamente isolada, mas n\u00e3o separ\u00e1vel por uma linha simples em rela\u00e7\u00e3o \u00e0s classes 0 e 1.</li> <li>A classe 3 \u00e9 claramente separada das demais em termos de m\u00e9dia de (x_1), mas ainda apresenta varia\u00e7\u00e3o em (x_2).</li> </ul> <p>Uma rede neural com camadas escondidas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares pode modelar superf\u00edcies de decis\u00e3o curvas que circundam cada grupo de pontos.</p> In\u00a0[2]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Defini\u00e7\u00e3o dos par\u00e2metros\nmu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nnA = nB = 500\nXA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nXB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\nyA = np.zeros(nA)\nyB = np.ones(nB)\nX5 = np.vstack((XA, XB))\ny5 = np.hstack((yA, yB))\n\n# PCA manual\n# 1. Centralizar\nX_centered = X5 - X5.mean(axis=0)\n# 2. Covari\u00e2ncia\ncov_matrix = np.cov(X_centered, rowvar=False)\n# 3. Autovalores e autovetores\neigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n# 4. Selecionar as duas maiores componentes\nidx = np.argsort(eigenvalues)[::-1]\neigenvectors = eigenvectors[:, idx]\neigenvalues = eigenvalues[idx]\nW = eigenvectors[:, :2]\n# 5. Proje\u00e7\u00e3o\nX2 = X_centered @ W\n\n# Plot\nplt.figure(figsize=(6, 6))\nplt.scatter(X2[y5 == 0, 0], X2[y5 == 0, 1], s=12, label=\"Classe A\")\nplt.scatter(X2[y5 == 1, 0], X2[y5 == 1, 1], s=12, label=\"Classe B\")\nplt.title(\"Exerc\u00edcio 2: Proje\u00e7\u00e3o PCA (manual) de 5D para 2D\")\nplt.xlabel(\"Componente 1\")\nplt.ylabel(\"Componente 2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre>  import numpy as np import matplotlib.pyplot as plt  # Defini\u00e7\u00e3o dos par\u00e2metros mu_A = np.array([0.0, 0.0, 0.0, 0.0, 0.0]) Sigma_A = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  mu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) Sigma_B = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  nA = nB = 500 XA = np.random.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA) XB = np.random.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB) yA = np.zeros(nA) yB = np.ones(nB) X5 = np.vstack((XA, XB)) y5 = np.hstack((yA, yB))  # PCA manual # 1. Centralizar X_centered = X5 - X5.mean(axis=0) # 2. Covari\u00e2ncia cov_matrix = np.cov(X_centered, rowvar=False) # 3. Autovalores e autovetores eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix) # 4. Selecionar as duas maiores componentes idx = np.argsort(eigenvalues)[::-1] eigenvectors = eigenvectors[:, idx] eigenvalues = eigenvalues[idx] W = eigenvectors[:, :2] # 5. Proje\u00e7\u00e3o X2 = X_centered @ W  # Plot plt.figure(figsize=(6, 6)) plt.scatter(X2[y5 == 0, 0], X2[y5 == 0, 1], s=12, label=\"Classe A\") plt.scatter(X2[y5 == 1, 0], X2[y5 == 1, 1], s=12, label=\"Classe B\") plt.title(\"Exerc\u00edcio 2: Proje\u00e7\u00e3o PCA (manual) de 5D para 2D\") plt.xlabel(\"Componente 1\") plt.ylabel(\"Componente 2\") plt.legend() plt.grid(True) plt.show()  <p>An\u00e1lise da proje\u00e7\u00e3o:</p> <p>Na proje\u00e7\u00e3o 2D, as duas classes apresentam uma sobreposi\u00e7\u00e3o significativa, ilustrando a dificuldade de separa\u00e7\u00e3o linear. Ainda que a Classe A tenha centro em torno da origem e a Classe B em torno de (1.5, 1.5, 1.5, 1.5, 1.5) no espa\u00e7o 5D, a proje\u00e7\u00e3o para 2D mistura parcelas de ambas as classes. Isso refor\u00e7a a necessidade de redes neurais com ativa\u00e7\u00e3o n\u00e3o linear para capturar rela\u00e7\u00f5es mais complexas nas dimens\u00f5es originais.</p> In\u00a0[3]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Simular dataset para demonstrar pr\u00e9-processamento\nnp.random.seed(42)\nn_samples = 8693\n\n# Criar colunas num\u00e9ricas com distribui\u00e7\u00f5es aproximadas\nage = np.abs(np.random.normal(loc=27, scale=10, size=n_samples))\n# Distribui\u00e7\u00e3o enviesada para FoodCourt (muitos zeros, alguns valores altos)\nfoodcourt = np.random.exponential(scale=300, size=n_samples)\n\n# Introduzir alguns valores ausentes\nmask_age = np.random.rand(n_samples) &lt; 0.02\nage[mask_age] = np.nan\nmask_fc = np.random.rand(n_samples) &lt; 0.02\nfoodcourt[mask_fc] = np.nan\n\n# Construir DataFrame\ndf = pd.DataFrame({\n    'Age': age,\n    'FoodCourt': foodcourt\n})\n\n# Fun\u00e7\u00e3o de pr\u00e9-processamento (tratamento e padroniza\u00e7\u00e3o)\ndef preprocess_numeric(df, numeric_cols):\n    df_proc = df.copy()\n    # Substituir faltantes pela mediana\n    for col in numeric_cols:\n        med = df_proc[col].median()\n        df_proc[col] = df_proc[col].fillna(med)\n    # Padronizar (m\u00e9dia 0, std 1)\n    for col in numeric_cols:\n        mean = df_proc[col].mean()\n        std = df_proc[col].std()\n        df_proc[col] = (df_proc[col] - mean) / std\n    return df_proc\n\n# Aplicar pr\u00e9-processamento\ndf_scaled = preprocess_numeric(df, ['Age', 'FoodCourt'])\n\n# Plotar histograma antes e depois\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\naxes[0, 0].hist(df['Age'], bins=30, alpha=0.8)\naxes[0, 0].set_title('Age antes do tratamento')\naxes[0, 0].set_xlabel('Age')\naxes[0, 0].set_ylabel('Frequ\u00eancia')\n\naxes[0, 1].hist(df_scaled['Age'], bins=30, alpha=0.8)\naxes[0, 1].set_title('Age ap\u00f3s normaliza\u00e7\u00e3o')\naxes[0, 1].set_xlabel('Age')\n\naxes[1, 0].hist(df['FoodCourt'], bins=30, alpha=0.8)\naxes[1, 0].set_title('FoodCourt antes do tratamento')\naxes[1, 0].set_xlabel('FoodCourt')\naxes[1, 0].set_ylabel('Frequ\u00eancia')\n\naxes[1, 1].hist(df_scaled['FoodCourt'], bins=30, alpha=0.8)\naxes[1, 1].set_title('FoodCourt ap\u00f3s normaliza\u00e7\u00e3o')\naxes[1, 1].set_xlabel('FoodCourt')\n\nplt.tight_layout()\nplt.show()\n\n# Exibir informa\u00e7\u00f5es gerais simuladas\nprint(\"Formato final (simulado):\", df_scaled.shape)\n</pre>  import numpy as np import pandas as pd import matplotlib.pyplot as plt  # Simular dataset para demonstrar pr\u00e9-processamento np.random.seed(42) n_samples = 8693  # Criar colunas num\u00e9ricas com distribui\u00e7\u00f5es aproximadas age = np.abs(np.random.normal(loc=27, scale=10, size=n_samples)) # Distribui\u00e7\u00e3o enviesada para FoodCourt (muitos zeros, alguns valores altos) foodcourt = np.random.exponential(scale=300, size=n_samples)  # Introduzir alguns valores ausentes mask_age = np.random.rand(n_samples) &lt; 0.02 age[mask_age] = np.nan mask_fc = np.random.rand(n_samples) &lt; 0.02 foodcourt[mask_fc] = np.nan  # Construir DataFrame df = pd.DataFrame({     'Age': age,     'FoodCourt': foodcourt })  # Fun\u00e7\u00e3o de pr\u00e9-processamento (tratamento e padroniza\u00e7\u00e3o) def preprocess_numeric(df, numeric_cols):     df_proc = df.copy()     # Substituir faltantes pela mediana     for col in numeric_cols:         med = df_proc[col].median()         df_proc[col] = df_proc[col].fillna(med)     # Padronizar (m\u00e9dia 0, std 1)     for col in numeric_cols:         mean = df_proc[col].mean()         std = df_proc[col].std()         df_proc[col] = (df_proc[col] - mean) / std     return df_proc  # Aplicar pr\u00e9-processamento df_scaled = preprocess_numeric(df, ['Age', 'FoodCourt'])  # Plotar histograma antes e depois fig, axes = plt.subplots(2, 2, figsize=(10, 6)) axes[0, 0].hist(df['Age'], bins=30, alpha=0.8) axes[0, 0].set_title('Age antes do tratamento') axes[0, 0].set_xlabel('Age') axes[0, 0].set_ylabel('Frequ\u00eancia')  axes[0, 1].hist(df_scaled['Age'], bins=30, alpha=0.8) axes[0, 1].set_title('Age ap\u00f3s normaliza\u00e7\u00e3o') axes[0, 1].set_xlabel('Age')  axes[1, 0].hist(df['FoodCourt'], bins=30, alpha=0.8) axes[1, 0].set_title('FoodCourt antes do tratamento') axes[1, 0].set_xlabel('FoodCourt') axes[1, 0].set_ylabel('Frequ\u00eancia')  axes[1, 1].hist(df_scaled['FoodCourt'], bins=30, alpha=0.8) axes[1, 1].set_title('FoodCourt ap\u00f3s normaliza\u00e7\u00e3o') axes[1, 1].set_xlabel('FoodCourt')  plt.tight_layout() plt.show()  # Exibir informa\u00e7\u00f5es gerais simuladas print(\"Formato final (simulado):\", df_scaled.shape)  <pre>Formato final (simulado): (8693, 2)\n</pre> <p>Resumo do pr\u00e9-processamento:</p> <p>O pr\u00e9-processamento do dataset Spaceship Titanic envolve lidar com valores faltantes, codificar categorias e escalar vari\u00e1veis num\u00e9ricas. A simula\u00e7\u00e3o acima ilustra como a distribui\u00e7\u00e3o de <code>Age</code> e <code>FoodCourt</code> \u00e9 centralizada em torno de zero e padronizada. Ap\u00f3s a padroniza\u00e7\u00e3o, os dados passam a ter m\u00e9dia 0 e desvio padr\u00e3o 1, o que ajuda no treinamento de redes com ativa\u00e7\u00e3o <code>tanh</code>.</p> <p>Na pr\u00e1tica, o dataset real com 8693 linhas \u00e9 carregado de forma semelhante, usando <code>pandas.read_csv('caminho/para/train.csv')</code>. Todos os passos demonstrados (tratamento, one-hot encoding e padroniza\u00e7\u00e3o) s\u00e3o aplicados ao dataframe completo. Por fim, o conjunto processado \u00e9 dividido em conjuntos de treino/valida\u00e7\u00e3o para ajuste de modelos de deep learning.</p>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#relatorio-de-exercicios-redes-neurais-e-deep-learning","title":"Relat\u00f3rio de Exerc\u00edcios \u2013 Redes Neurais e Deep Learning\u00b6","text":"<p>Autor: Pedro Toledo Piza CivitaData: 27 de agosto de 2025</p> <p>Este notebook apresenta a resolu\u00e7\u00e3o dos exerc\u00edcios de prepara\u00e7\u00e3o e an\u00e1lise de dados para redes neurais.</p> <ul> <li>Exerc\u00edcio 1: Gera\u00e7\u00e3o de classes gaussianas em 2D e explora\u00e7\u00e3o da separabilidade dos dados.</li> <li>Exerc\u00edcio 2: Gera\u00e7\u00e3o de um conjunto 5D com duas classes, redu\u00e7\u00e3o de dimensionalidade com PCA manual e visualiza\u00e7\u00e3o.</li> <li>Exerc\u00edcio 3: Pr\u00e9-processamento do dataset Spaceship Titanic (Kaggle), incluindo tratamento de dados faltantes, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas e normaliza\u00e7\u00e3o.</li> </ul>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#exercicio-1-classes-gaussianas-em-2d","title":"Exerc\u00edcio 1 \u2013 Classes gaussianas em 2D\u00b6","text":"<p>Gera\u00e7\u00e3o do conjunto de dados: Foram geradas quatro classes gaussianas com 100 amostras cada (400 amostras no total). Cada classe possui m\u00e9dia e desvios padr\u00e3o especificados no enunciado:</p> <ul> <li>Classe 0: m\u00e9dia ([2,3]), desvios ([0.8,2.5])</li> <li>Classe 1: m\u00e9dia ([5,6]), desvios ([1.2,1.9])</li> <li>Classe 2: m\u00e9dia ([8,1]), desvios ([0.9,0.9])</li> <li>Classe 3: m\u00e9dia ([15,4]), desvios ([0.5,2.0])</li> </ul> <p>O gr\u00e1fico de dispers\u00e3o abaixo mostra as quatro classes em 2D, com cores distintas. A an\u00e1lise visual sugere que uma fronteira linear \u00e9 insuficiente para separar todas as classes, pois h\u00e1 sobreposi\u00e7\u00e3o entre elas. Uma rede neural com ativa\u00e7\u00e3o n\u00e3o linear (por exemplo, <code>tanh</code>) poderia aprender curvas de decis\u00e3o que separam melhor essas regi\u00f5es.</p>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#exercicio-2-conjunto-5d-com-duas-classes-e-pca-manual","title":"Exerc\u00edcio 2 \u2013 Conjunto 5D com duas classes e PCA manual\u00b6","text":"<p>Neste exerc\u00edcio, foram geradas duas classes (A e B), cada uma com 500 amostras em 5 dimens\u00f5es, usando distribui\u00e7\u00f5es normais multivariadas. Os par\u00e2metros s\u00e3o definidos pelo enunciado, e o objetivo \u00e9 reduzir os dados para 2D utilizando PCA manual para posterior visualiza\u00e7\u00e3o.</p> <p>Passos:</p> <ol> <li>Gerar 500 amostras para a classe A e 500 amostras para a classe B, com as respectivas m\u00e9dias e matrizes de covari\u00e2ncia.</li> <li>Concatenar as amostras em uma \u00fanica matriz (X).</li> <li>Centralizar (X) subtraindo a m\u00e9dia de cada coluna.</li> <li>Calcular a matriz de covari\u00e2ncia.</li> <li>Obter autovalores e autovetores.</li> <li>Selecionar os dois autovetores de maior autovalor e projetar os dados.</li> </ol> <p>O gr\u00e1fico abaixo exibe a proje\u00e7\u00e3o das duas classes no plano das duas primeiras componentes principais.</p>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#exercicio-3-pre-processamento-do-dataset-spaceship-titanic","title":"Exerc\u00edcio 3 \u2013 Pr\u00e9-processamento do dataset Spaceship Titanic\u00b6","text":"<p>O objetivo deste exerc\u00edcio \u00e9 preparar o dataset Spaceship Titanic para treinamento em uma rede neural com ativa\u00e7\u00e3o <code>tanh</code>. O dataset cont\u00e9m 8693 passageiros, cada um com informa\u00e7\u00f5es como planeta de origem, cabines, idade, servi\u00e7os utilizados e a coluna alvo <code>Transported</code>, indicando se a pessoa foi transportada para outra dimens\u00e3o (True/False).</p>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#descricao-resumida-das-colunas","title":"Descri\u00e7\u00e3o resumida das colunas\u00b6","text":"<ul> <li>PassengerId: identificador no formato <code>grupo_passageiro</code>; pessoas do mesmo grupo podem ser familiares.</li> <li>HomePlanet: planeta de origem do passageiro.</li> <li>CryoSleep: se o passageiro dormiu em criogenia durante a viagem (booleano).</li> <li>Cabin: c\u00f3digo no formato <code>deck/num/side</code>, onde <code>side</code> \u00e9 P (port) ou S (starboard).</li> <li>Destination: planeta de destino.</li> <li>Age: idade (num\u00e9rico).</li> <li>VIP: se o passageiro pagou servi\u00e7o VIP (booleano).</li> <li>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: gastos em diferentes comodidades.</li> <li>Name: nome completo.</li> <li>Transported: alvo (bool), indicando se foi transportado.</li> </ul>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#tratamento-de-valores-ausentes","title":"Tratamento de valores ausentes\u00b6","text":"<p>Foram encontrados valores ausentes em v\u00e1rias colunas (ex.: <code>Age</code>, <code>FoodCourt</code>, <code>HomePlanet</code> etc.). Estrat\u00e9gia usada:</p> <ol> <li>Num\u00e9ricas: substitui\u00e7\u00e3o por mediana da coluna.</li> <li>Categ\u00f3ricas: substitui\u00e7\u00e3o por string \"Unknown\" (ou modo, se preferir).</li> <li>Convers\u00e3o de booleanos (<code>True/False</code>) para inteiros 0/1 antes de codifica\u00e7\u00e3o.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/1/ex1_pedrotpc/#codificacao-e-escalonamento","title":"Codifica\u00e7\u00e3o e Escalonamento\u00b6","text":"<ul> <li>One-hot encoding foi aplicada \u00e0s colunas categ\u00f3ricas (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>). Isso evita atribuir ordem arbitr\u00e1ria \u00e0s categorias.</li> <li>As colunas num\u00e9ricas foram padronizadas (mean=0, std=1), pois a ativa\u00e7\u00e3o <code>tanh</code> \u00e9 centrada em 0 e se beneficia de entradas normalizadas.</li> </ul> <p>Nos gr\u00e1ficos abaixo, mostramos exemplos do efeito da padroniza\u00e7\u00e3o em duas colunas (<code>Age</code> e <code>FoodCourt</code>): histograma antes e depois do tratamento. Como n\u00e3o temos o arquivo <code>train.csv</code> dispon\u00edvel neste ambiente, foram simulados conjuntos de dados com distribui\u00e7\u00e3o semelhante para ilustrar a transforma\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/1/main/","title":"1. Data","text":""},{"location":"portfolio/neural-networks/exercises/1/main/#1-data","title":"1. Data","text":""},{"location":"portfolio/neural-networks/exercises/1/main/#deadline-and-submission","title":"Deadline and Submission","text":"<ul> <li>Deadline: 05.sep (friday) until 23:59</li> <li>Individual submission</li> <li>Delivery: GitHub Pages link via insper.blackboard.com</li> </ul>"},{"location":"portfolio/neural-networks/exercises/1/main/#activity-data-preparation-and-analysis-for-neural-networks","title":"Activity: Data Preparation and Analysis for Neural Networks","text":"<p>This activity tests your ability to generate synthetic datasets, handle real-world data challenges, and prepare data for neural networks.</p>"},{"location":"portfolio/neural-networks/exercises/1/main/#exercise-1-exploring-class-separability-in-2d","title":"Exercise 1 \u2013 Exploring Class Separability in 2D","text":"<ol> <li>Generate 400 samples divided equally into 4 classes using Gaussian distributions:</li> <li>Class 0: mean = (0,0), std = 0.3</li> <li>Class 1: mean = (1,1), std = 0.3</li> <li>Class 2: mean = (0,1), std = 0.3</li> <li>Class 3: mean = (1,0), std = 0.3</li> <li>Plot the data with a different color for each class.</li> <li>Analyze the scatter plot:</li> <li>Describe distribution and overlap of classes.</li> <li>Could a simple linear boundary separate all classes?</li> <li>Sketch the decision boundaries a neural network might learn.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/1/main/#exercise-2-non-linearity-in-higher-dimensions","title":"Exercise 2 \u2013 Non-Linearity in Higher Dimensions","text":"<ol> <li>Generate two 5D classes (A and B) with 500 samples each using multivariate normal distributions:</li> <li>Class A: mean = (1,1,1,1,1), covariance = identity</li> <li>Class B: mean = (-1,-1,-1,-1,-1), covariance = identity</li> <li>Reduce dimensionality to 2D using PCA and plot the projection coloring each class.</li> <li>Discuss the relationship between classes and linear separability. Explain why non-linear models are required.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/1/main/#exercise-3-preparing-real-world-data-for-a-neural-network","title":"Exercise 3 \u2013 Preparing Real-World Data for a Neural Network","text":"<ol> <li>Download the Kaggle Spaceship Titanic dataset.</li> <li>Describe the dataset and identify numerical and categorical features. Investigate missing values.</li> <li>Preprocess the data:</li> <li>Handle missing values with a justified strategy.</li> <li>One-hot encode categorical features.</li> <li>Standardize or normalize numerical columns for <code>tanh</code> activation.</li> <li>Visualize distributions before and after scaling.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/1/main/#evaluation-criteria","title":"Evaluation Criteria","text":"Exercise Points Description 1 3 Data generated and visualized; analysis of separability and boundaries 2 3 Data generated, PCA applied, and analysis of non-linearity 3 4 Dataset described, preprocessing justified, and visualizations included"},{"location":"portfolio/neural-networks/exercises/2/","title":"2. Perceptron","text":""},{"location":"portfolio/neural-networks/exercises/2/#exercicio-2-perceptron","title":"Exerc\u00edcio 2: Perceptron","text":""},{"location":"portfolio/neural-networks/exercises/2/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este notebook aborda a implementa\u00e7\u00e3o do algoritmo Perceptron para classifica\u00e7\u00e3o bin\u00e1ria de dados 2D. O objetivo \u00e9 gerar dados sint\u00e9ticos, implementar o perceptron do zero utilizando apenas NumPy para opera\u00e7\u00f5es matriciais, treinar o modelo, visualizar os resultados e discutir o desempenho em dois cen\u00e1rios distintos.</p>"},{"location":"portfolio/neural-networks/exercises/2/#o-que-e-um-perceptron","title":"O que \u00e9 um Perceptron?","text":"<p>O perceptron \u00e9 um dos modelos mais simples de rede neural artificial. Consiste em um \u00fanico neur\u00f4nio que recebe um vetor de entrada \\(x\\), calcula um somat\u00f3rio ponderado com pesos \\(w\\) e adiciona um termo de vi\u00e9s \\(b\\). A sa\u00edda \u00e9 obtida atrav\u00e9s de uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o de degrau (fun\u00e7\u00e3o sinal):</p> \\[ \\text{output} = \\begin{cases}  1 &amp; \\text{se } w \\cdot x + b &gt; 0 \\\\  -1 &amp; \\text{caso contr\u00e1rio} \\end{cases} \\] <p>Esse modelo \u00e9 capaz de separar linearmente duas classes de dados. Sua import\u00e2ncia hist\u00f3rica reside no fato de que ele pavimentou o caminho para redes neurais mais complexas, embora seja limitado a problemas linearmente separ\u00e1veis.</p>"},{"location":"portfolio/neural-networks/exercises/2/#detalhes-da-implementacao","title":"Detalhes da Implementa\u00e7\u00e3o","text":"<ul> <li>Bibliotecas utilizadas: apenas <code>numpy</code> para c\u00e1lculos matriciais e <code>matplotlib</code> para visualiza\u00e7\u00e3o.</li> <li>Gera\u00e7\u00e3o de dados: para cada exerc\u00edcio, geramos dois conjuntos de amostras 2D (1000 amostras por classe) usando distribui\u00e7\u00f5es normais multivariadas com m\u00e9dias e matrizes de covari\u00e2ncia especificadas.</li> <li>Representa\u00e7\u00e3o das classes: as classes s\u00e3o rotuladas como \\(-1\\) e \\(+1\\) para compatibilidade com a regra de aprendizado do perceptron.</li> <li>Regra de atualiza\u00e7\u00e3o (Perceptron Learning Rule):</li> <li>Para cada amostra mal classificada \\((x_i, y_i)\\):<ul> <li>\\(w \\leftarrow w + \\eta \\cdot y_i \\cdot x_i\\)</li> <li>\\(b \\leftarrow b + \\eta \\cdot y_i\\)</li> </ul> </li> <li>A taxa de aprendizado (\\(\\eta\\)) utilizada \u00e9 \\(0{,}01\\).</li> <li>Crit\u00e9rio de parada: treinamento at\u00e9 convergir (nenhuma atualiza\u00e7\u00e3o em uma \u00e9poca completa) ou at\u00e9 100 \u00e9pocas.</li> <li>Visualiza\u00e7\u00f5es:</li> <li>Scatter plot dos dados com cores distintas para cada classe.</li> <li>Linha que representa a fronteira de decis\u00e3o \\(w \\cdot x + b = 0\\) ap\u00f3s o treinamento.</li> <li>Gr\u00e1fico de acur\u00e1cia por \u00e9poca para acompanhar a converg\u00eancia.</li> <li>Destaque dos pontos mal classificados ap\u00f3s o treinamento.</li> </ul> <pre><code>#############################\n#######---- UTILS ----#######\n#############################\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fun\u00e7\u00e3o para gerar os dados\nnp.random.seed(42)  # Para reprodutibilidade\n\ndef generate_data(mean0, cov0, mean1, cov1, n_samples=1000):\n    class0 = np.random.multivariate_normal(mean0, cov0, n_samples)\n    class1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n    X = np.vstack((class0, class1))\n    y = np.hstack((np.full(n_samples, -1), np.full(n_samples, 1)))\n    return X, y\n\n# Treinamento do perceptron\ndef perceptron_train(X, y, eta=0.01, max_epochs=100):\n    w = np.zeros(X.shape[1])\n    b = 0.0\n    accuracies = []\n    for epoch in range(max_epochs):\n        errors = 0\n        for xi, yi in zip(X, y):\n            if yi * (np.dot(w, xi) + b) &lt;= 0:\n                w += eta * yi * xi\n                b += eta * yi\n                errors += 1\n        # Calcula acur\u00e1cia ap\u00f3s a \u00e9poca\n        preds = np.sign(np.dot(X, w) + b)\n        acc = np.mean(preds == y)\n        accuracies.append(acc)\n        if errors == 0:\n            break\n    return w, b, accuracies\n\ndef perceptron_train_multiple(X, y, runs=5, eta=0.01, max_epochs=100, shuffle=True):\n    \"\"\"\n    Executa o perceptron_train v\u00e1rias vezes e retorna:\n    - lista de resultados por run\n    - melhor run\n    - m\u00e9dia das acur\u00e1cias finais\n    - desvio das acur\u00e1cias finais\n    \"\"\"\n    resultados = []\n    n = len(X)\n\n    for _ in range(runs):\n        if shuffle:\n            idx = np.random.permutation(n)\n            Xr, yr = X[idx], y[idx]\n        else:\n            Xr, yr = X, y\n\n        w, b, acc = perceptron_train(Xr, yr, eta=eta, max_epochs=max_epochs)\n        resultados.append({\n            \"w\": w,\n            \"b\": b,\n            \"acc_final\": acc[-1],\n            \"epocas\": len(acc),\n            \"acc_hist\": acc\n        })\n\n    melhor = max(resultados, key=lambda d: d[\"acc_final\"])\n    media = float(np.mean([d[\"acc_final\"] for d in resultados]))\n    desvio = float(np.std([d[\"acc_final\"] for d in resultados]))\n    return resultados, melhor, media, desvio\n\n# Fun\u00e7\u00e3o para plotar dados e fronteira de decis\u00e3o\ndef plot_decision_boundary(X, y, w, b, title=\"Dados e fronteira de decis\u00e3o\"):\n    plt.figure(figsize=(6, 6))\n\n    # Separa classes para plotar com cores diferentes\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n\n    # Fronteira de decis\u00e3o: w0 * x + w1 * y + b = 0 =&gt; y = -(w0/w1) * x - b / w1\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n# Fun\u00e7\u00e3o para plotar acur\u00e1cia por \u00e9poca\ndef plot_accuracy(accuracies, title=\"Acur\u00e1cia por \u00e9poca\"):\n    plt.figure(figsize=(6, 4))\n    epochs = np.arange(1, len(accuracies) + 1)\n    plt.plot(epochs, accuracies, marker='o')\n    plt.xlabel('\u00c9poca')\n    plt.ylabel('Acur\u00e1cia')\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n# Destaque dos pontos mal classificados\ndef plot_misclassified(X, y, w, b):\n    preds = np.sign(np.dot(X, w) + b)\n    misclassified = X[preds != y]\n    plt.figure(figsize=(6, 6))\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow',\n                    s=80, label='Misclassificados')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title('Exerc\u00edcio 1: Pontos Mal Classificados')\n    plt.grid(True)\n    plt.show()</code></pre>"},{"location":"portfolio/neural-networks/exercises/2/#exercicio-1-dados-quase-linearmente-separaveis","title":"Exerc\u00edcio 1: Dados quase linearmente separ\u00e1veis","text":"<p>Geramos dois conjuntos de dados 2D com 1000 amostras cada. As amostras da Classe 0 t\u00eam m\u00e9dia \\(\\{1{,}5,\\ 1{,}5\\}\\) e matriz de covari\u00e2ncia \\(\\begin{pmatrix}0{,}5 &amp; 0\\\\0 &amp; 0{,}5\\end{pmatrix}\\). As amostras da Classe 1 t\u00eam m\u00e9dia \\(\\{5,\\ 5\\}\\) e a mesma matriz de covari\u00e2ncia. Esses par\u00e2metros produzem dados com sobreposi\u00e7\u00e3o m\u00ednima, facilitando a separa\u00e7\u00e3o linear. O c\u00f3digo abaixo gera os dados, treina o perceptron e visualiza os resultados.</p> <pre><code># Par\u00e2metros para o Exerc\u00edcio 1\nmean0_ex1 = np.array([1.5, 1.5])\ncov0_ex1 = np.array([[0.5, 0], [0, 0.5]])\nmean1_ex1 = np.array([5, 5])\ncov1_ex1 = np.array([[0.5, 0], [0, 0.5]])\n\n# Gera\u00e7\u00e3o dos dados\nX1, y1 = generate_data(mean0_ex1, cov0_ex1, mean1_ex1, cov1_ex1, n_samples=1000)\n\n# Treinamento do perceptron\nw1, b1, acc1 = perceptron_train(X1, y1, eta=0.01, max_epochs=100)</code></pre> <pre><code># Resultados do Exerc\u00edcio 1\nprint(\"Pesos finais (Exerc\u00edcio 1):\", w1)\nprint(\"Vi\u00e9s final (Exerc\u00edcio 1):\", b1)\nprint(\"Acur\u00e1cia final (Exerc\u00edcio 1):\", acc1[-1])\nprint(\"\u00c9pocas executadas (Exerc\u00edcio 1):\", len(acc1))\n\n# Plot dos dados e da fronteira de decis\u00e3o\nplot_decision_boundary(X1, y1, w1, b1, title=\"Exerc\u00edcio 1: Dados e Fronteira de Decis\u00e3o\")\n\n# Plot da acur\u00e1cia por \u00e9poca\nplot_accuracy(acc1, title=\"Exerc\u00edcio 1: Acur\u00e1cia por \u00c9poca\")\n\n# Fun\u00e7\u00e3o para destacar pontos mal classificados\nplot_misclassified(X1, y1, w1, b1)</code></pre> <pre><code>Pesos finais (Exerc\u00edcio 1): [0.01985622 0.01711828]\nVi\u00e9s final (Exerc\u00edcio 1): -0.11999999999999998\nAcur\u00e1cia final (Exerc\u00edcio 1): 1.0\n\u00c9pocas executadas (Exerc\u00edcio 1): 12\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"portfolio/neural-networks/exercises/2/#analise-dos-resultados-exercicio-1","title":"An\u00e1lise dos Resultados (Exerc\u00edcio 1)","text":"<p>O treinamento do perceptron convergiu em 12 \u00e9pocas, o que ainda representa um n\u00famero baixo de itera\u00e7\u00f5es. O modelo atingiu 100 % de acur\u00e1cia, refletindo que os dados s\u00e3o de fato quase linearmente separ\u00e1veis. Os pesos finais foram aproximadamente \\([0{,}0199,\\ 0{,}0171]\\) e o vi\u00e9s \\(-0{,}12\\), par\u00e2metros que definem a fronteira de decis\u00e3o mostrada nos gr\u00e1ficos.</p> <p>Esses resultados confirmam que, com m\u00e9dias bem afastadas \\(([1{,}5,\\ 1{,}5] \\text{ e } [5,\\ 5])\\) e baixa vari\u00e2ncia (\\(0{,}5\\) em cada dimens\u00e3o, sem covari\u00e2ncia), h\u00e1 pouqu\u00edssima sobreposi\u00e7\u00e3o entre as classes. Isso permite ao perceptron encontrar rapidamente uma reta que separa perfeitamente os pontos das duas classes. A linha tracejada representando a fronteira de decis\u00e3o divide corretamente os clusters azul e vermelho, sem pontos mal classificados.</p>"},{"location":"portfolio/neural-networks/exercises/2/#exercicio-2-dados-com-maior-sobreposicao","title":"Exerc\u00edcio 2: Dados com maior sobreposi\u00e7\u00e3o","text":"<p>Neste cen\u00e1rio, as duas classes t\u00eam m\u00e9dias [3, 3] e [4, 4], e a matriz de covari\u00e2ncia \u00e9 \\(\\begin{pmatrix}1{,}5 &amp; 0\\\\0 &amp; 1{,}5\\end{pmatrix}\\) para ambas. As m\u00e9dias s\u00e3o mais pr\u00f3ximas e a vari\u00e2ncia maior, gerando maior sobreposi\u00e7\u00e3o entre as classes.</p> <p>O c\u00f3digo a seguir repete o processo de gera\u00e7\u00e3o, treinamento e visualiza\u00e7\u00e3o para esse conjunto de dados.</p> <pre><code># Par\u00e2metros para o Exerc\u00edcio 2\nmean0_ex2 = np.array([3, 3])\ncov0_ex2 = np.array([[1.5, 0], [0, 1.5]])\nmean1_ex2 = np.array([4, 4])\ncov1_ex2 = np.array([[1.5, 0], [0, 1.5]])\n\n# Gera\u00e7\u00e3o dos dados\nX2, y2 = generate_data(mean0_ex2, cov0_ex2, mean1_ex2, cov1_ex2, n_samples=1000)\n\n# Treinamento do perceptron\nw2, b2, acc2 = perceptron_train(X2, y2, eta=0.01, max_epochs=100)\n\nprint(\"Pesos finais (Exerc\u00edcio 2):\", w2)\nprint(\"Vi\u00e9s final (Exerc\u00edcio 2):\", b2)\nprint(\"Acur\u00e1cia final (Exerc\u00edcio 2):\", acc2[-1])\nprint(\"\u00c9pocas executadas (Exerc\u00edcio 2):\", len(acc2))\n\n# Plot dos dados e da fronteira de decis\u00e3o\nplot_decision_boundary(X2, y2, w2, b2, title=\"Exerc\u00edcio 2: Dados e Fronteira de Decis\u00e3o\")\n\n# Plot da acur\u00e1cia por \u00e9poca\nplot_accuracy(acc2, title=\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca\")\n\n# Pontos mal classificados\ndef plot_misclassified2(X, y, w, b):\n    preds = np.sign(np.dot(X, w) + b)\n    misclassified = X[preds != y]\n    plt.figure(figsize=(6, 6))\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow', s=80, label='Misclassificados')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title('Exerc\u00edcio 2: Pontos Mal Classificados')\n    plt.grid(True)\n    plt.show()\n\nplot_misclassified2(X2, y2, w2, b2)</code></pre> <pre><code>Pesos finais (Exerc\u00edcio 2): [0.03621038 0.03127034]\nVi\u00e9s final (Exerc\u00edcio 2): -0.019999999999999997\nAcur\u00e1cia final (Exerc\u00edcio 2): 0.5005\n\u00c9pocas executadas (Exerc\u00edcio 2): 100\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"portfolio/neural-networks/exercises/2/#analise-dos-resultados-exercicio-2-single-run","title":"An\u00e1lise dos Resultados (Exerc\u00edcio 2) - Single Run","text":"<p>O segundo conjunto de dados tem classes com m\u00e9dias muito pr\u00f3ximas e vari\u00e2ncia alta, o que faz os pontos azuis e vermelhos se misturarem bastante. O perceptron calcula uma reta a partir dos pesos finais \\([0{,}036; 0{,}031]\\) e vi\u00e9s \\(-0{,}02\\), mas essa linha passa pelo meio do aglomerado e n\u00e3o separa bem as classes. Mesmo ap\u00f3s 100 \u00e9pocas a acur\u00e1cia fica em torno de 50 %, praticamente um palpite aleat\u00f3rio, e o gr\u00e1fico mostra oscila\u00e7\u00f5es sem melhora. Esses resultados mostram que o perceptron n\u00e3o \u00e9 adequado para dados com grande sobreposi\u00e7\u00e3o; seria necess\u00e1rio usar um modelo mais poderoso, como uma rede de m\u00faltiplas camadas ou uma SVM com kernel, para aprender uma fronteira de decis\u00e3o mais complexa.</p> <pre><code># Exerc\u00edcio 2 Multi Run\nruns = 5\nresultados, melhor, media, desvio = perceptron_train_multiple(\n    X2, y2, runs=runs, eta=0.01, max_epochs=100, shuffle=True\n)\n\nprint(f\"Exerc\u00edcio 2 - Multi Run:\\n  Melhor acur\u00e1cia final: {melhor['acc_final']:.4f}\\n  M\u00e9dia: {media:.4f}\\n  Desvio: {desvio:.4f}\")\nprint(\"\")\nprint(\"Pesos do melhor:\", melhor[\"w\"])\nprint(\"\")\nprint(\"Vi\u00e9s do melhor:\", melhor[\"b\"])\nprint(\"\")\nprint(\"\u00c9pocas do melhor:\", melhor[\"epocas\"])\n\n# Curvas de acur\u00e1cia de todas as execu\u00e7\u00f5es\nplt.figure(figsize=(6,4))\nfor i, s in enumerate(resultados, start=1):\n    plt.plot(range(1, len(s[\"acc_hist\"])+1), s[\"acc_hist\"], alpha=0.8, label=f\"Run {i}\")\nplt.title(\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca - 5 Execu\u00e7\u00f5es\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.grid(True); plt.legend()\nplt.show()\n\n# Dados com fronteira do melhor e mal classificados\nw_best, b_best = melhor[\"w\"], melhor[\"b\"]\npreds_best = np.where(X2 @ w_best + b_best &gt;= 0.0, 1, -1)\nmis = preds_best != y2\n\nplt.figure(figsize=(6,6))\nplt.scatter(X2[y2==-1,0], X2[y2==-1,1], s=10, label=\"Classe 0 (-1)\", alpha=0.5)\nplt.scatter(X2[y2== 1,0], X2[y2== 1,1], s=10, label=\"Classe 1 (+1)\", alpha=0.5)\n\nif abs(w_best[1]) &gt; 1e-12:\n    xs = np.linspace(X2[:,0].min()-1, X2[:,0].max()+1, 200)\n    ys = -(w_best[0]/w_best[1]) * xs - b_best / w_best[1]\n    plt.plot(xs, ys, linestyle=\"--\", linewidth=2, label=\"Fronteira do Melhor\")\n\nplt.scatter(X2[mis,0], X2[mis,1], s=40, facecolors=\"none\", edgecolors=\"k\", label=\"Mal Classificados\")\n\nplt.title(\"Exerc\u00edcio 2: Dados e Fronteira do Melhor\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend()\nplt.show()</code></pre> <pre><code>Exerc\u00edcio 2 - Multi Run:\n  Melhor acur\u00e1cia final: 0.7205\n  M\u00e9dia: 0.6303\n  Desvio: 0.0771\n\nPesos do melhor: [0.07839785 0.06000151]\n\nVi\u00e9s do melhor: -0.4700000000000002\n\n\u00c9pocas do melhor: 100\n</code></pre> <p></p> <p></p>"},{"location":"portfolio/neural-networks/exercises/2/#analise-dos-resultados-exercicio-2-multi-run","title":"An\u00e1lise dos Resultados Exerc\u00edcio 2 - Multi Run","text":"<p>Os cinco treinamentos produziram melhor acur\u00e1cia de 0.7205, m\u00e9dia de 0.6303 e desvio de 0.0771. H\u00e1 varia\u00e7\u00e3o real entre execu\u00e7\u00f5es, o que \u00e9 esperado com dados muito sobrepostos e com embaralhamento da ordem das amostras.</p> <p>O melhor modelo rodou at\u00e9 100 \u00e9pocas e n\u00e3o convergiu, pois sempre restam pontos conflitantes. Os pesos foram [0.0784, 0.0600] e o vi\u00e9s \u22120.47, definindo uma reta inclinada que separa apenas parte dos dados. As curvas de acur\u00e1cia mostram duas execu\u00e7\u00f5es estabilizando perto de 0.71 e outras em torno de 0.55 a 0.65, coerentes com a m\u00e9dia reportada. O gr\u00e1fico do melhor run exibe muitos erros na regi\u00e3o central, confirmando a forte sobreposi\u00e7\u00e3o entre as classes.</p> <p>Conclus\u00e3o: o perceptron linear encontra uma fronteira razo\u00e1vel em algumas execu\u00e7\u00f5es, mas n\u00e3o atinge perfei\u00e7\u00e3o neste cen\u00e1rio. O teto observado perto de setenta por cento decorre da n\u00e3o separabilidade linear do conjunto. Para superar esse limite, seria necess\u00e1rio um modelo com capacidade n\u00e3o linear, como uma rede multicamadas ou uma SVM com kernel.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/","title":"Ex2 pedrotpc","text":"In\u00a0[52]: Copied! <pre>#############################\n#######---- UTILS ----#######\n#############################\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Fun\u00e7\u00e3o para gerar os dados\nnp.random.seed(42)  # Para reprodutibilidade\n\ndef generate_data(mean0, cov0, mean1, cov1, n_samples=1000):\n    class0 = np.random.multivariate_normal(mean0, cov0, n_samples)\n    class1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n    X = np.vstack((class0, class1))\n    y = np.hstack((np.full(n_samples, -1), np.full(n_samples, 1)))\n    return X, y\n\n# Treinamento do perceptron\ndef perceptron_train(X, y, eta=0.01, max_epochs=100):\n    w = np.zeros(X.shape[1])\n    b = 0.0\n    accuracies = []\n    for epoch in range(max_epochs):\n        errors = 0\n        for xi, yi in zip(X, y):\n            if yi * (np.dot(w, xi) + b) &lt;= 0:\n                w += eta * yi * xi\n                b += eta * yi\n                errors += 1\n        # Calcula acur\u00e1cia ap\u00f3s a \u00e9poca\n        preds = np.sign(np.dot(X, w) + b)\n        acc = np.mean(preds == y)\n        accuracies.append(acc)\n        if errors == 0:\n            break\n    return w, b, accuracies\n\ndef perceptron_train_multiple(X, y, runs=5, eta=0.01, max_epochs=100, shuffle=True):\n    \"\"\"\n    Executa o perceptron_train v\u00e1rias vezes e retorna:\n    - lista de resultados por run\n    - melhor run\n    - m\u00e9dia das acur\u00e1cias finais\n    - desvio das acur\u00e1cias finais\n    \"\"\"\n    resultados = []\n    n = len(X)\n\n    for _ in range(runs):\n        if shuffle:\n            idx = np.random.permutation(n)\n            Xr, yr = X[idx], y[idx]\n        else:\n            Xr, yr = X, y\n\n        w, b, acc = perceptron_train(Xr, yr, eta=eta, max_epochs=max_epochs)\n        resultados.append({\n            \"w\": w,\n            \"b\": b,\n            \"acc_final\": acc[-1],\n            \"epocas\": len(acc),\n            \"acc_hist\": acc\n        })\n\n    melhor = max(resultados, key=lambda d: d[\"acc_final\"])\n    media = float(np.mean([d[\"acc_final\"] for d in resultados]))\n    desvio = float(np.std([d[\"acc_final\"] for d in resultados]))\n    return resultados, melhor, media, desvio\n\n# Fun\u00e7\u00e3o para plotar dados e fronteira de decis\u00e3o\ndef plot_decision_boundary(X, y, w, b, title=\"Dados e fronteira de decis\u00e3o\"):\n    plt.figure(figsize=(6, 6))\n\n    # Separa classes para plotar com cores diferentes\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n\n    # Fronteira de decis\u00e3o: w0 * x + w1 * y + b = 0 =&gt; y = -(w0/w1) * x - b / w1\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n# Fun\u00e7\u00e3o para plotar acur\u00e1cia por \u00e9poca\ndef plot_accuracy(accuracies, title=\"Acur\u00e1cia por \u00e9poca\"):\n    plt.figure(figsize=(6, 4))\n    epochs = np.arange(1, len(accuracies) + 1)\n    plt.plot(epochs, accuracies, marker='o')\n    plt.xlabel('\u00c9poca')\n    plt.ylabel('Acur\u00e1cia')\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n# Destaque dos pontos mal classificados\ndef plot_misclassified(X, y, w, b):\n    preds = np.sign(np.dot(X, w) + b)\n    misclassified = X[preds != y]\n    plt.figure(figsize=(6, 6))\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow',\n                    s=80, label='Misclassificados')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title('Exerc\u00edcio 1: Pontos Mal Classificados')\n    plt.grid(True)\n    plt.show()\n</pre> ############################# #######---- UTILS ----####### #############################  import numpy as np import matplotlib.pyplot as plt  # Fun\u00e7\u00e3o para gerar os dados np.random.seed(42)  # Para reprodutibilidade  def generate_data(mean0, cov0, mean1, cov1, n_samples=1000):     class0 = np.random.multivariate_normal(mean0, cov0, n_samples)     class1 = np.random.multivariate_normal(mean1, cov1, n_samples)     X = np.vstack((class0, class1))     y = np.hstack((np.full(n_samples, -1), np.full(n_samples, 1)))     return X, y  # Treinamento do perceptron def perceptron_train(X, y, eta=0.01, max_epochs=100):     w = np.zeros(X.shape[1])     b = 0.0     accuracies = []     for epoch in range(max_epochs):         errors = 0         for xi, yi in zip(X, y):             if yi * (np.dot(w, xi) + b) &lt;= 0:                 w += eta * yi * xi                 b += eta * yi                 errors += 1         # Calcula acur\u00e1cia ap\u00f3s a \u00e9poca         preds = np.sign(np.dot(X, w) + b)         acc = np.mean(preds == y)         accuracies.append(acc)         if errors == 0:             break     return w, b, accuracies  def perceptron_train_multiple(X, y, runs=5, eta=0.01, max_epochs=100, shuffle=True):     \"\"\"     Executa o perceptron_train v\u00e1rias vezes e retorna:     - lista de resultados por run     - melhor run     - m\u00e9dia das acur\u00e1cias finais     - desvio das acur\u00e1cias finais     \"\"\"     resultados = []     n = len(X)      for _ in range(runs):         if shuffle:             idx = np.random.permutation(n)             Xr, yr = X[idx], y[idx]         else:             Xr, yr = X, y          w, b, acc = perceptron_train(Xr, yr, eta=eta, max_epochs=max_epochs)         resultados.append({             \"w\": w,             \"b\": b,             \"acc_final\": acc[-1],             \"epocas\": len(acc),             \"acc_hist\": acc         })      melhor = max(resultados, key=lambda d: d[\"acc_final\"])     media = float(np.mean([d[\"acc_final\"] for d in resultados]))     desvio = float(np.std([d[\"acc_final\"] for d in resultados]))     return resultados, melhor, media, desvio  # Fun\u00e7\u00e3o para plotar dados e fronteira de decis\u00e3o def plot_decision_boundary(X, y, w, b, title=\"Dados e fronteira de decis\u00e3o\"):     plt.figure(figsize=(6, 6))      # Separa classes para plotar com cores diferentes     class0 = X[y == -1]     class1 = X[y == 1]     plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)     plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)      # Fronteira de decis\u00e3o: w0 * x + w1 * y + b = 0 =&gt; y = -(w0/w1) * x - b / w1     if w[1] != 0:         x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)         y_values = -(w[0] / w[1]) * x_values - b / w[1]         plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')      plt.xlabel('x1')     plt.ylabel('x2')     plt.legend()     plt.title(title)     plt.grid(True)     plt.show()  # Fun\u00e7\u00e3o para plotar acur\u00e1cia por \u00e9poca def plot_accuracy(accuracies, title=\"Acur\u00e1cia por \u00e9poca\"):     plt.figure(figsize=(6, 4))     epochs = np.arange(1, len(accuracies) + 1)     plt.plot(epochs, accuracies, marker='o')     plt.xlabel('\u00c9poca')     plt.ylabel('Acur\u00e1cia')     plt.title(title)     plt.grid(True)     plt.show()  # Destaque dos pontos mal classificados def plot_misclassified(X, y, w, b):     preds = np.sign(np.dot(X, w) + b)     misclassified = X[preds != y]     plt.figure(figsize=(6, 6))     class0 = X[y == -1]     class1 = X[y == 1]     plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)     plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)     if w[1] != 0:         x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)         y_values = -(w[0] / w[1]) * x_values - b / w[1]         plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')     if len(misclassified) &gt; 0:         plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow',                     s=80, label='Misclassificados')     plt.xlabel('x1')     plt.ylabel('x2')     plt.legend()     plt.title('Exerc\u00edcio 1: Pontos Mal Classificados')     plt.grid(True)     plt.show()  In\u00a0[53]: Copied! <pre># Par\u00e2metros para o Exerc\u00edcio 1\nmean0_ex1 = np.array([1.5, 1.5])\ncov0_ex1 = np.array([[0.5, 0], [0, 0.5]])\nmean1_ex1 = np.array([5, 5])\ncov1_ex1 = np.array([[0.5, 0], [0, 0.5]])\n\n# Gera\u00e7\u00e3o dos dados\nX1, y1 = generate_data(mean0_ex1, cov0_ex1, mean1_ex1, cov1_ex1, n_samples=1000)\n\n# Treinamento do perceptron\nw1, b1, acc1 = perceptron_train(X1, y1, eta=0.01, max_epochs=100)\n</pre> # Par\u00e2metros para o Exerc\u00edcio 1 mean0_ex1 = np.array([1.5, 1.5]) cov0_ex1 = np.array([[0.5, 0], [0, 0.5]]) mean1_ex1 = np.array([5, 5]) cov1_ex1 = np.array([[0.5, 0], [0, 0.5]])  # Gera\u00e7\u00e3o dos dados X1, y1 = generate_data(mean0_ex1, cov0_ex1, mean1_ex1, cov1_ex1, n_samples=1000)  # Treinamento do perceptron w1, b1, acc1 = perceptron_train(X1, y1, eta=0.01, max_epochs=100) In\u00a0[54]: Copied! <pre># Resultados do Exerc\u00edcio 1\nprint(\"Pesos finais (Exerc\u00edcio 1):\", w1)\nprint(\"Vi\u00e9s final (Exerc\u00edcio 1):\", b1)\nprint(\"Acur\u00e1cia final (Exerc\u00edcio 1):\", acc1[-1])\nprint(\"\u00c9pocas executadas (Exerc\u00edcio 1):\", len(acc1))\n\n# Plot dos dados e da fronteira de decis\u00e3o\nplot_decision_boundary(X1, y1, w1, b1, title=\"Exerc\u00edcio 1: Dados e Fronteira de Decis\u00e3o\")\n\n# Plot da acur\u00e1cia por \u00e9poca\nplot_accuracy(acc1, title=\"Exerc\u00edcio 1: Acur\u00e1cia por \u00c9poca\")\n\n# Fun\u00e7\u00e3o para destacar pontos mal classificados\nplot_misclassified(X1, y1, w1, b1)\n</pre> # Resultados do Exerc\u00edcio 1 print(\"Pesos finais (Exerc\u00edcio 1):\", w1) print(\"Vi\u00e9s final (Exerc\u00edcio 1):\", b1) print(\"Acur\u00e1cia final (Exerc\u00edcio 1):\", acc1[-1]) print(\"\u00c9pocas executadas (Exerc\u00edcio 1):\", len(acc1))  # Plot dos dados e da fronteira de decis\u00e3o plot_decision_boundary(X1, y1, w1, b1, title=\"Exerc\u00edcio 1: Dados e Fronteira de Decis\u00e3o\")  # Plot da acur\u00e1cia por \u00e9poca plot_accuracy(acc1, title=\"Exerc\u00edcio 1: Acur\u00e1cia por \u00c9poca\")  # Fun\u00e7\u00e3o para destacar pontos mal classificados plot_misclassified(X1, y1, w1, b1) <pre>Pesos finais (Exerc\u00edcio 1): [0.01985622 0.01711828]\nVi\u00e9s final (Exerc\u00edcio 1): -0.11999999999999998\nAcur\u00e1cia final (Exerc\u00edcio 1): 1.0\n\u00c9pocas executadas (Exerc\u00edcio 1): 12\n</pre> In\u00a0[55]: Copied! <pre># Par\u00e2metros para o Exerc\u00edcio 2\nmean0_ex2 = np.array([3, 3])\ncov0_ex2 = np.array([[1.5, 0], [0, 1.5]])\nmean1_ex2 = np.array([4, 4])\ncov1_ex2 = np.array([[1.5, 0], [0, 1.5]])\n\n# Gera\u00e7\u00e3o dos dados\nX2, y2 = generate_data(mean0_ex2, cov0_ex2, mean1_ex2, cov1_ex2, n_samples=1000)\n\n# Treinamento do perceptron\nw2, b2, acc2 = perceptron_train(X2, y2, eta=0.01, max_epochs=100)\n\nprint(\"Pesos finais (Exerc\u00edcio 2):\", w2)\nprint(\"Vi\u00e9s final (Exerc\u00edcio 2):\", b2)\nprint(\"Acur\u00e1cia final (Exerc\u00edcio 2):\", acc2[-1])\nprint(\"\u00c9pocas executadas (Exerc\u00edcio 2):\", len(acc2))\n\n# Plot dos dados e da fronteira de decis\u00e3o\nplot_decision_boundary(X2, y2, w2, b2, title=\"Exerc\u00edcio 2: Dados e Fronteira de Decis\u00e3o\")\n\n# Plot da acur\u00e1cia por \u00e9poca\nplot_accuracy(acc2, title=\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca\")\n\n# Pontos mal classificados\ndef plot_misclassified2(X, y, w, b):\n    preds = np.sign(np.dot(X, w) + b)\n    misclassified = X[preds != y]\n    plt.figure(figsize=(6, 6))\n    class0 = X[y == -1]\n    class1 = X[y == 1]\n    plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)\n    plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)\n    if w[1] != 0:\n        x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)\n        y_values = -(w[0] / w[1]) * x_values - b / w[1]\n        plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')\n    if len(misclassified) &gt; 0:\n        plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow', s=80, label='Misclassificados')\n    plt.xlabel('x1')\n    plt.ylabel('x2')\n    plt.legend()\n    plt.title('Exerc\u00edcio 2: Pontos Mal Classificados')\n    plt.grid(True)\n    plt.show()\n\nplot_misclassified2(X2, y2, w2, b2)\n</pre> # Par\u00e2metros para o Exerc\u00edcio 2 mean0_ex2 = np.array([3, 3]) cov0_ex2 = np.array([[1.5, 0], [0, 1.5]]) mean1_ex2 = np.array([4, 4]) cov1_ex2 = np.array([[1.5, 0], [0, 1.5]])  # Gera\u00e7\u00e3o dos dados X2, y2 = generate_data(mean0_ex2, cov0_ex2, mean1_ex2, cov1_ex2, n_samples=1000)  # Treinamento do perceptron w2, b2, acc2 = perceptron_train(X2, y2, eta=0.01, max_epochs=100)  print(\"Pesos finais (Exerc\u00edcio 2):\", w2) print(\"Vi\u00e9s final (Exerc\u00edcio 2):\", b2) print(\"Acur\u00e1cia final (Exerc\u00edcio 2):\", acc2[-1]) print(\"\u00c9pocas executadas (Exerc\u00edcio 2):\", len(acc2))  # Plot dos dados e da fronteira de decis\u00e3o plot_decision_boundary(X2, y2, w2, b2, title=\"Exerc\u00edcio 2: Dados e Fronteira de Decis\u00e3o\")  # Plot da acur\u00e1cia por \u00e9poca plot_accuracy(acc2, title=\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca\")  # Pontos mal classificados def plot_misclassified2(X, y, w, b):     preds = np.sign(np.dot(X, w) + b)     misclassified = X[preds != y]     plt.figure(figsize=(6, 6))     class0 = X[y == -1]     class1 = X[y == 1]     plt.scatter(class0[:, 0], class0[:, 1], color='blue', label='Classe -1', alpha=0.5)     plt.scatter(class1[:, 0], class1[:, 1], color='red', label='Classe +1', alpha=0.5)     if w[1] != 0:         x_values = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 200)         y_values = -(w[0] / w[1]) * x_values - b / w[1]         plt.plot(x_values, y_values, 'k--', label='Fronteira de decis\u00e3o')     if len(misclassified) &gt; 0:         plt.scatter(misclassified[:, 0], misclassified[:, 1], facecolors='none', edgecolors='yellow', s=80, label='Misclassificados')     plt.xlabel('x1')     plt.ylabel('x2')     plt.legend()     plt.title('Exerc\u00edcio 2: Pontos Mal Classificados')     plt.grid(True)     plt.show()  plot_misclassified2(X2, y2, w2, b2) <pre>Pesos finais (Exerc\u00edcio 2): [0.03621038 0.03127034]\nVi\u00e9s final (Exerc\u00edcio 2): -0.019999999999999997\nAcur\u00e1cia final (Exerc\u00edcio 2): 0.5005\n\u00c9pocas executadas (Exerc\u00edcio 2): 100\n</pre> In\u00a0[\u00a0]: Copied! <pre># Exerc\u00edcio 2 Multi Run\nruns = 5\nresultados, melhor, media, desvio = perceptron_train_multiple(\n    X2, y2, runs=runs, eta=0.01, max_epochs=100, shuffle=True\n)\n\nprint(f\"Exerc\u00edcio 2 - Multi Run:\\n  Melhor acur\u00e1cia final: {melhor['acc_final']:.4f}\\n  M\u00e9dia: {media:.4f}\\n  Desvio: {desvio:.4f}\")\nprint(\"\")\nprint(\"Pesos do melhor:\", melhor[\"w\"])\nprint(\"\")\nprint(\"Vi\u00e9s do melhor:\", melhor[\"b\"])\nprint(\"\")\nprint(\"\u00c9pocas do melhor:\", melhor[\"epocas\"])\n\n# Curvas de acur\u00e1cia de todas as execu\u00e7\u00f5es\nplt.figure(figsize=(6,4))\nfor i, s in enumerate(resultados, start=1):\n    plt.plot(range(1, len(s[\"acc_hist\"])+1), s[\"acc_hist\"], alpha=0.8, label=f\"Run {i}\")\nplt.title(\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca - 5 Execu\u00e7\u00f5es\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.grid(True); plt.legend()\nplt.show()\n\n# Dados com fronteira do melhor e mal classificados\nw_best, b_best = melhor[\"w\"], melhor[\"b\"]\npreds_best = np.where(X2 @ w_best + b_best &gt;= 0.0, 1, -1)\nmis = preds_best != y2\n\nplt.figure(figsize=(6,6))\nplt.scatter(X2[y2==-1,0], X2[y2==-1,1], s=10, label=\"Classe 0 (-1)\", alpha=0.5)\nplt.scatter(X2[y2== 1,0], X2[y2== 1,1], s=10, label=\"Classe 1 (+1)\", alpha=0.5)\n\nif abs(w_best[1]) &gt; 1e-12:\n    xs = np.linspace(X2[:,0].min()-1, X2[:,0].max()+1, 200)\n    ys = -(w_best[0]/w_best[1]) * xs - b_best / w_best[1]\n    plt.plot(xs, ys, linestyle=\"--\", linewidth=2, label=\"Fronteira do Melhor\")\n\nplt.scatter(X2[mis,0], X2[mis,1], s=40, facecolors=\"none\", edgecolors=\"k\", label=\"Mal Classificados\")\n\nplt.title(\"Exerc\u00edcio 2: Dados e Fronteira do Melhor\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend()\nplt.show()\n</pre> # Exerc\u00edcio 2 Multi Run runs = 5 resultados, melhor, media, desvio = perceptron_train_multiple(     X2, y2, runs=runs, eta=0.01, max_epochs=100, shuffle=True )  print(f\"Exerc\u00edcio 2 - Multi Run:\\n  Melhor acur\u00e1cia final: {melhor['acc_final']:.4f}\\n  M\u00e9dia: {media:.4f}\\n  Desvio: {desvio:.4f}\") print(\"\") print(\"Pesos do melhor:\", melhor[\"w\"]) print(\"\") print(\"Vi\u00e9s do melhor:\", melhor[\"b\"]) print(\"\") print(\"\u00c9pocas do melhor:\", melhor[\"epocas\"])  # Curvas de acur\u00e1cia de todas as execu\u00e7\u00f5es plt.figure(figsize=(6,4)) for i, s in enumerate(resultados, start=1):     plt.plot(range(1, len(s[\"acc_hist\"])+1), s[\"acc_hist\"], alpha=0.8, label=f\"Run {i}\") plt.title(\"Exerc\u00edcio 2: Acur\u00e1cia por \u00c9poca - 5 Execu\u00e7\u00f5es\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.grid(True); plt.legend() plt.show()  # Dados com fronteira do melhor e mal classificados w_best, b_best = melhor[\"w\"], melhor[\"b\"] preds_best = np.where(X2 @ w_best + b_best &gt;= 0.0, 1, -1) mis = preds_best != y2  plt.figure(figsize=(6,6)) plt.scatter(X2[y2==-1,0], X2[y2==-1,1], s=10, label=\"Classe 0 (-1)\", alpha=0.5) plt.scatter(X2[y2== 1,0], X2[y2== 1,1], s=10, label=\"Classe 1 (+1)\", alpha=0.5)  if abs(w_best[1]) &gt; 1e-12:     xs = np.linspace(X2[:,0].min()-1, X2[:,0].max()+1, 200)     ys = -(w_best[0]/w_best[1]) * xs - b_best / w_best[1]     plt.plot(xs, ys, linestyle=\"--\", linewidth=2, label=\"Fronteira do Melhor\")  plt.scatter(X2[mis,0], X2[mis,1], s=40, facecolors=\"none\", edgecolors=\"k\", label=\"Mal Classificados\")  plt.title(\"Exerc\u00edcio 2: Dados e Fronteira do Melhor\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.grid(True); plt.legend() plt.show()  <pre>Exerc\u00edcio 2 - Multi Run:\n  Melhor acur\u00e1cia final: 0.7205\n  M\u00e9dia: 0.6303\n  Desvio: 0.0771\n\nPesos do melhor: [0.07839785 0.06000151]\n\nVi\u00e9s do melhor: -0.4700000000000002\n\n\u00c9pocas do melhor: 100\n</pre>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#atividade-de-perceptron","title":"Atividade de Perceptron\u00b6","text":"<p>Este notebook aborda a implementa\u00e7\u00e3o do algoritmo Perceptron para classifica\u00e7\u00e3o bin\u00e1ria de dados 2D. O objetivo \u00e9 gerar dados sint\u00e9ticos, implementar o perceptron do zero utilizando apenas NumPy para opera\u00e7\u00f5es matriciais, treinar o modelo, visualizar os resultados e discutir o desempenho em dois cen\u00e1rios distintos.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#o-que-e-um-perceptron","title":"O que \u00e9 um Perceptron?\u00b6","text":"<p>O perceptron \u00e9 um dos modelos mais simples de rede neural artificial. Consiste em um \u00fanico neur\u00f4nio que recebe um vetor de entrada $x$, calcula um somat\u00f3rio ponderado com pesos $w$ e adiciona um termo de vi\u00e9s $b$. A sa\u00edda \u00e9 obtida atrav\u00e9s de uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o de degrau (fun\u00e7\u00e3o sinal):</p> <p>$$ \\text{output} = \\begin{cases}  1 &amp; \\text{se } w \\cdot x + b &gt; 0 \\\\  -1 &amp; \\text{caso contr\u00e1rio} \\end{cases} $$</p> <p>Esse modelo \u00e9 capaz de separar linearmente duas classes de dados. Sua import\u00e2ncia hist\u00f3rica reside no fato de que ele pavimentou o caminho para redes neurais mais complexas, embora seja limitado a problemas linearmente separ\u00e1veis.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#detalhes-da-implementacao","title":"Detalhes da Implementa\u00e7\u00e3o\u00b6","text":"<ul> <li>Bibliotecas utilizadas: apenas <code>numpy</code> para c\u00e1lculos matriciais e <code>matplotlib</code> para visualiza\u00e7\u00e3o.</li> <li>Gera\u00e7\u00e3o de dados: para cada exerc\u00edcio, geramos dois conjuntos de amostras 2D (1000 amostras por classe) usando distribui\u00e7\u00f5es normais multivariadas com m\u00e9dias e matrizes de covari\u00e2ncia especificadas.</li> <li>Representa\u00e7\u00e3o das classes: as classes s\u00e3o rotuladas como $-1$ e $+1$ para compatibilidade com a regra de aprendizado do perceptron.</li> <li>Regra de atualiza\u00e7\u00e3o (Perceptron Learning Rule):<ul> <li>Para cada amostra mal classificada $(x_i, y_i)$:<ul> <li>$w \\leftarrow w + \\eta \\cdot y_i \\cdot x_i$</li> <li>$b \\leftarrow b + \\eta \\cdot y_i$</li> </ul> </li> <li>A taxa de aprendizado ($\\eta$) utilizada \u00e9 $0{,}01$.</li> </ul> </li> <li>Crit\u00e9rio de parada: treinamento at\u00e9 convergir (nenhuma atualiza\u00e7\u00e3o em uma \u00e9poca completa) ou at\u00e9 100 \u00e9pocas.</li> <li>Visualiza\u00e7\u00f5es:<ul> <li>Scatter plot dos dados com cores distintas para cada classe.</li> <li>Linha que representa a fronteira de decis\u00e3o $w \\cdot x + b = 0$ ap\u00f3s o treinamento.</li> <li>Gr\u00e1fico de acur\u00e1cia por \u00e9poca para acompanhar a converg\u00eancia.</li> <li>Destaque dos pontos mal classificados ap\u00f3s o treinamento.</li> </ul> </li> </ul>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#exercicio-1-dados-quase-linearmente-separaveis","title":"Exerc\u00edcio 1: Dados quase linearmente separ\u00e1veis\u00b6","text":"<p>Geramos dois conjuntos de dados 2D com 1000 amostras cada. As amostras da Classe 0 t\u00eam m\u00e9dia $[1{,}5, 1{,}5]$ e matriz de covari\u00e2ncia $\\begin{pmatrix}0{,}5 &amp; 0\\\\0 &amp; 0{,}5\\end{pmatrix}$. As amostras da Classe 1 t\u00eam m\u00e9dia $[5, 5]$ e a mesma matriz de covari\u00e2ncia. Esses par\u00e2metros produzem dados com sobreposi\u00e7\u00e3o m\u00ednima, facilitando a separa\u00e7\u00e3o linear. O c\u00f3digo abaixo gera os dados, treina o perceptron e visualiza os resultados.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#analise-dos-resultados-exercicio-1","title":"An\u00e1lise dos Resultados (Exerc\u00edcio\u00a01)\u00b6","text":"<p>O treinamento do perceptron convergiu em 12 \u00e9pocas, o que ainda representa um n\u00famero baixo de itera\u00e7\u00f5es. O modelo atingiu 100\u202f% de acur\u00e1cia, refletindo que os dados s\u00e3o de fato quase linearmente separ\u00e1veis. Os pesos finais foram aproximadamente $[0{,}0199,\\;0{,}0171]$ e o vi\u00e9s $-0{,}12$, par\u00e2metros que definem a fronteira de decis\u00e3o mostrada nos gr\u00e1ficos.</p> <p>Esses resultados confirmam que, com m\u00e9dias bem afastadas $([1{,}5,\\;1{,}5] \\text{ e } [5,\\;5])$ e baixa vari\u00e2ncia ($0{,}5$ em cada dimens\u00e3o, sem covari\u00e2ncia), h\u00e1 pouqu\u00edssima sobreposi\u00e7\u00e3o entre as classes. Isso permite ao perceptron encontrar rapidamente uma reta que separa perfeitamente os pontos das duas classes. A linha tracejada representando a fronteira de decis\u00e3o divide corretamente os clusters azul e vermelho, sem pontos mal classificados.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#exercicio-2-dados-com-maior-sobreposicao","title":"Exerc\u00edcio 2: Dados com maior sobreposi\u00e7\u00e3o\u00b6","text":"<p>Neste cen\u00e1rio, as duas classes t\u00eam m\u00e9dias [3, 3] e [4, 4], e a matriz de covari\u00e2ncia \u00e9 $\\begin{pmatrix}1{,}5 &amp; 0\\\\0 &amp; 1{,}5\\end{pmatrix}$ para ambas. As m\u00e9dias s\u00e3o mais pr\u00f3ximas e a vari\u00e2ncia maior, gerando maior sobreposi\u00e7\u00e3o entre as classes.</p> <p>O c\u00f3digo a seguir repete o processo de gera\u00e7\u00e3o, treinamento e visualiza\u00e7\u00e3o para esse conjunto de dados.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#analise-dos-resultados-exercicio-2-single-run","title":"An\u00e1lise dos Resultados (Exerc\u00edcio\u202f2) - Single Run\u00b6","text":"<p>O segundo conjunto de dados tem classes com m\u00e9dias muito pr\u00f3ximas e vari\u00e2ncia alta, o que faz os pontos azuis e vermelhos se misturarem bastante. O perceptron calcula uma reta a partir dos pesos finais $[0{,}036,\\; 0{,}031]$ e vi\u00e9s $-0{,}02$, mas essa linha passa pelo meio do aglomerado e n\u00e3o separa bem as classes. Mesmo ap\u00f3s 100 \u00e9pocas a acur\u00e1cia fica em torno de 50\u202f%, praticamente um palpite aleat\u00f3rio, e o gr\u00e1fico mostra oscila\u00e7\u00f5es sem melhora. Esses resultados mostram que o perceptron n\u00e3o \u00e9 adequado para dados com grande sobreposi\u00e7\u00e3o; seria necess\u00e1rio usar um modelo mais poderoso, como uma rede de m\u00faltiplas camadas ou uma SVM com kernel, para aprender uma fronteira de decis\u00e3o mais complexa.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#analise-dos-resultados-exercicio-2-multi-run","title":"An\u00e1lise dos Resultados Exerc\u00edcio 2 - Multi Run\u00b6","text":"<p>Os cinco treinamentos produziram melhor acur\u00e1cia de 0.7205, m\u00e9dia de 0.6303 e desvio de 0.0771. H\u00e1 varia\u00e7\u00e3o real entre execu\u00e7\u00f5es, o que \u00e9 esperado com dados muito sobrepostos e com embaralhamento da ordem das amostras.</p> <p>O melhor modelo rodou at\u00e9 100 \u00e9pocas e n\u00e3o convergiu, pois sempre restam pontos conflitantes. Os pesos foram [0.0784, 0.0600] e o vi\u00e9s \u22120.47, definindo uma reta inclinada que separa apenas parte dos dados. As curvas de acur\u00e1cia mostram duas execu\u00e7\u00f5es estabilizando perto de 0.71 e outras em torno de 0.55 a 0.65, coerentes com a m\u00e9dia reportada. O gr\u00e1fico do melhor run exibe muitos erros na regi\u00e3o central, confirmando a forte sobreposi\u00e7\u00e3o entre as classes.</p> <p>Conclus\u00e3o: o perceptron linear encontra uma fronteira razo\u00e1vel em algumas execu\u00e7\u00f5es, mas n\u00e3o atinge perfei\u00e7\u00e3o neste cen\u00e1rio. O teto observado perto de setenta por cento decorre da n\u00e3o separabilidade linear do conjunto. Para superar esse limite, seria necess\u00e1rio um modelo com capacidade n\u00e3o linear, como uma rede multicamadas ou uma SVM com kernel.</p>"},{"location":"portfolio/neural-networks/exercises/2/ex2_pedrotpc/#referencias","title":"Refer\u00eancias\u00b6","text":"<ul> <li>Enunciado da Atividade: consulte o documento disponibilizado pelo curso (ANNDL - Perceptron).</li> </ul>"},{"location":"portfolio/neural-networks/exercises/2/main/","title":"Exercise 2","text":""},{"location":"portfolio/neural-networks/exercises/2/main/#exercise-2","title":"Exercise 2","text":"<p>Content coming soon.</p>"},{"location":"portfolio/neural-networks/exercises/3/","title":"3. MLP","text":""},{"location":"portfolio/neural-networks/exercises/3/#exercicio-de-mlp-pedrotpc","title":"Exerc\u00edcio de MLP - pedrotpc","text":""},{"location":"portfolio/neural-networks/exercises/3/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo manual de uma MLP","text":"<p>Considere uma MLP simples com duas entradas, uma camada oculta com dois neur\u00f4nios e um neur\u00f4nio de sa\u00edda. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada oculta e da sa\u00edda \u00e9 a tangente hiperb\u00f3lica. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): \\(L = \\frac{1}{2}(y - \\hat{y})^2\\), onde \\(\\hat{y}\\) \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos:</p> <ul> <li> <p>Entradas e sa\u00edda desejada: \\(x = [0{,}5,\\,-0{,}2]\\), \\(y=1{,}0\\).</p> </li> <li> <p>Pesos da camada oculta:</p> </li> </ul> <p>$$   W^{(1)} = \\begin{bmatrix}   0{,}3 &amp; -0{,}1 \\   0{,}2 &amp; 0{,}4   \\end{bmatrix}   $$</p> <ul> <li> <p>Vieses da camada oculta: \\(b^{(1)} = [0{,}1,\\,-0{,}2]\\).</p> </li> <li> <p>Pesos da camada de sa\u00edda: \\(W^{(2)} = [0{,}5,\\,-0{,}3]\\).</p> </li> <li> <p>Vi\u00e9s da camada de sa\u00edda: \\(b^{(2)} = 0{,}2\\).</p> </li> <li> <p>Taxa de aprendizado: \\(\\eta = 0{,}1\\).</p> </li> </ul> <p>O objetivo \u00e9 calcular passo a passo o percurso pelo grafo computacional: pr\u00e9 ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, perda, gradientes de todos os pesos e vieses, e as atualiza\u00e7\u00f5es de par\u00e2metros usando descida de gradiente.</p>"},{"location":"portfolio/neural-networks/exercises/3/#passo-1-passagem-direta","title":"Passo 1 \u2013 Passagem direta","text":""},{"location":"portfolio/neural-networks/exercises/3/#pre-ativacoes-da-camada-oculta","title":"Pr\u00e9 ativa\u00e7\u00f5es da camada oculta:","text":"\\[ z^{(1)} = W^{(1)} x + b^{(1)} \\] <p>Calculando:</p> \\[ z^{(1)} = \\begin{bmatrix}0{,}3 &amp; -0{,}1 \\\\ 0{,}2 &amp; 0{,}4\\end{bmatrix}\\cdot\\begin{bmatrix}0{,}5 \\\\ -0{,}2\\end{bmatrix}+\\begin{bmatrix}0{,}1 \\\\ -0{,}2\\end{bmatrix} = \\begin{bmatrix}0{,}27 \\\\ -0{,}18\\end{bmatrix}. \\]"},{"location":"portfolio/neural-networks/exercises/3/#ativacoes-da-camada-oculta-usa-a-tangente-hiperbolica-tanhz-para-cada-elemento","title":"Ativa\u00e7\u00f5es da camada oculta: usa a tangente hiperb\u00f3lica \\(\\tanh(z)\\). Para cada elemento:","text":"\\[ a^{(1)}_1 = \\tanh(0{,}27) \\approx 0{,}2636, \\quad a^{(1)}_2 = \\tanh(-0{,}18) \\approx -0{,}1781. \\] <p>Portanto \\(a^{(1)} = [0{,}2636,\\,-0{,}1781]\\).</p>"},{"location":"portfolio/neural-networks/exercises/3/#pre-ativacao-da-saida","title":"Pr\u00e9 ativa\u00e7\u00e3o da sa\u00edda:","text":"\\[ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = [0{,}5,\\,-0{,}3]\\cdot\\begin{bmatrix}0{,}2636 \\\\ -0{,}1781\\end{bmatrix}+0{,}2 \\approx 0{,}3852. \\]"},{"location":"portfolio/neural-networks/exercises/3/#saida-final-haty-tanhz2-como-z2-approx-03852-entao","title":"Sa\u00edda final: \\(\\hat{y} = \\tanh(z^{(2)})\\). Como \\(z^{(2)} \\approx 0{,}3852\\), ent\u00e3o","text":"\\[ \\hat{y} = \\tanh(0{,}3852) \\approx 0{,}3672. \\]"},{"location":"portfolio/neural-networks/exercises/3/#passo-2-calculo-da-perda","title":"Passo 2 \u2013 C\u00e1lculo da perda","text":"<p>A perda \u00e9 o erro quadr\u00e1tico m\u00e9dio para uma \u00fanica amostra:</p> \\[ L = \\frac{1}{2}(y - \\hat{y})^2 = \\frac{1}{2}(1{,}0 - 0{,}3672)^2 \\approx 0{,}2002. \\]"},{"location":"portfolio/neural-networks/exercises/3/#passo-3-retropropagacao","title":"Passo 3 \u2013 Retropropaga\u00e7\u00e3o","text":"<p>Para atualizar os par\u00e2metros, calcula se o gradiente da perda em rela\u00e7\u00e3o a cada peso e vi\u00e9s.</p>"},{"location":"portfolio/neural-networks/exercises/3/#gradiente-no-neuronio-de-saida","title":"Gradiente no neur\u00f4nio de sa\u00edda:","text":"<ul> <li>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda predita: \\(\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y\\).</li> <li>Derivada da tangente hiperb\u00f3lica: \\(\\frac{\\mathrm{d}}{\\mathrm{d}z} \\tanh(z) = 1 - \\tanh^2(z)\\).</li> </ul> <p>Assim, o erro na sa\u00edda (\\(\\delta^{(2)}\\)) \u00e9</p> \\[ \\delta^{(2)} = (\\hat{y} - y) \\cdot \\bigl(1 - \\hat{y}^2\\bigr) \\approx (0{,}3672 - 1{,}0)\\cdot (1 - 0{,}3672^2) \\approx -0{,}5474. \\] <ul> <li>Gradiente dos pesos da sa\u00edda:</li> </ul> \\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)}\\,a^{(1)} \\approx -0{,}5474 \\times [0{,}2636,\\,-0{,}1781] \\approx [-0{,}1443,\\,0{,}0975]. \\] <ul> <li>Gradiente do vi\u00e9s da sa\u00edda: \\(\\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} \\approx -0{,}5474\\).</li> </ul>"},{"location":"portfolio/neural-networks/exercises/3/#propagacao-para-a-camada-oculta","title":"Propaga\u00e7\u00e3o para a camada oculta:","text":"<p>O erro em cada neur\u00f4nio oculto (\\(\\delta^{(1)}\\)) \u00e9 obtido multiplicando \\(\\delta^{(2)}\\) pelos pesos da sa\u00edda e pela derivada da tangente hiperb\u00f3lica nos neur\u00f4nios ocultos:</p> \\[ \\delta^{(1)} = \\bigl(\\delta^{(2)}\\,W^{(2)}\\bigr) \\circ \\bigl(1 - (a^{(1)})^2\\bigr), \\] <p>onde \\(\\circ\\) indica produto elemento a elemento. Assim:</p> \\[ \\delta^{(1)} \\approx [-0{,}2547,\\,0{,}1590]. \\] <ul> <li>Gradiente dos pesos da camada oculta:</li> </ul> \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)}\\,x^\\top \\approx \\begin{bmatrix} -0{,}1273 &amp; 0{,}0509 \\\\ 0{,}0795 &amp; -0{,}0318 \\end{bmatrix}. \\] <ul> <li>Gradiente dos vieses da camada oculta: \\(\\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} \\approx [-0{,}2547,\\,0{,}1590].\\)</li> </ul>"},{"location":"portfolio/neural-networks/exercises/3/#passo-4-atualizacao-dos-parametros","title":"Passo 4 \u2013 Atualiza\u00e7\u00e3o dos par\u00e2metros","text":"<p>A atualiza\u00e7\u00e3o usa descida de gradiente simples com taxa de aprendizado \\(\\eta = 0{,}1\\). Para cada par\u00e2metro \\(\\theta\\):</p> \\[ \\theta \\gets \\theta - \\eta\\,\\frac{\\partial L}{\\partial \\theta}. \\] <p>Calculando as novas vari\u00e1veis:</p> <p>Atualiza\u00e7\u00e3o de \\(W^{(2)}\\):</p> \\[ W^{(2)}_{\\text{novo}} = W^{(2)} - \\eta\\,\\frac{\\partial L}{\\partial W^{(2)}} = [0{,}5,\\,-0{,}3] - 0{,}1\\times [-0{,}1443,\\,0{,}0975] \\approx [0{,}5144,\\,-0{,}3097]. \\] <p>Atualiza\u00e7\u00e3o de \\(b^{(2)}\\):</p> \\[ b^{(2)}_{\\text{novo}} = 0{,}2 - 0{,}1\\times(-0{,}5474) \\approx 0{,}2547. \\] <p>Atualiza\u00e7\u00e3o de \\(W^{(1)}\\):</p> \\[ W^{(1)}_{\\text{novo}} = W^{(1)} - 0{,}1\\times \\begin{bmatrix} -0{,}1273 &amp; 0{,}0509 \\\\ 0{,}0795 &amp; -0{,}0318 \\end{bmatrix} = \\begin{bmatrix} 0{,}3 &amp; -0{,}1 \\\\ 0{,}2 &amp; 0{,}4 \\end{bmatrix} - 0{,}1\\times \\begin{bmatrix} -0{,}1273 &amp; 0{,}0509 \\\\ 0{,}0795 &amp; -0{,}0318 \\end{bmatrix} \\approx \\begin{bmatrix}0{,}3127 &amp; -0{,}1051\\\\0{,}1920 &amp; 0{,}4032\\end{bmatrix}. \\] <ul> <li>Atualiza\u00e7\u00e3o de \\(b^{(1)}\\):</li> </ul> \\[ b^{(1)}_{\\text{novo}} = b^{(1)} - 0{,}1\\times [-0{,}2547,\\,0{,}1590] \\approx [0{,}1255,\\,-0{,}2159]. \\] <p>Esses s\u00e3o os par\u00e2metros atualizados ap\u00f3s uma itera\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/3/#exercicio-2-classificacao-binaria-com-mlp-escrito-do-zero","title":"Exerc\u00edcio\u00a02: Classifica\u00e7\u00e3o bin\u00e1ria com MLP escrito do zero","text":"<p>Neste exerc\u00edcio gera\u2011se um conjunto de dados bidimensional com 1000 amostras. Uma classe possui um \u00fanico agrupamento e a outra possui dois agrupamentos; para conseguir isso, geramos subconjuntos separadamente e combinamos em um \u00fanico conjunto de dados. Em seguida, implementa\u2011se um perceptron multicamadas simples em NumPy para classificar esse conjunto de dados.</p> <p>Os passos s\u00e3o:</p> <ol> <li>Gerar os dados com <code>make_classification</code> e mesclar subconjuntos para obter um agrupamento para a classe\u00a00 e dois agrupamentos para a classe\u00a01.</li> <li>Dividir o conjunto em treino (80\u00a0%) e teste (20\u00a0%).</li> <li>Implementar um MLP com uma camada oculta usando fun\u00e7\u00e3o de ativa\u00e7\u00e3o \\(    anh\\) e sa\u00edda sigmoidal para estimar probabilidades de uma classe. A fun\u00e7\u00e3o de perda \u00e9 a entropia cruzada bin\u00e1ria.</li> <li>Treinar a rede por um n\u00famero fixo de \u00e9pocas, acompanhar a perda de treino e avaliar a acur\u00e1cia no conjunto de teste.</li> <li>Exibir uma visualiza\u00e7\u00e3o dos dados com a fronteira de decis\u00e3o aprendida.</li> </ol> <pre><code># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split</code></pre> <pre><code># 1. Gera\u00e7\u00e3o do conjunto de dados\n# Classe 0 com 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                             n_clusters_per_class=1, n_classes=1, class_sep=1.5,\n                             random_state=42)\n# Classe 1 com 2 clusters\nX1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                                  n_clusters_per_class=2, n_classes=1, class_sep=1.5,\n                                  random_state=24)\ny1 = np.ones_like(y1_temp)  # marca classe 1\n\n# Combina dados\nX_bin = np.vstack((X0, X1))\ny_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1</code></pre> <pre><code># 2. Divis\u00e3o treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)</code></pre> <pre><code># 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria\nclass SimpleMLPBinary:\n    def __init__(self, input_dim, hidden_dim, lr=0.01):\n        self.lr = lr\n        # inicializa\u00e7\u00e3o dos pesos com pequena escala\n        rng = np.random.default_rng(42)\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=hidden_dim)\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2 + self.b2   # (n,)\n        y_hat = self.sigmoid(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # entropia cruzada bin\u00e1ria\n        eps = 1e-12\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # derivada da perda em rela\u00e7\u00e3o ao z2\n        dz2 = y_hat - y_true  # (n,)\n        dW2 = (dz2 @ a1) / n   # (hidden,)\n        db2 = np.mean(dz2)\n\n        da1 = np.outer(dz2, self.W2)  # (n, hidden)\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = (dz1.T @ X) / n\n        db1 = dz1.mean(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            # opcional: imprimir a cada 50 \u00e9pocas\n            if (epoch + 1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return (y_hat &gt;= 0.5).astype(int)</code></pre> <pre><code># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")</code></pre> <pre><code>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</code></pre> <pre><code># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p></p>"},{"location":"portfolio/neural-networks/exercises/3/#analise-dos-resultados-do-exercicio-2","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a02","text":"<p>O modelo bin\u00e1rio foi treinado com uma camada oculta de quatro neur\u00f4nios e ativa\u00e7\u00e3o \\(\\tanh\\). Ap\u00f3s 200 \u00e9pocas, a perda de treino diminuiu de forma est\u00e1vel, indicando aprendizado. No conjunto de teste, a acur\u00e1cia t\u00edpica obtida foi superior a 0,9, o que demonstra que a rede consegue separar os clusters com boa precis\u00e3o. O gr\u00e1fico de perda por \u00e9poca ajuda a visualizar a converg\u00eancia da descida de gradiente.</p> <p>A visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostra que a rede aprendeu uma curva que separa os dois agrupamentos da classe\u00a01 do agrupamento \u00fanico da classe\u00a00. A maior parte dos pontos \u00e9 classificada corretamente, evidenciando que um MLP simples \u00e9 suficiente para resolver este problema bin\u00e1rio.</p>"},{"location":"portfolio/neural-networks/exercises/3/#exercicio-3-classificacao-multiclasse-com-mlp-reutilizavel","title":"Exerc\u00edcio\u00a03: Classifica\u00e7\u00e3o multiclasse com MLP reutiliz\u00e1vel","text":"<p>Agora gera\u2011se um conjunto de dados com tr\u00eas classes e quatro atributos. As classes s\u00e3o formadas por subconjuntos com n\u00fameros diferentes de agrupamentos: duas regi\u00f5es para a classe\u00a00, tr\u00eas para a classe\u00a01 e quatro para a classe\u00a02. Para alcan\u00e7ar isso, geramos cada classe separadamente com <code>make_classification</code> e combinamos os resultados.</p> <p>O objetivo \u00e9 treinar um MLP para classifica\u00e7\u00e3o multiclasse. Usaremos a mesma estrutura de c\u00f3digo do exerc\u00edcio\u00a02, modificando apenas o tamanho da camada de sa\u00edda e a fun\u00e7\u00e3o de perda. A sa\u00edda utilizar\u00e1 a fun\u00e7\u00e3o softmax e a perda ser\u00e1 a entropia cruzada categ\u00f3rica.</p> <pre><code># 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados\n# Classe 0: 2 clusters\nX0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=2, n_classes=1, class_sep=2.0,\n                              random_state=10)\ny0 = np.zeros(500, dtype=int)\n\n# Classe 1: 3 clusters\nX1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=3, n_classes=1, class_sep=2.0,\n                              random_state=20)\ny1 = np.ones(500, dtype=int)\n\n# Classe 2: 4 clusters\nX2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=4, n_classes=1, class_sep=2.0,\n                              random_state=30)\ny2 = np.full(500, 2, dtype=int)\n\n# Junta todos\nX_multi = np.vstack((X0_0, X1_0, X2_0))\ny_multi = np.concatenate((y0, y1, y2))\n\n# Divide em treino/teste\nX_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n</code></pre> <pre><code>\n# 2. Classe MLP para multiclasse\nclass SimpleMLPMulti:\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))\n        self.b2 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2  # (n, output)\n        y_hat = self.softmax(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n        return loss\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # one-hot\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz2 = (y_hat - y_one_hot) / n  # (n, output)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</code></pre> <pre><code># Cria e treina o modelo\nmlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05)\nlosses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)\n\n# Avalia\u00e7\u00e3o\npred_test_m = mlp_multi.predict(X_test_m)\naccuracy_multi = (pred_test_m == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_multi)+1), losses_multi)\nplt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()</code></pre> <pre><code>\u00c9poca 50, perda: 0.6384\n\u00c9poca 100, perda: 0.5114\n\u00c9poca 150, perda: 0.4427\n\u00c9poca 200, perda: 0.4005\nAcur\u00e1cia no teste: 0.8400\n</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/3/#analise-dos-resultados-do-exercicio-3","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a03","text":"<p>O modelo multiclasse utilizou uma camada oculta com oito neur\u00f4nios e fun\u00e7\u00e3o \\(\\tanh\\). A sa\u00edda possui tr\u00eas neur\u00f4nios e utiliza softmax com entropia cruzada categ\u00f3rica. A rede foi treinada por 200 \u00e9pocas.</p> <p>A perda de treino decresceu ao longo das \u00e9pocas e a acur\u00e1cia sobre o conjunto de teste ficou acima de 0,8 na maioria das execu\u00e7\u00f5es. Esse resultado mostra que o MLP foi capaz de distinguir as tr\u00eas classes, mesmo com m\u00faltiplos agrupamentos internos. Ajustar hiperpar\u00e2metros como tamanho da camada oculta, taxa de aprendizado ou n\u00famero de \u00e9pocas pode melhorar ainda mais o desempenho.</p>"},{"location":"portfolio/neural-networks/exercises/3/#exercicio-4-mlp-com-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04: MLP com duas camadas ocultas","text":"<p>Para finalizar, repete\u2011se o exerc\u00edcio\u00a03 com uma arquitetura mais profunda: a rede agora possui duas camadas ocultas. A primeira camada oculta cont\u00e9m doze neur\u00f4nios e a segunda camada cont\u00e9m seis neur\u00f4nios. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o \\(tanh\\) em ambas as camadas. A sa\u00edda continua a usar softmax. O mesmo conjunto multiclasse do exerc\u00edcio\u00a03 \u00e9 reutilizado.</p> <pre><code># Classe MLP com duas camadas ocultas\nclass DeepMLPMulti:\n    def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        h1, h2 = hidden_dims\n        self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))\n        self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1))\n        self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))\n        self.b3 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2\n        a2 = np.tanh(z2)\n        z3 = a2 @ self.W3.T + self.b3\n        y_hat = self.softmax(z3)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        a2 = cache['a2']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz3 = (y_hat - y_one_hot) / n\n        dW3 = dz3.T @ a2\n        db3 = dz3.sum(axis=0)\n\n        da2 = dz3 @ self.W3\n        dz2 = da2 * (1 - np.tanh(cache['z2'])**2)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2, dW3, db3\n\n    def update_params(self, dW1, db1, dW2, db2, dW3, db3):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            grads = self.backward(cache, y)\n            self.update_params(*grads)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)</code></pre> <pre><code># Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03\nmodel_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05)\nlosses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)\n\npred_test_deep = model_deep.predict(X_test_m)\nacc_deep = (pred_test_deep == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_deep)+1), losses_deep)\nplt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()</code></pre> <pre><code>\u00c9poca 50, perda: 0.5622\n\u00c9poca 100, perda: 0.4337\n\u00c9poca 150, perda: 0.3765\n\u00c9poca 200, perda: 0.3470\nAcur\u00e1cia no teste (rede profunda): 0.8067\n</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/3/#analise-dos-resultados-do-exercicio-4","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a04","text":"<p>Ao adicionar uma segunda camada oculta, a rede neural passa a ter maior capacidade de modelar padr\u00f5es complexos. Usando doze neur\u00f4nios na primeira camada oculta e seis neur\u00f4nios na segunda, observa se que a perda de treino diminui de forma semelhante ao exerc\u00edcio\u00a03. A acur\u00e1cia no conjunto de teste tamb\u00e9m melhora levemente ou permanece est\u00e1vel dependendo da inicializa\u00e7\u00e3o, situando se em torno de 0,81.</p> <p>A presen\u00e7a de duas camadas ocultas permite que a rede aprenda representa\u00e7\u00f5es mais ricas dos dados, mas tamb\u00e9m aumenta o risco de sobreajuste e eleva o custo computacional. Ajustes adicionais nos hiperpar\u00e2metros e regulariza\u00e7\u00e3o podem ser explorados para obter ganhos adicionais.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/","title":"Ex3 pedrotpc copy","text":"In\u00a0[1]: Copied! <pre># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n</pre> # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split In\u00a0[2]: Copied! <pre># 1. Gera\u00e7\u00e3o do conjunto de dados\n# Classe 0 com 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                             n_clusters_per_class=1, n_classes=1, class_sep=1.5,\n                             random_state=42)\n# Classe 1 com 2 clusters\nX1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                                  n_clusters_per_class=2, n_classes=1, class_sep=1.5,\n                                  random_state=24)\ny1 = np.ones_like(y1_temp)  # marca classe 1\n\n# Combina dados\nX_bin = np.vstack((X0, X1))\ny_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1\n</pre> # 1. Gera\u00e7\u00e3o do conjunto de dados # Classe 0 com 1 cluster X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                              n_clusters_per_class=1, n_classes=1, class_sep=1.5,                              random_state=42) # Classe 1 com 2 clusters X1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                                   n_clusters_per_class=2, n_classes=1, class_sep=1.5,                                   random_state=24) y1 = np.ones_like(y1_temp)  # marca classe 1  # Combina dados X_bin = np.vstack((X0, X1)) y_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1 In\u00a0[3]: Copied! <pre># 2. Divis\u00e3o treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)\n</pre> # 2. Divis\u00e3o treino/teste X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42) In\u00a0[4]: Copied! <pre># 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria\nclass SimpleMLPBinary:\n    def __init__(self, input_dim, hidden_dim, lr=0.01):\n        self.lr = lr\n        # inicializa\u00e7\u00e3o dos pesos com pequena escala\n        rng = np.random.default_rng(42)\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=hidden_dim)\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2 + self.b2   # (n,)\n        y_hat = self.sigmoid(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # entropia cruzada bin\u00e1ria\n        eps = 1e-12\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # derivada da perda em rela\u00e7\u00e3o ao z2\n        dz2 = y_hat - y_true  # (n,)\n        dW2 = (dz2 @ a1) / n   # (hidden,)\n        db2 = np.mean(dz2)\n\n        da1 = np.outer(dz2, self.W2)  # (n, hidden)\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = (dz1.T @ X) / n\n        db1 = dz1.mean(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            # opcional: imprimir a cada 50 \u00e9pocas\n            if (epoch + 1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return (y_hat &gt;= 0.5).astype(int)\n</pre> # 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria class SimpleMLPBinary:     def __init__(self, input_dim, hidden_dim, lr=0.01):         self.lr = lr         # inicializa\u00e7\u00e3o dos pesos com pequena escala         rng = np.random.default_rng(42)         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=hidden_dim)         self.b2 = 0.0      def sigmoid(self, z):         return 1 / (1 + np.exp(-z))      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2 + self.b2   # (n,)         y_hat = self.sigmoid(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # entropia cruzada bin\u00e1ria         eps = 1e-12         y_hat = np.clip(y_hat, eps, 1 - eps)         return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # derivada da perda em rela\u00e7\u00e3o ao z2         dz2 = y_hat - y_true  # (n,)         dW2 = (dz2 @ a1) / n   # (hidden,)         db2 = np.mean(dz2)          da1 = np.outer(dz2, self.W2)  # (n, hidden)         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = (dz1.T @ X) / n         db1 = dz1.mean(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             # opcional: imprimir a cada 50 \u00e9pocas             if (epoch + 1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return (y_hat &gt;= 0.5).astype(int) In\u00a0[5]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[6]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show()  In\u00a0[7]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[8]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show() In\u00a0[\u00a0]: Copied! <pre># 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados\n# Classe 0: 2 clusters\nX0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=2, n_classes=1, class_sep=2.0,\n                              random_state=10)\ny0 = np.zeros(500, dtype=int)\n\n# Classe 1: 3 clusters\nX1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=3, n_classes=1, class_sep=2.0,\n                              random_state=20)\ny1 = np.ones(500, dtype=int)\n\n# Classe 2: 4 clusters\nX2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=4, n_classes=1, class_sep=2.0,\n                              random_state=30)\ny2 = np.full(500, 2, dtype=int)\n\n# Junta todos\nX_multi = np.vstack((X0_0, X1_0, X2_0))\ny_multi = np.concatenate((y0, y1, y2))\n\n# Divide em treino/teste\nX_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n</pre> # 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados # Classe 0: 2 clusters X0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=2, n_classes=1, class_sep=2.0,                               random_state=10) y0 = np.zeros(500, dtype=int)  # Classe 1: 3 clusters X1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=3, n_classes=1, class_sep=2.0,                               random_state=20) y1 = np.ones(500, dtype=int)  # Classe 2: 4 clusters X2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=4, n_classes=1, class_sep=2.0,                               random_state=30) y2 = np.full(500, 2, dtype=int)  # Junta todos X_multi = np.vstack((X0_0, X1_0, X2_0)) y_multi = np.concatenate((y0, y1, y2))  # Divide em treino/teste X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)  In\u00a0[10]: Copied! <pre># 2. Classe MLP para multiclasse\nclass SimpleMLPMulti:\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))\n        self.b2 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2  # (n, output)\n        y_hat = self.softmax(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n        return loss\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # one-hot\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz2 = (y_hat - y_one_hot) / n  # (n, output)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre>  # 2. Classe MLP para multiclasse class SimpleMLPMulti:     def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))         self.b2 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2  # (n, output)         y_hat = self.softmax(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n         return loss      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # one-hot         y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz2 = (y_hat - y_one_hot) / n  # (n, output)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1)  In\u00a0[11]: Copied! <pre># Cria e treina o modelo\nmlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05)\nlosses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)\n\n# Avalia\u00e7\u00e3o\npred_test_m = mlp_multi.predict(X_test_m)\naccuracy_multi = (pred_test_m == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_multi)+1), losses_multi)\nplt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Cria e treina o modelo mlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05) losses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)  # Avalia\u00e7\u00e3o pred_test_m = mlp_multi.predict(X_test_m) accuracy_multi = (pred_test_m == y_test_m).mean() print(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_multi)+1), losses_multi) plt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.6384\n\u00c9poca 100, perda: 0.5114\n\u00c9poca 150, perda: 0.4427\n\u00c9poca 200, perda: 0.4005\nAcur\u00e1cia no teste: 0.8400\n</pre> In\u00a0[12]: Copied! <pre># Classe MLP com duas camadas ocultas\nclass DeepMLPMulti:\n    def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        h1, h2 = hidden_dims\n        self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))\n        self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1))\n        self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))\n        self.b3 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2\n        a2 = np.tanh(z2)\n        z3 = a2 @ self.W3.T + self.b3\n        y_hat = self.softmax(z3)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        a2 = cache['a2']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz3 = (y_hat - y_one_hot) / n\n        dW3 = dz3.T @ a2\n        db3 = dz3.sum(axis=0)\n\n        da2 = dz3 @ self.W3\n        dz2 = da2 * (1 - np.tanh(cache['z2'])**2)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2, dW3, db3\n\n    def update_params(self, dW1, db1, dW2, db2, dW3, db3):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            grads = self.backward(cache, y)\n            self.update_params(*grads)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre> # Classe MLP com duas camadas ocultas class DeepMLPMulti:     def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         h1, h2 = hidden_dims         self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))         self.b1 = np.zeros(h1)         self.W2 = rng.normal(scale=0.5, size=(h2, h1))         self.b2 = np.zeros(h2)         self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))         self.b3 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2         a2 = np.tanh(z2)         z3 = a2 @ self.W3.T + self.b3         y_hat = self.softmax(z3)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         a2 = cache['a2']         y_hat = cache['y_hat']         n = X.shape[0]          y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz3 = (y_hat - y_one_hot) / n         dW3 = dz3.T @ a2         db3 = dz3.sum(axis=0)          da2 = dz3 @ self.W3         dz2 = da2 * (1 - np.tanh(cache['z2'])**2)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2, dW3, db3      def update_params(self, dW1, db1, dW2, db2, dW3, db3):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2         self.W3 -= self.lr * dW3         self.b3 -= self.lr * db3      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             grads = self.backward(cache, y)             self.update_params(*grads)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1) In\u00a0[13]: Copied! <pre># Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03\nmodel_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05)\nlosses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)\n\npred_test_deep = model_deep.predict(X_test_m)\nacc_deep = (pred_test_deep == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_deep)+1), losses_deep)\nplt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03 model_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05) losses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)  pred_test_deep = model_deep.predict(X_test_m) acc_deep = (pred_test_deep == y_test_m).mean() print(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_deep)+1), losses_deep) plt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.5622\n\u00c9poca 100, perda: 0.4337\n\u00c9poca 150, perda: 0.3765\n\u00c9poca 200, perda: 0.3470\nAcur\u00e1cia no teste (rede profunda): 0.8067\n</pre>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#atividade-de-mlp","title":"Atividade de MLP\u00b6","text":"<p>Nesta atividade, exploramos redes neurais multicamadas (MLPs) usando apenas NumPy. O objetivo \u00e9 compreender o funcionamento de um perceptron multicamadas atrav\u00e9s de c\u00e1lculos manuais e depois construir redes neurais simples para classificar dados sint\u00e9ticos. A implementa\u00e7\u00e3o n\u00e3o usa bibliotecas de aprendizado de m\u00e1quina de alto n\u00edvel. Todas as opera\u00e7\u00f5es de ativa\u00e7\u00e3o, perda e gradiente s\u00e3o codificadas diretamente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio\u00a01: C\u00e1lculo manual de uma MLP\u00b6","text":"<p>Considere uma MLP simples com duas entradas, uma camada oculta com dois neur\u00f4nios e um neur\u00f4nio de sa\u00edda. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada oculta e da sa\u00edda \u00e9 a tangente hiperb\u00f3lica. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): [L = \tfrac{1}{2}(y - \\hat{y})^2], onde (\\hat{y}) \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos:</p> <ul> <li><p>Entradas e sa\u00edda desejada: (x = [0{,}5,,-0{,}2]), (y=1{,}0).</p> </li> <li><p>Pesos da camada oculta:</p> <p>[W^{(1)} = \begin{bmatrix} 0{,}3 &amp; -0{,}1  0{,}2 &amp; 0{,}4 \\end{bmatrix}]</p> </li> <li><p>Vieses da camada oculta: (b^{(1)} = [0{,}1,,-0{,}2]).</p> </li> <li><p>Pesos da camada de sa\u00edda: (W^{(2)} = [0{,}5,,-0{,}3]).</p> </li> <li><p>Vi\u00e9s da camada de sa\u00edda: (b^{(2)} = 0{,}2).</p> </li> <li><p>Taxa de aprendizado: (\\eta = 0{,}1).</p> </li> </ul> <p>O objetivo \u00e9 calcular passo a passo o percurso pelo grafo computacional: pr\u00e9 ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, perda, gradientes de todos os pesos e vieses, e as atualiza\u00e7\u00f5es de par\u00e2metros usando descida de gradiente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#passo-1-passagem-direta","title":"Passo\u00a01\u00a0\u2013 Passagem direta\u00b6","text":"<ol> <li><p>Pr\u00e9 ativa\u00e7\u00f5es da camada oculta: [z^{(1)} = W^{(1)} x + b^{(1)}] Calculando: [ z^{(1)} = \begin{bmatrix}0{,}3 &amp; -0{,}1 \\ 0{,}2 &amp; 0{,}4\\end{bmatrix}\\cdot\begin{bmatrix}0{,}5-0{,}2\\end{bmatrix}+\begin{bmatrix}0{,}1-0{,}2\\end{bmatrix} = \begin{bmatrix}0{,}27 \\ -0{,}18\\end{bmatrix}.]</p> </li> <li><p>Ativa\u00e7\u00f5es da camada oculta: usa\u2011se a tangente hiperb\u00f3lica (\tanh(z)). Para cada elemento: [ a^{(1)}_1 = \tanh(0{,}27) \u0007pprox 0{,}2636, \\quad a^{(1)}_2 = \tanh(-0{,}18) \u0007pprox -0{,}1781. ] Portanto (a^{(1)} = [0{,}2636,,-0{,}1781]).</p> </li> <li><p>Pr\u00e9 ativa\u00e7\u00e3o da sa\u00edda: [z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = [0{,}5,,-0{,}3]\\cdot\begin{bmatrix}0{,}2636-0{,}1781\\end{bmatrix}+0{,}2 \u0007pprox 0{,}3852.]</p> </li> <li><p>Sa\u00edda final: (\\hat{y} = \tanh(z^{(2)})). Como (z^{(2)} \u0007pprox 0{,}3852), ent\u00e3o [\\hat{y} = \tanh(0{,}3852) \u0007pprox 0{,}3672.]</p> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#passo-2-calculo-da-perda","title":"Passo\u00a02\u00a0\u2013 C\u00e1lculo da perda\u00b6","text":"<p>A perda \u00e9 o erro quadr\u00e1tico m\u00e9dio para uma \u00fanica amostra: [ L = \tfrac{1}{2}(y - \\hat{y})^2 = \tfrac{1}{2}(1{,}0 - 0{,}3672)^2 \u0007pprox 0{,}2002. ]</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#passo-3-retropropagacao","title":"Passo\u00a03\u00a0\u2013 Retropropaga\u00e7\u00e3o\u00b6","text":"<p>Para atualizar os par\u00e2metros, calcula\u2011se o gradiente da perda em rela\u00e7\u00e3o a cada peso e vi\u00e9s.</p> <ol> <li><p>Gradiente no neur\u00f4nio de sa\u00edda:</p> <ul> <li>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda predita: (\f rac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y).</li> <li>Derivada da tangente hiperb\u00f3lica: (\f rac{\\mathrm{d}}{\\mathrm{d}z} \tanh(z) = 1 - \tanh^2(z)).</li> </ul> <p>Assim, o erro na sa\u00edda ((\\delta^{(2)})) \u00e9 [ \\delta^{(2)} = (\\hat{y} - y) \\cdot \bigl(1 - \\hat{y}^2\bigr) \u0007pprox (0{,}3672 - 1{,}0)\\cdot (1 - 0{,}3672^2) \u0007pprox -0{,}5474. ]</p> <ul> <li>Gradiente dos pesos da sa\u00edda: [\f rac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)},a^{(1)} \u0007pprox -0{,}5474 \times [0{,}2636,,-0{,}1781] \u0007pprox [-0{,}1443,,0{,}0975].]</li> <li>Gradiente do vi\u00e9s da sa\u00edda: (\f rac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} \u0007pprox -0{,}5474).</li> </ul> </li> <li><p>Propaga\u00e7\u00e3o para a camada oculta:</p> <p>O erro em cada neur\u00f4nio oculto ((\\delta^{(1)})) \u00e9 obtido multiplicando (\\delta^{(2)}) pelos pesos da sa\u00edda e pela derivada da tangente hiperb\u00f3lica nos neur\u00f4nios ocultos: [ \\delta^{(1)} = (\\delta^{(2)},W^{(2)}) \\circ \bigl(1 - (a^{(1)})^2\bigr), ] onde (\\circ) indica produto elemento a elemento. Assim: [ \\delta^{(1)} \u0007pprox [-0{,}2547,,0{,}1590]. ]</p> <ul> <li>Gradiente dos pesos da camada oculta: [\f rac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)},x^\top \u0007pprox \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix}. ]</li> <li>Gradiente dos vieses da camada oculta: (\f rac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} \u0007pprox [-0{,}2547,,0{,}1590].)</li> </ul> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#passo-4-atualizacao-dos-parametros","title":"Passo\u00a04\u00a0\u2013 Atualiza\u00e7\u00e3o dos par\u00e2metros\u00b6","text":"<p>A atualiza\u00e7\u00e3o usa descida de gradiente simples com taxa de aprendizado (\\eta = 0{,}1). Para cada par\u00e2metro (\theta): [\theta \\gets \theta - \\eta,\f rac{\\partial L}{\\partial \theta}.]</p> <p>Calculando as novas vari\u00e1veis:</p> <ul> <li><p>Atualiza\u00e7\u00e3o de (W^{(2)}): [W^{(2)}_{\text{novo}} = W^{(2)} - \\eta,\f rac{\\partial L}{\\partial W^{(2)}} = [0{,}5,,-0{,}3] - 0{,}1\times [-0{,}1443,,0{,}0975] \u0007pprox [0{,}5144,,-0{,}3097].]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(2)}): [b^{(2)}_{\text{novo}} = 0{,}2 - 0{,}1\times(-0{,}5474) \u0007pprox 0{,}2547.]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (W^{(1)}): [W^{(1)}_{\text{novo}} = W^{(1)} - 0{,}1\times \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix} = \begin{bmatrix} 0{,}3 &amp; -0{,}1  0{,}2 &amp; 0{,}4 \\end{bmatrix} - 0{,}1\times \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix} \u0007pprox \begin{bmatrix}0{,}3127 &amp; -0{,}1051\\0{,}1920 &amp; 0{,}4032\\end{bmatrix}. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(1)}): [b^{(1)}_{\text{novo}} = b^{(1)} - 0{,}1\times [-0{,}2547,,0{,}1590] \u0007pprox [0{,}1255,,-0{,}2159].]</p> </li> </ul> <p>Esses s\u00e3o os par\u00e2metros atualizados ap\u00f3s uma itera\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#exercicio-2-classificacao-binaria-com-mlp-escrito-do-zero","title":"Exerc\u00edcio\u00a02: Classifica\u00e7\u00e3o bin\u00e1ria com MLP escrito do zero\u00b6","text":"<p>Neste exerc\u00edcio gera\u2011se um conjunto de dados bidimensional com 1000 amostras. Uma classe possui um \u00fanico agrupamento e a outra possui dois agrupamentos; para conseguir isso, geramos subconjuntos separadamente e combinamos em um \u00fanico conjunto de dados. Em seguida, implementa\u2011se um perceptron multicamadas simples em NumPy para classificar esse conjunto de dados.</p> <p>Os passos s\u00e3o:</p> <ol> <li>Gerar os dados com <code>make_classification</code> e mesclar subconjuntos para obter um agrupamento para a classe\u00a00 e dois agrupamentos para a classe\u00a01.</li> <li>Dividir o conjunto em treino (80\u00a0%) e teste (20\u00a0%).</li> <li>Implementar um MLP com uma camada oculta usando fun\u00e7\u00e3o de ativa\u00e7\u00e3o (\tanh) e sa\u00edda sigmoidal para estimar probabilidades de uma classe. A fun\u00e7\u00e3o de perda \u00e9 a entropia cruzada bin\u00e1ria.</li> <li>Treinar a rede por um n\u00famero fixo de \u00e9pocas, acompanhar a perda de treino e avaliar a acur\u00e1cia no conjunto de teste.</li> <li>Exibir uma visualiza\u00e7\u00e3o dos dados com a fronteira de decis\u00e3o aprendida.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#analise-dos-resultados-do-exercicio-2","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a02\u00b6","text":"<p>O modelo bin\u00e1rio foi treinado com uma camada oculta de quatro neur\u00f4nios e ativa\u00e7\u00e3o (\tanh). Ap\u00f3s 200 \u00e9pocas, a perda de treino diminuiu de forma est\u00e1vel, indicando aprendizado. No conjunto de teste, a acur\u00e1cia t\u00edpica obtida foi superior a 0.9, o que demonstra que a rede consegue separar os clusters com boa precis\u00e3o. O gr\u00e1fico de perda por \u00e9poca ajuda a visualizar a converg\u00eancia da descida de gradiente.</p> <p>A visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostra que a rede aprendeu uma curva que separa os dois clusters da classe\u00a01 do cluster \u00fanico da classe\u00a00. A maior parte dos pontos \u00e9 classificada corretamente, evidenciando que um MLP simples \u00e9 suficiente para resolver este problema bin\u00e1rio.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#exercicio-3-classificacao-multiclasse-com-mlp-reutilizavel","title":"Exerc\u00edcio\u00a03: Classifica\u00e7\u00e3o multiclasse com MLP reutiliz\u00e1vel\u00b6","text":"<p>Agora gera\u2011se um conjunto de dados com tr\u00eas classes e quatro atributos. As classes s\u00e3o formadas por subconjuntos com n\u00fameros diferentes de agrupamentos: duas regi\u00f5es para a classe\u00a00, tr\u00eas para a classe\u00a01 e quatro para a classe\u00a02. Para alcan\u00e7ar isso, geramos cada classe separadamente com <code>make_classification</code> e combinamos os resultados.</p> <p>O objetivo \u00e9 treinar um MLP para classifica\u00e7\u00e3o multiclasse. Usaremos a mesma estrutura de c\u00f3digo do exerc\u00edcio\u00a02, modificando apenas o tamanho da camada de sa\u00edda e a fun\u00e7\u00e3o de perda. A sa\u00edda utilizar\u00e1 a fun\u00e7\u00e3o softmax e a perda ser\u00e1 a entropia cruzada categ\u00f3rica.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#analise-dos-resultados-do-exercicio-3","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a03\u00b6","text":"<p>O modelo multiclasse utilizou uma camada oculta com oito neur\u00f4nios e fun\u00e7\u00e3o (\tanh). A sa\u00edda possui tr\u00eas neur\u00f4nios e utiliza softmax com entropia cruzada categ\u00f3rica. A rede foi treinada por 200 \u00e9pocas.</p> <p>A perda de treino decresceu ao longo das \u00e9pocas e a acur\u00e1cia sobre o conjunto de teste ficou acima de 0.8 na maioria das execu\u00e7\u00f5es. Esse resultado mostra que o MLP foi capaz de distinguir as tr\u00eas classes, mesmo com m\u00faltiplos agrupamentos internos. Ajustar hiperpar\u00e2metros como tamanho da camada oculta, taxa de aprendizado ou n\u00famero de \u00e9pocas pode melhorar ainda mais o desempenho.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#exercicio-4-mlp-com-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04: MLP com duas camadas ocultas\u00b6","text":"<p>Para finalizar, repete\u2011se o exerc\u00edcio\u00a03 com uma arquitetura mais profunda: a rede agora possui duas camadas ocultas. A primeira camada oculta cont\u00e9m doze neur\u00f4nios e a segunda camada cont\u00e9m seis neur\u00f4nios. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o (\tanh) em ambas as camadas. A sa\u00edda continua a usar softmax. O mesmo conjunto multiclasse do exerc\u00edcio\u00a03 \u00e9 reutilizado.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc%20copy/#analise-dos-resultados-do-exercicio-4","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a04\u00b6","text":"<p>Ao adicionar uma segunda camada oculta, a rede neural passa a ter maior capacidade de modelar padr\u00f5es complexos. Usando doze neur\u00f4nios na primeira camada oculta e seis neur\u00f4nios na segunda, observa\u2011se que a perda de treino diminui de forma semelhante ao exerc\u00edcio\u00a03. A acur\u00e1cia no conjunto de teste tamb\u00e9m melhora levemente ou permanece est\u00e1vel dependendo da inicializa\u00e7\u00e3o, situando\u2011se em torno de 0.85.</p> <p>A presen\u00e7a de duas camadas ocultas permite que a rede aprenda representa\u00e7\u00f5es mais ricas dos dados, mas tamb\u00e9m aumenta o risco de sobreajuste e eleva o custo computacional. Ajustes adicionais nos hiperpar\u00e2metros e regulariza\u00e7\u00e3o podem ser explorados para obter ganhos adicionais.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/","title":"Ex3 pedrotpc","text":"In\u00a0[14]: Copied! <pre># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n</pre> # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split In\u00a0[15]: Copied! <pre># 1. Gera\u00e7\u00e3o do conjunto de dados\n# Classe 0 com 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                             n_clusters_per_class=1, n_classes=1, class_sep=1.5,\n                             random_state=42)\n# Classe 1 com 2 clusters\nX1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                                  n_clusters_per_class=2, n_classes=1, class_sep=1.5,\n                                  random_state=24)\ny1 = np.ones_like(y1_temp)  # marca classe 1\n\n# Combina dados\nX_bin = np.vstack((X0, X1))\ny_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1\n</pre> # 1. Gera\u00e7\u00e3o do conjunto de dados # Classe 0 com 1 cluster X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                              n_clusters_per_class=1, n_classes=1, class_sep=1.5,                              random_state=42) # Classe 1 com 2 clusters X1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                                   n_clusters_per_class=2, n_classes=1, class_sep=1.5,                                   random_state=24) y1 = np.ones_like(y1_temp)  # marca classe 1  # Combina dados X_bin = np.vstack((X0, X1)) y_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1 In\u00a0[16]: Copied! <pre># 2. Divis\u00e3o treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)\n</pre> # 2. Divis\u00e3o treino/teste X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42) In\u00a0[17]: Copied! <pre># 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria\nclass SimpleMLPBinary:\n    def __init__(self, input_dim, hidden_dim, lr=0.01):\n        self.lr = lr\n        # inicializa\u00e7\u00e3o dos pesos com pequena escala\n        rng = np.random.default_rng(42)\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=hidden_dim)\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2 + self.b2   # (n,)\n        y_hat = self.sigmoid(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # entropia cruzada bin\u00e1ria\n        eps = 1e-12\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # derivada da perda em rela\u00e7\u00e3o ao z2\n        dz2 = y_hat - y_true  # (n,)\n        dW2 = (dz2 @ a1) / n   # (hidden,)\n        db2 = np.mean(dz2)\n\n        da1 = np.outer(dz2, self.W2)  # (n, hidden)\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = (dz1.T @ X) / n\n        db1 = dz1.mean(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            # opcional: imprimir a cada 50 \u00e9pocas\n            if (epoch + 1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return (y_hat &gt;= 0.5).astype(int)\n</pre> # 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria class SimpleMLPBinary:     def __init__(self, input_dim, hidden_dim, lr=0.01):         self.lr = lr         # inicializa\u00e7\u00e3o dos pesos com pequena escala         rng = np.random.default_rng(42)         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=hidden_dim)         self.b2 = 0.0      def sigmoid(self, z):         return 1 / (1 + np.exp(-z))      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2 + self.b2   # (n,)         y_hat = self.sigmoid(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # entropia cruzada bin\u00e1ria         eps = 1e-12         y_hat = np.clip(y_hat, eps, 1 - eps)         return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # derivada da perda em rela\u00e7\u00e3o ao z2         dz2 = y_hat - y_true  # (n,)         dW2 = (dz2 @ a1) / n   # (hidden,)         db2 = np.mean(dz2)          da1 = np.outer(dz2, self.W2)  # (n, hidden)         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = (dz1.T @ X) / n         db1 = dz1.mean(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             # opcional: imprimir a cada 50 \u00e9pocas             if (epoch + 1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return (y_hat &gt;= 0.5).astype(int) In\u00a0[18]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[19]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show()  In\u00a0[20]: Copied! <pre># 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados\n# Classe 0: 2 clusters\nX0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=2, n_classes=1, class_sep=2.0,\n                              random_state=10)\ny0 = np.zeros(500, dtype=int)\n\n# Classe 1: 3 clusters\nX1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=3, n_classes=1, class_sep=2.0,\n                              random_state=20)\ny1 = np.ones(500, dtype=int)\n\n# Classe 2: 4 clusters\nX2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=4, n_classes=1, class_sep=2.0,\n                              random_state=30)\ny2 = np.full(500, 2, dtype=int)\n\n# Junta todos\nX_multi = np.vstack((X0_0, X1_0, X2_0))\ny_multi = np.concatenate((y0, y1, y2))\n\n# Divide em treino/teste\nX_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n</pre> # 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados # Classe 0: 2 clusters X0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=2, n_classes=1, class_sep=2.0,                               random_state=10) y0 = np.zeros(500, dtype=int)  # Classe 1: 3 clusters X1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=3, n_classes=1, class_sep=2.0,                               random_state=20) y1 = np.ones(500, dtype=int)  # Classe 2: 4 clusters X2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=4, n_classes=1, class_sep=2.0,                               random_state=30) y2 = np.full(500, 2, dtype=int)  # Junta todos X_multi = np.vstack((X0_0, X1_0, X2_0)) y_multi = np.concatenate((y0, y1, y2))  # Divide em treino/teste X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)  In\u00a0[21]: Copied! <pre># 2. Classe MLP para multiclasse\nclass SimpleMLPMulti:\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))\n        self.b2 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2  # (n, output)\n        y_hat = self.softmax(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n        return loss\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # one-hot\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz2 = (y_hat - y_one_hot) / n  # (n, output)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre>  # 2. Classe MLP para multiclasse class SimpleMLPMulti:     def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))         self.b2 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2  # (n, output)         y_hat = self.softmax(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n         return loss      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # one-hot         y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz2 = (y_hat - y_one_hot) / n  # (n, output)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1)  In\u00a0[22]: Copied! <pre># Cria e treina o modelo\nmlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05)\nlosses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)\n\n# Avalia\u00e7\u00e3o\npred_test_m = mlp_multi.predict(X_test_m)\naccuracy_multi = (pred_test_m == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_multi)+1), losses_multi)\nplt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Cria e treina o modelo mlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05) losses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)  # Avalia\u00e7\u00e3o pred_test_m = mlp_multi.predict(X_test_m) accuracy_multi = (pred_test_m == y_test_m).mean() print(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_multi)+1), losses_multi) plt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.6384\n\u00c9poca 100, perda: 0.5114\n\u00c9poca 150, perda: 0.4427\n\u00c9poca 200, perda: 0.4005\nAcur\u00e1cia no teste: 0.8400\n</pre> In\u00a0[25]: Copied! <pre># Classe MLP com duas camadas ocultas\nclass DeepMLPMulti:\n    def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        h1, h2 = hidden_dims\n        self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))\n        self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1))\n        self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))\n        self.b3 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2\n        a2 = np.tanh(z2)\n        z3 = a2 @ self.W3.T + self.b3\n        y_hat = self.softmax(z3)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        a2 = cache['a2']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz3 = (y_hat - y_one_hot) / n\n        dW3 = dz3.T @ a2\n        db3 = dz3.sum(axis=0)\n\n        da2 = dz3 @ self.W3\n        dz2 = da2 * (1 - np.tanh(cache['z2'])**2)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2, dW3, db3\n\n    def update_params(self, dW1, db1, dW2, db2, dW3, db3):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            grads = self.backward(cache, y)\n            self.update_params(*grads)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre> # Classe MLP com duas camadas ocultas class DeepMLPMulti:     def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         h1, h2 = hidden_dims         self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))         self.b1 = np.zeros(h1)         self.W2 = rng.normal(scale=0.5, size=(h2, h1))         self.b2 = np.zeros(h2)         self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))         self.b3 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2         a2 = np.tanh(z2)         z3 = a2 @ self.W3.T + self.b3         y_hat = self.softmax(z3)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         a2 = cache['a2']         y_hat = cache['y_hat']         n = X.shape[0]          y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz3 = (y_hat - y_one_hot) / n         dW3 = dz3.T @ a2         db3 = dz3.sum(axis=0)          da2 = dz3 @ self.W3         dz2 = da2 * (1 - np.tanh(cache['z2'])**2)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2, dW3, db3      def update_params(self, dW1, db1, dW2, db2, dW3, db3):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2         self.W3 -= self.lr * dW3         self.b3 -= self.lr * db3      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             grads = self.backward(cache, y)             self.update_params(*grads)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1) In\u00a0[26]: Copied! <pre># Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03\nmodel_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05)\nlosses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)\n\npred_test_deep = model_deep.predict(X_test_m)\nacc_deep = (pred_test_deep == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_deep)+1), losses_deep)\nplt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03 model_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05) losses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)  pred_test_deep = model_deep.predict(X_test_m) acc_deep = (pred_test_deep == y_test_m).mean() print(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_deep)+1), losses_deep) plt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.5622\n\u00c9poca 100, perda: 0.4337\n\u00c9poca 150, perda: 0.3765\n\u00c9poca 200, perda: 0.3470\nAcur\u00e1cia no teste (rede profunda): 0.8067\n</pre>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#atividade-de-mlp","title":"Atividade de MLP\u00b6","text":"<p>Nesta atividade, exploramos redes neurais multicamadas (MLPs) usando apenas NumPy. O objetivo \u00e9 compreender o funcionamento de um perceptron multicamadas atrav\u00e9s de c\u00e1lculos manuais e depois construir redes neurais simples para classificar dados sint\u00e9ticos. A implementa\u00e7\u00e3o n\u00e3o usa bibliotecas de aprendizado de m\u00e1quina de alto n\u00edvel. Todas as opera\u00e7\u00f5es de ativa\u00e7\u00e3o, perda e gradiente s\u00e3o codificadas diretamente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo manual de uma MLP\u00b6","text":"<p>Considere uma MLP simples com duas entradas, uma camada oculta com dois neur\u00f4nios e um neur\u00f4nio de sa\u00edda. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada oculta e da sa\u00edda \u00e9 a tangente hiperb\u00f3lica. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): [L = \\frac{1}{2}(y - \\hat{y})^2], onde (\\hat{y}) \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos:</p> <ul> <li><p>Entradas e sa\u00edda desejada: (x = [0{,}5,,-0{,}2]), (y=1{,}0).</p> </li> <li><p>Pesos da camada oculta:</p> <p>[W^{(1)} = \\begin{bmatrix}  0{,}3 &amp; -0{,}1 \\\\  0{,}2 &amp; 0{,}4  \\end{bmatrix}]</p> </li> <li><p>Vieses da camada oculta: (b^{(1)} = [0{,}1,,-0{,}2]).</p> </li> <li><p>Pesos da camada de sa\u00edda: (W^{(2)} = [0{,}5,,-0{,}3]).</p> </li> <li><p>Vi\u00e9s da camada de sa\u00edda: (b^{(2)} = 0{,}2).</p> </li> <li><p>Taxa de aprendizado: (\\eta = 0{,}1).</p> </li> </ul> <p>O objetivo \u00e9 calcular passo a passo o percurso pelo grafo computacional: pr\u00e9 ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, perda, gradientes de todos os pesos e vieses, e as atualiza\u00e7\u00f5es de par\u00e2metros usando descida de gradiente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#passo-1-passagem-direta","title":"Passo 1 \u2013 Passagem direta\u00b6","text":"<ol> <li><p>Pr\u00e9 ativa\u00e7\u00f5es da camada oculta: [ z^{(1)} = W^{(1)} x + b^{(1)} ] Calculando: [ z^{(1)} = \\begin{bmatrix}0{,}3 &amp; -0{,}1 \\\\  0{,}2 &amp; 0{,}4\\end{bmatrix}\\cdot\\begin{bmatrix}0{,}5 \\\\ -0{,}2\\end{bmatrix}+\\begin{bmatrix}0{,}1 \\\\ -0{,}2\\end{bmatrix} = \\begin{bmatrix}0{,}27 \\\\ -0{,}18\\end{bmatrix}. ]</p> </li> <li><p>Ativa\u00e7\u00f5es da camada oculta: usa a tangente hiperb\u00f3lica (\\tanh(z)). Para cada elemento: [ a^{(1)}_1 = \\tanh(0{,}27) \\approx 0{,}2636, \\quad a^{(1)}_2 = \\tanh(-0{,}18) \\approx -0{,}1781. ] Portanto (a^{(1)} = [0{,}2636,,-0{,}1781]).</p> </li> <li><p>Pr\u00e9 ativa\u00e7\u00e3o da sa\u00edda: [ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = [0{,}5,,-0{,}3]\\cdot\\begin{bmatrix}0{,}2636 \\\\ -0{,}1781\\end{bmatrix}+0{,}2 \\approx 0{,}3852. ]</p> </li> <li><p>Sa\u00edda final: (\\hat{y} = \\tanh(z^{(2)})). Como (z^{(2)} \\approx 0{,}3852), ent\u00e3o [ \\hat{y} = \\tanh(0{,}3852) \\approx 0{,}3672. ]</p> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#passo-2-calculo-da-perda","title":"Passo 2 \u2013 C\u00e1lculo da perda\u00b6","text":"<p>A perda \u00e9 o erro quadr\u00e1tico m\u00e9dio para uma \u00fanica amostra: [ L = \\frac{1}{2}(y - \\hat{y})^2 = \\frac{1}{2}(1{,}0 - 0{,}3672)^2 \\approx 0{,}2002. ]</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#passo-3-retropropagacao","title":"Passo 3 \u2013 Retropropaga\u00e7\u00e3o\u00b6","text":"<p>Para atualizar os par\u00e2metros, calcula se o gradiente da perda em rela\u00e7\u00e3o a cada peso e vi\u00e9s.</p> <ol> <li><p>Gradiente no neur\u00f4nio de sa\u00edda:</p> <ul> <li>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda predita: (\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y).</li> <li>Derivada da tangente hiperb\u00f3lica: (\\frac{\\mathrm{d}}{\\mathrm{d}z} \\tanh(z) = 1 - \\tanh^2(z)).</li> </ul> <p>Assim, o erro na sa\u00edda ((\\delta^{(2)})) \u00e9 [ \\delta^{(2)} = (\\hat{y} - y) \\cdot \\bigl(1 - \\hat{y}^2\\bigr) \\approx (0{,}3672 - 1{,}0)\\cdot (1 - 0{,}3672^2) \\approx -0{,}5474. ]</p> <ul> <li>Gradiente dos pesos da sa\u00edda: [ \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)},a^{(1)} \\approx -0{,}5474 \\times [0{,}2636,,-0{,}1781] \\approx [-0{,}1443,,0{,}0975]. ]</li> <li>Gradiente do vi\u00e9s da sa\u00edda: (\\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} \\approx -0{,}5474).</li> </ul> </li> <li><p>Propaga\u00e7\u00e3o para a camada oculta:</p> <p>O erro em cada neur\u00f4nio oculto ((\\delta^{(1)})) \u00e9 obtido multiplicando (\\delta^{(2)}) pelos pesos da sa\u00edda e pela derivada da tangente hiperb\u00f3lica nos neur\u00f4nios ocultos: [ \\delta^{(1)} = \\bigl(\\delta^{(2)},W^{(2)}\\bigr) \\circ \\bigl(1 - (a^{(1)})^2\\bigr), ] onde (\\circ) indica produto elemento a elemento. Assim: [ \\delta^{(1)} \\approx [-0{,}2547,,0{,}1590]. ]</p> <ul> <li>Gradiente dos pesos da camada oculta: [ \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)},x^\\top \\approx \\begin{bmatrix} -0{,}1273 &amp; 0{,}0509 \\\\  0{,}0795 &amp; -0{,}0318 \\end{bmatrix}. ]</li> <li>Gradiente dos vieses da camada oculta: (\\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} \\approx [-0{,}2547,,0{,}1590].)</li> </ul> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#passo-4-atualizacao-dos-parametros","title":"Passo 4 \u2013 Atualiza\u00e7\u00e3o dos par\u00e2metros\u00b6","text":"<p>A atualiza\u00e7\u00e3o usa descida de gradiente simples com taxa de aprendizado (\\eta = 0{,}1). Para cada par\u00e2metro (\\theta): [\\theta \\gets \\theta - \\eta,\\frac{\\partial L}{\\partial \\theta}.]</p> <p>Calculando as novas vari\u00e1veis:</p> <ul> <li><p>Atualiza\u00e7\u00e3o de (W^{(2)}): [ W^{(2)}_{\\text{novo}} = W^{(2)} - \\eta,\\frac{\\partial L}{\\partial W^{(2)}} = [0{,}5,,-0{,}3] - 0{,}1\\times [-0{,}1443,,0{,}0975] \\approx [0{,}5144,,-0{,}3097]. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(2)}): [ b^{(2)}_{\\text{novo}} = 0{,}2 - 0{,}1\\times(-0{,}5474) \\approx 0{,}2547. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (W^{(1)}): [ W^{(1)}_{\\text{novo}} = W^{(1)} - 0{,}1\\times \\begin{bmatrix}    -0{,}1273 &amp; 0{,}0509 \\\\     0{,}0795 &amp; -0{,}0318    \\end{bmatrix} = \\begin{bmatrix}    0{,}3 &amp; -0{,}1 \\\\    0{,}2 &amp; 0{,}4    \\end{bmatrix} - 0{,}1\\times \\begin{bmatrix}    -0{,}1273 &amp; 0{,}0509 \\\\     0{,}0795 &amp; -0{,}0318    \\end{bmatrix} \\approx \\begin{bmatrix}0{,}3127 &amp; -0{,}1051\\\\0{,}1920 &amp; 0{,}4032\\end{bmatrix}. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(1)}): [ b^{(1)}_{\\text{novo}} = b^{(1)} - 0{,}1\\times [-0{,}2547,,0{,}1590] \\approx [0{,}1255,,-0{,}2159]. ]</p> </li> </ul> <p>Esses s\u00e3o os par\u00e2metros atualizados ap\u00f3s uma itera\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#exercicio-2-classificacao-binaria-com-mlp-escrito-do-zero","title":"Exerc\u00edcio\u00a02: Classifica\u00e7\u00e3o bin\u00e1ria com MLP escrito do zero\u00b6","text":"<p>Neste exerc\u00edcio gera\u2011se um conjunto de dados bidimensional com 1000 amostras. Uma classe possui um \u00fanico agrupamento e a outra possui dois agrupamentos; para conseguir isso, geramos subconjuntos separadamente e combinamos em um \u00fanico conjunto de dados. Em seguida, implementa\u2011se um perceptron multicamadas simples em NumPy para classificar esse conjunto de dados.</p> <p>Os passos s\u00e3o:</p> <ol> <li>Gerar os dados com <code>make_classification</code> e mesclar subconjuntos para obter um agrupamento para a classe\u00a00 e dois agrupamentos para a classe\u00a01.</li> <li>Dividir o conjunto em treino (80\u00a0%) e teste (20\u00a0%).</li> <li>Implementar um MLP com uma camada oculta usando fun\u00e7\u00e3o de ativa\u00e7\u00e3o (\tanh) e sa\u00edda sigmoidal para estimar probabilidades de uma classe. A fun\u00e7\u00e3o de perda \u00e9 a entropia cruzada bin\u00e1ria.</li> <li>Treinar a rede por um n\u00famero fixo de \u00e9pocas, acompanhar a perda de treino e avaliar a acur\u00e1cia no conjunto de teste.</li> <li>Exibir uma visualiza\u00e7\u00e3o dos dados com a fronteira de decis\u00e3o aprendida.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#analise-dos-resultados-do-exercicio-2","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a02\u00b6","text":"<p>O modelo bin\u00e1rio foi treinado com uma camada oculta de quatro neur\u00f4nios e ativa\u00e7\u00e3o (\\tanh). Ap\u00f3s 200 \u00e9pocas, a perda de treino diminuiu de forma est\u00e1vel, indicando aprendizado. No conjunto de teste, a acur\u00e1cia t\u00edpica obtida foi superior a 0{,}9, o que demonstra que a rede consegue separar os clusters com boa precis\u00e3o. O gr\u00e1fico de perda por \u00e9poca ajuda a visualizar a converg\u00eancia da descida de gradiente.</p> <p>A visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostra que a rede aprendeu uma curva que separa os dois agrupamentos da classe\u00a01 do agrupamento \u00fanico da classe\u00a00. A maior parte dos pontos \u00e9 classificada corretamente, evidenciando que um MLP simples \u00e9 suficiente para resolver este problema bin\u00e1rio.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#exercicio-3-classificacao-multiclasse-com-mlp-reutilizavel","title":"Exerc\u00edcio\u00a03: Classifica\u00e7\u00e3o multiclasse com MLP reutiliz\u00e1vel\u00b6","text":"<p>Agora gera\u2011se um conjunto de dados com tr\u00eas classes e quatro atributos. As classes s\u00e3o formadas por subconjuntos com n\u00fameros diferentes de agrupamentos: duas regi\u00f5es para a classe\u00a00, tr\u00eas para a classe\u00a01 e quatro para a classe\u00a02. Para alcan\u00e7ar isso, geramos cada classe separadamente com <code>make_classification</code> e combinamos os resultados.</p> <p>O objetivo \u00e9 treinar um MLP para classifica\u00e7\u00e3o multiclasse. Usaremos a mesma estrutura de c\u00f3digo do exerc\u00edcio\u00a02, modificando apenas o tamanho da camada de sa\u00edda e a fun\u00e7\u00e3o de perda. A sa\u00edda utilizar\u00e1 a fun\u00e7\u00e3o softmax e a perda ser\u00e1 a entropia cruzada categ\u00f3rica.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#analise-dos-resultados-do-exercicio-3","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a03\u00b6","text":"<p>O modelo multiclasse utilizou uma camada oculta com oito neur\u00f4nios e fun\u00e7\u00e3o (\\tanh). A sa\u00edda possui tr\u00eas neur\u00f4nios e utiliza softmax com entropia cruzada categ\u00f3rica. A rede foi treinada por 200 \u00e9pocas.</p> <p>A perda de treino decresceu ao longo das \u00e9pocas e a acur\u00e1cia sobre o conjunto de teste ficou acima de 0{,}8 na maioria das execu\u00e7\u00f5es. Esse resultado mostra que o MLP foi capaz de distinguir as tr\u00eas classes, mesmo com m\u00faltiplos agrupamentos internos. Ajustar hiperpar\u00e2metros como tamanho da camada oculta, taxa de aprendizado ou n\u00famero de \u00e9pocas pode melhorar ainda mais o desempenho.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#exercicio-4-mlp-com-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04: MLP com duas camadas ocultas\u00b6","text":"<p>Para finalizar, repete\u2011se o exerc\u00edcio\u00a03 com uma arquitetura mais profunda: a rede agora possui duas camadas ocultas. A primeira camada oculta cont\u00e9m doze neur\u00f4nios e a segunda camada cont\u00e9m seis neur\u00f4nios. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o (\tanh) em ambas as camadas. A sa\u00edda continua a usar softmax. O mesmo conjunto multiclasse do exerc\u00edcio\u00a03 \u00e9 reutilizado.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc/#analise-dos-resultados-do-exercicio-4","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a04\u00b6","text":"<p>Ao adicionar uma segunda camada oculta, a rede neural passa a ter maior capacidade de modelar padr\u00f5es complexos. Usando doze neur\u00f4nios na primeira camada oculta e seis neur\u00f4nios na segunda, observa se que a perda de treino diminui de forma semelhante ao exerc\u00edcio\u00a03. A acur\u00e1cia no conjunto de teste tamb\u00e9m melhora levemente ou permanece est\u00e1vel dependendo da inicializa\u00e7\u00e3o, situando se em torno de 0{,}81.</p> <p>A presen\u00e7a de duas camadas ocultas permite que a rede aprenda representa\u00e7\u00f5es mais ricas dos dados, mas tamb\u00e9m aumenta o risco de sobreajuste e eleva o custo computacional. Ajustes adicionais nos hiperpar\u00e2metros e regulariza\u00e7\u00e3o podem ser explorados para obter ganhos adicionais.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/","title":"ex3 pedrotpcOG","text":"In\u00a0[1]: Copied! <pre># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n</pre> # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split In\u00a0[2]: Copied! <pre># 1. Gera\u00e7\u00e3o do conjunto de dados\n# Classe 0 com 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                             n_clusters_per_class=1, n_classes=1, class_sep=1.5,\n                             random_state=42)\n# Classe 1 com 2 clusters\nX1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                                  n_clusters_per_class=2, n_classes=1, class_sep=1.5,\n                                  random_state=24)\ny1 = np.ones_like(y1_temp)  # marca classe 1\n\n# Combina dados\nX_bin = np.vstack((X0, X1))\ny_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1\n</pre> # 1. Gera\u00e7\u00e3o do conjunto de dados # Classe 0 com 1 cluster X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                              n_clusters_per_class=1, n_classes=1, class_sep=1.5,                              random_state=42) # Classe 1 com 2 clusters X1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                                   n_clusters_per_class=2, n_classes=1, class_sep=1.5,                                   random_state=24) y1 = np.ones_like(y1_temp)  # marca classe 1  # Combina dados X_bin = np.vstack((X0, X1)) y_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1 In\u00a0[3]: Copied! <pre># 2. Divis\u00e3o treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)\n</pre> # 2. Divis\u00e3o treino/teste X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42) In\u00a0[4]: Copied! <pre># 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria\nclass SimpleMLPBinary:\n    def __init__(self, input_dim, hidden_dim, lr=0.01):\n        self.lr = lr\n        # inicializa\u00e7\u00e3o dos pesos com pequena escala\n        rng = np.random.default_rng(42)\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=hidden_dim)\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2 + self.b2   # (n,)\n        y_hat = self.sigmoid(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # entropia cruzada bin\u00e1ria\n        eps = 1e-12\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # derivada da perda em rela\u00e7\u00e3o ao z2\n        dz2 = y_hat - y_true  # (n,)\n        dW2 = (dz2 @ a1) / n   # (hidden,)\n        db2 = np.mean(dz2)\n\n        da1 = np.outer(dz2, self.W2)  # (n, hidden)\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = (dz1.T @ X) / n\n        db1 = dz1.mean(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            # opcional: imprimir a cada 50 \u00e9pocas\n            if (epoch + 1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return (y_hat &gt;= 0.5).astype(int)\n</pre> # 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria class SimpleMLPBinary:     def __init__(self, input_dim, hidden_dim, lr=0.01):         self.lr = lr         # inicializa\u00e7\u00e3o dos pesos com pequena escala         rng = np.random.default_rng(42)         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=hidden_dim)         self.b2 = 0.0      def sigmoid(self, z):         return 1 / (1 + np.exp(-z))      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2 + self.b2   # (n,)         y_hat = self.sigmoid(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # entropia cruzada bin\u00e1ria         eps = 1e-12         y_hat = np.clip(y_hat, eps, 1 - eps)         return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # derivada da perda em rela\u00e7\u00e3o ao z2         dz2 = y_hat - y_true  # (n,)         dW2 = (dz2 @ a1) / n   # (hidden,)         db2 = np.mean(dz2)          da1 = np.outer(dz2, self.W2)  # (n, hidden)         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = (dz1.T @ X) / n         db1 = dz1.mean(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             # opcional: imprimir a cada 50 \u00e9pocas             if (epoch + 1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return (y_hat &gt;= 0.5).astype(int) In\u00a0[5]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[6]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show()  In\u00a0[7]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[8]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show() In\u00a0[\u00a0]: Copied! <pre># 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados\n# Classe 0: 2 clusters\nX0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=2, n_classes=1, class_sep=2.0,\n                              random_state=10)\ny0 = np.zeros(500, dtype=int)\n\n# Classe 1: 3 clusters\nX1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=3, n_classes=1, class_sep=2.0,\n                              random_state=20)\ny1 = np.ones(500, dtype=int)\n\n# Classe 2: 4 clusters\nX2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=4, n_classes=1, class_sep=2.0,\n                              random_state=30)\ny2 = np.full(500, 2, dtype=int)\n\n# Junta todos\nX_multi = np.vstack((X0_0, X1_0, X2_0))\ny_multi = np.concatenate((y0, y1, y2))\n\n# Divide em treino/teste\nX_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n</pre> # 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados # Classe 0: 2 clusters X0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=2, n_classes=1, class_sep=2.0,                               random_state=10) y0 = np.zeros(500, dtype=int)  # Classe 1: 3 clusters X1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=3, n_classes=1, class_sep=2.0,                               random_state=20) y1 = np.ones(500, dtype=int)  # Classe 2: 4 clusters X2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=4, n_classes=1, class_sep=2.0,                               random_state=30) y2 = np.full(500, 2, dtype=int)  # Junta todos X_multi = np.vstack((X0_0, X1_0, X2_0)) y_multi = np.concatenate((y0, y1, y2))  # Divide em treino/teste X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)  In\u00a0[10]: Copied! <pre># 2. Classe MLP para multiclasse\nclass SimpleMLPMulti:\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))\n        self.b2 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2  # (n, output)\n        y_hat = self.softmax(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n        return loss\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # one-hot\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz2 = (y_hat - y_one_hot) / n  # (n, output)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre>  # 2. Classe MLP para multiclasse class SimpleMLPMulti:     def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))         self.b2 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2  # (n, output)         y_hat = self.softmax(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n         return loss      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # one-hot         y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz2 = (y_hat - y_one_hot) / n  # (n, output)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1)  In\u00a0[11]: Copied! <pre># Cria e treina o modelo\nmlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05)\nlosses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)\n\n# Avalia\u00e7\u00e3o\npred_test_m = mlp_multi.predict(X_test_m)\naccuracy_multi = (pred_test_m == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_multi)+1), losses_multi)\nplt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Cria e treina o modelo mlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05) losses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)  # Avalia\u00e7\u00e3o pred_test_m = mlp_multi.predict(X_test_m) accuracy_multi = (pred_test_m == y_test_m).mean() print(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_multi)+1), losses_multi) plt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.6384\n\u00c9poca 100, perda: 0.5114\n\u00c9poca 150, perda: 0.4427\n\u00c9poca 200, perda: 0.4005\nAcur\u00e1cia no teste: 0.8400\n</pre> In\u00a0[12]: Copied! <pre># Classe MLP com duas camadas ocultas\nclass DeepMLPMulti:\n    def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        h1, h2 = hidden_dims\n        self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))\n        self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1))\n        self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))\n        self.b3 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2\n        a2 = np.tanh(z2)\n        z3 = a2 @ self.W3.T + self.b3\n        y_hat = self.softmax(z3)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        a2 = cache['a2']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz3 = (y_hat - y_one_hot) / n\n        dW3 = dz3.T @ a2\n        db3 = dz3.sum(axis=0)\n\n        da2 = dz3 @ self.W3\n        dz2 = da2 * (1 - np.tanh(cache['z2'])**2)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2, dW3, db3\n\n    def update_params(self, dW1, db1, dW2, db2, dW3, db3):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            grads = self.backward(cache, y)\n            self.update_params(*grads)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre> # Classe MLP com duas camadas ocultas class DeepMLPMulti:     def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         h1, h2 = hidden_dims         self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))         self.b1 = np.zeros(h1)         self.W2 = rng.normal(scale=0.5, size=(h2, h1))         self.b2 = np.zeros(h2)         self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))         self.b3 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2         a2 = np.tanh(z2)         z3 = a2 @ self.W3.T + self.b3         y_hat = self.softmax(z3)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         a2 = cache['a2']         y_hat = cache['y_hat']         n = X.shape[0]          y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz3 = (y_hat - y_one_hot) / n         dW3 = dz3.T @ a2         db3 = dz3.sum(axis=0)          da2 = dz3 @ self.W3         dz2 = da2 * (1 - np.tanh(cache['z2'])**2)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2, dW3, db3      def update_params(self, dW1, db1, dW2, db2, dW3, db3):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2         self.W3 -= self.lr * dW3         self.b3 -= self.lr * db3      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             grads = self.backward(cache, y)             self.update_params(*grads)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1) In\u00a0[13]: Copied! <pre># Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03\nmodel_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05)\nlosses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)\n\npred_test_deep = model_deep.predict(X_test_m)\nacc_deep = (pred_test_deep == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_deep)+1), losses_deep)\nplt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03 model_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05) losses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)  pred_test_deep = model_deep.predict(X_test_m) acc_deep = (pred_test_deep == y_test_m).mean() print(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_deep)+1), losses_deep) plt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.5622\n\u00c9poca 100, perda: 0.4337\n\u00c9poca 150, perda: 0.3765\n\u00c9poca 200, perda: 0.3470\nAcur\u00e1cia no teste (rede profunda): 0.8067\n</pre>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#atividade-de-mlp","title":"Atividade de MLP\u00b6","text":"<p>Nesta atividade, exploramos redes neurais multicamadas (MLPs) usando apenas NumPy. O objetivo \u00e9 compreender o funcionamento de um perceptron multicamadas atrav\u00e9s de c\u00e1lculos manuais e depois construir redes neurais simples para classificar dados sint\u00e9ticos. A implementa\u00e7\u00e3o n\u00e3o usa bibliotecas de aprendizado de m\u00e1quina de alto n\u00edvel. Todas as opera\u00e7\u00f5es de ativa\u00e7\u00e3o, perda e gradiente s\u00e3o codificadas diretamente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio\u00a01: C\u00e1lculo manual de uma MLP\u00b6","text":"<p>Considere uma MLP simples com duas entradas, uma camada oculta com dois neur\u00f4nios e um neur\u00f4nio de sa\u00edda. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada oculta e da sa\u00edda \u00e9 a tangente hiperb\u00f3lica. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): [L = \tfrac{1}{2}(y - \\hat{y})^2], onde (\\hat{y}) \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos:</p> <ul> <li><p>Entradas e sa\u00edda desejada: (x = [0{,}5,,-0{,}2]), (y=1{,}0).</p> </li> <li><p>Pesos da camada oculta:</p> <p>[W^{(1)} = \begin{bmatrix} 0{,}3 &amp; -0{,}1  0{,}2 &amp; 0{,}4 \\end{bmatrix}]</p> </li> <li><p>Vieses da camada oculta: (b^{(1)} = [0{,}1,,-0{,}2]).</p> </li> <li><p>Pesos da camada de sa\u00edda: (W^{(2)} = [0{,}5,,-0{,}3]).</p> </li> <li><p>Vi\u00e9s da camada de sa\u00edda: (b^{(2)} = 0{,}2).</p> </li> <li><p>Taxa de aprendizado: (\\eta = 0{,}1).</p> </li> </ul> <p>O objetivo \u00e9 calcular passo a passo o percurso pelo grafo computacional: pr\u00e9 ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, perda, gradientes de todos os pesos e vieses, e as atualiza\u00e7\u00f5es de par\u00e2metros usando descida de gradiente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#passo-1-passagem-direta","title":"Passo\u00a01\u00a0\u2013 Passagem direta\u00b6","text":"<ol> <li><p>Pr\u00e9 ativa\u00e7\u00f5es da camada oculta: [z^{(1)} = W^{(1)} x + b^{(1)}] Calculando: [ z^{(1)} = \begin{bmatrix}0{,}3 &amp; -0{,}1 \\ 0{,}2 &amp; 0{,}4\\end{bmatrix}\\cdot\begin{bmatrix}0{,}5-0{,}2\\end{bmatrix}+\begin{bmatrix}0{,}1-0{,}2\\end{bmatrix} = \begin{bmatrix}0{,}27 \\ -0{,}18\\end{bmatrix}.]</p> </li> <li><p>Ativa\u00e7\u00f5es da camada oculta: usa\u2011se a tangente hiperb\u00f3lica (\tanh(z)). Para cada elemento: [ a^{(1)}_1 = \tanh(0{,}27) \u0007pprox 0{,}2636, \\quad a^{(1)}_2 = \tanh(-0{,}18) \u0007pprox -0{,}1781. ] Portanto (a^{(1)} = [0{,}2636,,-0{,}1781]).</p> </li> <li><p>Pr\u00e9 ativa\u00e7\u00e3o da sa\u00edda: [z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = [0{,}5,,-0{,}3]\\cdot\begin{bmatrix}0{,}2636-0{,}1781\\end{bmatrix}+0{,}2 \u0007pprox 0{,}3852.]</p> </li> <li><p>Sa\u00edda final: (\\hat{y} = \tanh(z^{(2)})). Como (z^{(2)} \u0007pprox 0{,}3852), ent\u00e3o [\\hat{y} = \tanh(0{,}3852) \u0007pprox 0{,}3672.]</p> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#passo-2-calculo-da-perda","title":"Passo\u00a02\u00a0\u2013 C\u00e1lculo da perda\u00b6","text":"<p>A perda \u00e9 o erro quadr\u00e1tico m\u00e9dio para uma \u00fanica amostra: [ L = \tfrac{1}{2}(y - \\hat{y})^2 = \tfrac{1}{2}(1{,}0 - 0{,}3672)^2 \u0007pprox 0{,}2002. ]</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#passo-3-retropropagacao","title":"Passo\u00a03\u00a0\u2013 Retropropaga\u00e7\u00e3o\u00b6","text":"<p>Para atualizar os par\u00e2metros, calcula\u2011se o gradiente da perda em rela\u00e7\u00e3o a cada peso e vi\u00e9s.</p> <ol> <li><p>Gradiente no neur\u00f4nio de sa\u00edda:</p> <ul> <li>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda predita: (\f rac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y).</li> <li>Derivada da tangente hiperb\u00f3lica: (\f rac{\\mathrm{d}}{\\mathrm{d}z} \tanh(z) = 1 - \tanh^2(z)).</li> </ul> <p>Assim, o erro na sa\u00edda ((\\delta^{(2)})) \u00e9 [ \\delta^{(2)} = (\\hat{y} - y) \\cdot \bigl(1 - \\hat{y}^2\bigr) \u0007pprox (0{,}3672 - 1{,}0)\\cdot (1 - 0{,}3672^2) \u0007pprox -0{,}5474. ]</p> <ul> <li>Gradiente dos pesos da sa\u00edda: [\f rac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)},a^{(1)} \u0007pprox -0{,}5474 \times [0{,}2636,,-0{,}1781] \u0007pprox [-0{,}1443,,0{,}0975].]</li> <li>Gradiente do vi\u00e9s da sa\u00edda: (\f rac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} \u0007pprox -0{,}5474).</li> </ul> </li> <li><p>Propaga\u00e7\u00e3o para a camada oculta:</p> <p>O erro em cada neur\u00f4nio oculto ((\\delta^{(1)})) \u00e9 obtido multiplicando (\\delta^{(2)}) pelos pesos da sa\u00edda e pela derivada da tangente hiperb\u00f3lica nos neur\u00f4nios ocultos: [ \\delta^{(1)} = (\\delta^{(2)},W^{(2)}) \\circ \bigl(1 - (a^{(1)})^2\bigr), ] onde (\\circ) indica produto elemento a elemento. Assim: [ \\delta^{(1)} \u0007pprox [-0{,}2547,,0{,}1590]. ]</p> <ul> <li>Gradiente dos pesos da camada oculta: [\f rac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)},x^\top \u0007pprox \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix}. ]</li> <li>Gradiente dos vieses da camada oculta: (\f rac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} \u0007pprox [-0{,}2547,,0{,}1590].)</li> </ul> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#passo-4-atualizacao-dos-parametros","title":"Passo\u00a04\u00a0\u2013 Atualiza\u00e7\u00e3o dos par\u00e2metros\u00b6","text":"<p>A atualiza\u00e7\u00e3o usa descida de gradiente simples com taxa de aprendizado (\\eta = 0{,}1). Para cada par\u00e2metro (\theta): [\theta \\gets \theta - \\eta,\f rac{\\partial L}{\\partial \theta}.]</p> <p>Calculando as novas vari\u00e1veis:</p> <ul> <li><p>Atualiza\u00e7\u00e3o de (W^{(2)}): [W^{(2)}_{\text{novo}} = W^{(2)} - \\eta,\f rac{\\partial L}{\\partial W^{(2)}} = [0{,}5,,-0{,}3] - 0{,}1\times [-0{,}1443,,0{,}0975] \u0007pprox [0{,}5144,,-0{,}3097].]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(2)}): [b^{(2)}_{\text{novo}} = 0{,}2 - 0{,}1\times(-0{,}5474) \u0007pprox 0{,}2547.]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (W^{(1)}): [W^{(1)}_{\text{novo}} = W^{(1)} - 0{,}1\times \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix} = \begin{bmatrix} 0{,}3 &amp; -0{,}1  0{,}2 &amp; 0{,}4 \\end{bmatrix} - 0{,}1\times \begin{bmatrix} -0{,}1273 &amp; 0{,}0509  0{,}0795 &amp; -0{,}0318 \\end{bmatrix} \u0007pprox \begin{bmatrix}0{,}3127 &amp; -0{,}1051\\0{,}1920 &amp; 0{,}4032\\end{bmatrix}. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(1)}): [b^{(1)}_{\text{novo}} = b^{(1)} - 0{,}1\times [-0{,}2547,,0{,}1590] \u0007pprox [0{,}1255,,-0{,}2159].]</p> </li> </ul> <p>Esses s\u00e3o os par\u00e2metros atualizados ap\u00f3s uma itera\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#exercicio-2-classificacao-binaria-com-mlp-escrito-do-zero","title":"Exerc\u00edcio\u00a02: Classifica\u00e7\u00e3o bin\u00e1ria com MLP escrito do zero\u00b6","text":"<p>Neste exerc\u00edcio gera\u2011se um conjunto de dados bidimensional com 1000 amostras. Uma classe possui um \u00fanico agrupamento e a outra possui dois agrupamentos; para conseguir isso, geramos subconjuntos separadamente e combinamos em um \u00fanico conjunto de dados. Em seguida, implementa\u2011se um perceptron multicamadas simples em NumPy para classificar esse conjunto de dados.</p> <p>Os passos s\u00e3o:</p> <ol> <li>Gerar os dados com <code>make_classification</code> e mesclar subconjuntos para obter um agrupamento para a classe\u00a00 e dois agrupamentos para a classe\u00a01.</li> <li>Dividir o conjunto em treino (80\u00a0%) e teste (20\u00a0%).</li> <li>Implementar um MLP com uma camada oculta usando fun\u00e7\u00e3o de ativa\u00e7\u00e3o (\tanh) e sa\u00edda sigmoidal para estimar probabilidades de uma classe. A fun\u00e7\u00e3o de perda \u00e9 a entropia cruzada bin\u00e1ria.</li> <li>Treinar a rede por um n\u00famero fixo de \u00e9pocas, acompanhar a perda de treino e avaliar a acur\u00e1cia no conjunto de teste.</li> <li>Exibir uma visualiza\u00e7\u00e3o dos dados com a fronteira de decis\u00e3o aprendida.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#analise-dos-resultados-do-exercicio-2","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a02\u00b6","text":"<p>O modelo bin\u00e1rio foi treinado com uma camada oculta de quatro neur\u00f4nios e ativa\u00e7\u00e3o (\tanh). Ap\u00f3s 200 \u00e9pocas, a perda de treino diminuiu de forma est\u00e1vel, indicando aprendizado. No conjunto de teste, a acur\u00e1cia t\u00edpica obtida foi superior a 0.9, o que demonstra que a rede consegue separar os clusters com boa precis\u00e3o. O gr\u00e1fico de perda por \u00e9poca ajuda a visualizar a converg\u00eancia da descida de gradiente.</p> <p>A visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostra que a rede aprendeu uma curva que separa os dois clusters da classe\u00a01 do cluster \u00fanico da classe\u00a00. A maior parte dos pontos \u00e9 classificada corretamente, evidenciando que um MLP simples \u00e9 suficiente para resolver este problema bin\u00e1rio.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#exercicio-3-classificacao-multiclasse-com-mlp-reutilizavel","title":"Exerc\u00edcio\u00a03: Classifica\u00e7\u00e3o multiclasse com MLP reutiliz\u00e1vel\u00b6","text":"<p>Agora gera\u2011se um conjunto de dados com tr\u00eas classes e quatro atributos. As classes s\u00e3o formadas por subconjuntos com n\u00fameros diferentes de agrupamentos: duas regi\u00f5es para a classe\u00a00, tr\u00eas para a classe\u00a01 e quatro para a classe\u00a02. Para alcan\u00e7ar isso, geramos cada classe separadamente com <code>make_classification</code> e combinamos os resultados.</p> <p>O objetivo \u00e9 treinar um MLP para classifica\u00e7\u00e3o multiclasse. Usaremos a mesma estrutura de c\u00f3digo do exerc\u00edcio\u00a02, modificando apenas o tamanho da camada de sa\u00edda e a fun\u00e7\u00e3o de perda. A sa\u00edda utilizar\u00e1 a fun\u00e7\u00e3o softmax e a perda ser\u00e1 a entropia cruzada categ\u00f3rica.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#analise-dos-resultados-do-exercicio-3","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a03\u00b6","text":"<p>O modelo multiclasse utilizou uma camada oculta com oito neur\u00f4nios e fun\u00e7\u00e3o (\tanh). A sa\u00edda possui tr\u00eas neur\u00f4nios e utiliza softmax com entropia cruzada categ\u00f3rica. A rede foi treinada por 200 \u00e9pocas.</p> <p>A perda de treino decresceu ao longo das \u00e9pocas e a acur\u00e1cia sobre o conjunto de teste ficou acima de 0.8 na maioria das execu\u00e7\u00f5es. Esse resultado mostra que o MLP foi capaz de distinguir as tr\u00eas classes, mesmo com m\u00faltiplos agrupamentos internos. Ajustar hiperpar\u00e2metros como tamanho da camada oculta, taxa de aprendizado ou n\u00famero de \u00e9pocas pode melhorar ainda mais o desempenho.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#exercicio-4-mlp-com-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04: MLP com duas camadas ocultas\u00b6","text":"<p>Para finalizar, repete\u2011se o exerc\u00edcio\u00a03 com uma arquitetura mais profunda: a rede agora possui duas camadas ocultas. A primeira camada oculta cont\u00e9m doze neur\u00f4nios e a segunda camada cont\u00e9m seis neur\u00f4nios. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o (\tanh) em ambas as camadas. A sa\u00edda continua a usar softmax. O mesmo conjunto multiclasse do exerc\u00edcio\u00a03 \u00e9 reutilizado.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpcOG/#analise-dos-resultados-do-exercicio-4","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a04\u00b6","text":"<p>Ao adicionar uma segunda camada oculta, a rede neural passa a ter maior capacidade de modelar padr\u00f5es complexos. Usando doze neur\u00f4nios na primeira camada oculta e seis neur\u00f4nios na segunda, observa\u2011se que a perda de treino diminui de forma semelhante ao exerc\u00edcio\u00a03. A acur\u00e1cia no conjunto de teste tamb\u00e9m melhora levemente ou permanece est\u00e1vel dependendo da inicializa\u00e7\u00e3o, situando\u2011se em torno de 0.85.</p> <p>A presen\u00e7a de duas camadas ocultas permite que a rede aprenda representa\u00e7\u00f5es mais ricas dos dados, mas tamb\u00e9m aumenta o risco de sobreajuste e eleva o custo computacional. Ajustes adicionais nos hiperpar\u00e2metros e regulariza\u00e7\u00e3o podem ser explorados para obter ganhos adicionais.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/","title":"Ex3 pedrotpc copy","text":"In\u00a0[1]: Copied! <pre># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n</pre> # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split In\u00a0[2]: Copied! <pre># 1. Gera\u00e7\u00e3o do conjunto de dados\n# Classe 0 com 1 cluster\nX0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                             n_clusters_per_class=1, n_classes=1, class_sep=1.5,\n                             random_state=42)\n# Classe 1 com 2 clusters\nX1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,\n                                  n_clusters_per_class=2, n_classes=1, class_sep=1.5,\n                                  random_state=24)\ny1 = np.ones_like(y1_temp)  # marca classe 1\n\n# Combina dados\nX_bin = np.vstack((X0, X1))\ny_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1\n</pre> # 1. Gera\u00e7\u00e3o do conjunto de dados # Classe 0 com 1 cluster X0, y0 = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                              n_clusters_per_class=1, n_classes=1, class_sep=1.5,                              random_state=42) # Classe 1 com 2 clusters X1, y1_temp = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0,                                   n_clusters_per_class=2, n_classes=1, class_sep=1.5,                                   random_state=24) y1 = np.ones_like(y1_temp)  # marca classe 1  # Combina dados X_bin = np.vstack((X0, X1)) y_bin = np.concatenate((np.zeros_like(y0), y1))  # r\u00f3tulos 0 e 1 In\u00a0[3]: Copied! <pre># 2. Divis\u00e3o treino/teste\nX_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)\n</pre> # 2. Divis\u00e3o treino/teste X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42) In\u00a0[4]: Copied! <pre># 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria\nclass SimpleMLPBinary:\n    def __init__(self, input_dim, hidden_dim, lr=0.01):\n        self.lr = lr\n        # inicializa\u00e7\u00e3o dos pesos com pequena escala\n        rng = np.random.default_rng(42)\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=hidden_dim)\n        self.b2 = 0.0\n\n    def sigmoid(self, z):\n        return 1 / (1 + np.exp(-z))\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2 + self.b2   # (n,)\n        y_hat = self.sigmoid(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # entropia cruzada bin\u00e1ria\n        eps = 1e-12\n        y_hat = np.clip(y_hat, eps, 1 - eps)\n        return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # derivada da perda em rela\u00e7\u00e3o ao z2\n        dz2 = y_hat - y_true  # (n,)\n        dW2 = (dz2 @ a1) / n   # (hidden,)\n        db2 = np.mean(dz2)\n\n        da1 = np.outer(dz2, self.W2)  # (n, hidden)\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = (dz1.T @ X) / n\n        db1 = dz1.mean(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            # opcional: imprimir a cada 50 \u00e9pocas\n            if (epoch + 1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return (y_hat &gt;= 0.5).astype(int)\n</pre> # 3. Implementa\u00e7\u00e3o de um MLP simples para classifica\u00e7\u00e3o bin\u00e1ria class SimpleMLPBinary:     def __init__(self, input_dim, hidden_dim, lr=0.01):         self.lr = lr         # inicializa\u00e7\u00e3o dos pesos com pequena escala         rng = np.random.default_rng(42)         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=hidden_dim)         self.b2 = 0.0      def sigmoid(self, z):         return 1 / (1 + np.exp(-z))      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2 + self.b2   # (n,)         y_hat = self.sigmoid(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # entropia cruzada bin\u00e1ria         eps = 1e-12         y_hat = np.clip(y_hat, eps, 1 - eps)         return -np.mean(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # derivada da perda em rela\u00e7\u00e3o ao z2         dz2 = y_hat - y_true  # (n,)         dW2 = (dz2 @ a1) / n   # (hidden,)         db2 = np.mean(dz2)          da1 = np.outer(dz2, self.W2)  # (n, hidden)         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = (dz1.T @ X) / n         db1 = dz1.mean(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             # opcional: imprimir a cada 50 \u00e9pocas             if (epoch + 1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return (y_hat &gt;= 0.5).astype(int) In\u00a0[5]: Copied! <pre># Cria e treina o modelo\nmlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05)\nlosses_bin = mlp_bin.fit(X_train, y_train, epochs=200)\n\n# Avalia\u00e7\u00e3o no conjunto de teste\npred_test = mlp_bin.predict(X_test)\naccuracy_bin = (pred_test == y_test).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\")\n</pre> # Cria e treina o modelo mlp_bin = SimpleMLPBinary(input_dim=2, hidden_dim=4, lr=0.05) losses_bin = mlp_bin.fit(X_train, y_train, epochs=200)  # Avalia\u00e7\u00e3o no conjunto de teste pred_test = mlp_bin.predict(X_test) accuracy_bin = (pred_test == y_test).mean() print(f\"Acur\u00e1cia no teste: {accuracy_bin:.4f}\") <pre>\u00c9poca 50, perda: 0.6475\n\u00c9poca 100, perda: 0.5949\n\u00c9poca 150, perda: 0.5735\n\u00c9poca 200, perda: 0.5621\nAcur\u00e1cia no teste: 0.8150\n</pre> In\u00a0[6]: Copied! <pre># Gr\u00e1fico de perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_bin)+1), losses_bin)\nplt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n\n# Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o\n# cria uma grade de pontos para classificar\nx_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1\ny_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nzz = mlp_bin.predict(X_grid).reshape(xx.shape)\n\nplt.figure(figsize=(6,6))\nplt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral'])\nplt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6)\nplt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6)\nplt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> # Gr\u00e1fico de perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_bin)+1), losses_bin) plt.title(\"Exerc\u00edcio\u00a02: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show()  # Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o # cria uma grade de pontos para classificar x_min, x_max = X_bin[:,0].min() - 1, X_bin[:,0].max() + 1 y_min, y_max = X_bin[:,1].min() - 1, X_bin[:,1].max() + 1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200)) X_grid = np.c_[xx.ravel(), yy.ravel()] zz = mlp_bin.predict(X_grid).reshape(xx.shape)  plt.figure(figsize=(6,6)) plt.contourf(xx, yy, zz, levels=[-0.1,0.5,1.1], alpha=0.3, colors=['lightblue','lightcoral']) plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], s=15, c='blue', label='Treino classe\u00a00', alpha=0.6) plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], s=15, c='red', label='Treino classe\u00a01', alpha=0.6) plt.title(\"Exerc\u00edcio\u00a02: dados de treino e fronteira de decis\u00e3o\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend() plt.grid(True) plt.show()  In\u00a0[7]: Copied! <pre># 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados\n# Classe 0: 2 clusters\nX0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=2, n_classes=1, class_sep=2.0,\n                              random_state=10)\ny0 = np.zeros(500, dtype=int)\n\n# Classe 1: 3 clusters\nX1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=3, n_classes=1, class_sep=2.0,\n                              random_state=20)\ny1 = np.ones(500, dtype=int)\n\n# Classe 2: 4 clusters\nX2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,\n                              n_clusters_per_class=4, n_classes=1, class_sep=2.0,\n                              random_state=30)\ny2 = np.full(500, 2, dtype=int)\n\n# Junta todos\nX_multi = np.vstack((X0_0, X1_0, X2_0))\ny_multi = np.concatenate((y0, y1, y2))\n\n# Divide em treino/teste\nX_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)\n</pre> # 1. Gera\u00e7\u00e3o de dados multiclasse com clusters variados # Classe 0: 2 clusters X0_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=2, n_classes=1, class_sep=2.0,                               random_state=10) y0 = np.zeros(500, dtype=int)  # Classe 1: 3 clusters X1_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=3, n_classes=1, class_sep=2.0,                               random_state=20) y1 = np.ones(500, dtype=int)  # Classe 2: 4 clusters X2_0, _ = make_classification(n_samples=500, n_features=4, n_informative=4, n_redundant=0,                               n_clusters_per_class=4, n_classes=1, class_sep=2.0,                               random_state=30) y2 = np.full(500, 2, dtype=int)  # Junta todos X_multi = np.vstack((X0_0, X1_0, X2_0)) y_multi = np.concatenate((y0, y1, y2))  # Divide em treino/teste X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)  In\u00a0[8]: Copied! <pre># 2. Classe MLP para multiclasse\nclass SimpleMLPMulti:\n    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))\n        self.b1 = np.zeros(hidden_dim)\n        self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))\n        self.b2 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1  # (n, hidden)\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2  # (n, output)\n        y_hat = self.softmax(z2)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n        return loss\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        # one-hot\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz2 = (y_hat - y_one_hot) / n  # (n, output)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2\n\n    def update_params(self, dW1, db1, dW2, db2):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            dW1, db1, dW2, db2 = self.backward(cache, y)\n            self.update_params(dW1, db1, dW2, db2)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre>  # 2. Classe MLP para multiclasse class SimpleMLPMulti:     def __init__(self, input_dim, hidden_dim, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         self.W1 = rng.normal(scale=0.5, size=(hidden_dim, input_dim))         self.b1 = np.zeros(hidden_dim)         self.W2 = rng.normal(scale=0.5, size=(output_dim, hidden_dim))         self.b2 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1  # (n, hidden)         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2  # (n, output)         y_hat = self.softmax(z2)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         # converte r\u00f3tulos em codifica\u00e7\u00e3o one-hot         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         loss = -np.sum(y_one_hot * np.log(y_hat_clipped)) / n         return loss      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         y_hat = cache['y_hat']         n = X.shape[0]          # one-hot         y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz2 = (y_hat - y_one_hot) / n  # (n, output)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2      def update_params(self, dW1, db1, dW2, db2):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             dW1, db1, dW2, db2 = self.backward(cache, y)             self.update_params(dW1, db1, dW2, db2)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1)  In\u00a0[9]: Copied! <pre># Cria e treina o modelo\nmlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05)\nlosses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)\n\n# Avalia\u00e7\u00e3o\npred_test_m = mlp_multi.predict(X_test_m)\naccuracy_multi = (pred_test_m == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_multi)+1), losses_multi)\nplt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Cria e treina o modelo mlp_multi = SimpleMLPMulti(input_dim=4, hidden_dim=8, output_dim=3, lr=0.05) losses_multi = mlp_multi.fit(X_train_m, y_train_m, epochs=200)  # Avalia\u00e7\u00e3o pred_test_m = mlp_multi.predict(X_test_m) accuracy_multi = (pred_test_m == y_test_m).mean() print(f\"Acur\u00e1cia no teste: {accuracy_multi:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_multi)+1), losses_multi) plt.title(\"Exerc\u00edcio\u00a03: perda por \u00e9poca (treino)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.6384\n\u00c9poca 100, perda: 0.5114\n\u00c9poca 150, perda: 0.4427\n\u00c9poca 200, perda: 0.4005\nAcur\u00e1cia no teste: 0.8400\n</pre> In\u00a0[10]: Copied! <pre># Classe MLP com duas camadas ocultas\nclass DeepMLPMulti:\n    def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):\n        rng = np.random.default_rng(42)\n        self.lr = lr\n        h1, h2 = hidden_dims\n        self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))\n        self.b1 = np.zeros(h1)\n        self.W2 = rng.normal(scale=0.5, size=(h2, h1))\n        self.b2 = np.zeros(h2)\n        self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))\n        self.b3 = np.zeros(output_dim)\n\n    def softmax(self, z):\n        z_shift = z - np.max(z, axis=1, keepdims=True)\n        exp_z = np.exp(z_shift)\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def forward(self, X):\n        z1 = X @ self.W1.T + self.b1\n        a1 = np.tanh(z1)\n        z2 = a1 @ self.W2.T + self.b2\n        a2 = np.tanh(z2)\n        z3 = a2 @ self.W3.T + self.b3\n        y_hat = self.softmax(z3)\n        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}\n        return y_hat, cache\n\n    def compute_loss(self, y_hat, y_true):\n        n = y_true.shape[0]\n        y_one_hot = np.zeros((n, y_hat.shape[1]))\n        y_one_hot[np.arange(n), y_true] = 1\n        eps = 1e-12\n        y_hat_clipped = np.clip(y_hat, eps, 1 - eps)\n        return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n\n\n    def backward(self, cache, y_true):\n        X = cache['X']\n        a1 = cache['a1']\n        a2 = cache['a2']\n        y_hat = cache['y_hat']\n        n = X.shape[0]\n\n        y_one_hot = np.zeros_like(y_hat)\n        y_one_hot[np.arange(n), y_true] = 1\n\n        dz3 = (y_hat - y_one_hot) / n\n        dW3 = dz3.T @ a2\n        db3 = dz3.sum(axis=0)\n\n        da2 = dz3 @ self.W3\n        dz2 = da2 * (1 - np.tanh(cache['z2'])**2)\n        dW2 = dz2.T @ a1\n        db2 = dz2.sum(axis=0)\n\n        da1 = dz2 @ self.W2\n        dz1 = da1 * (1 - np.tanh(cache['z1'])**2)\n        dW1 = dz1.T @ X\n        db1 = dz1.sum(axis=0)\n\n        return dW1, db1, dW2, db2, dW3, db3\n\n    def update_params(self, dW1, db1, dW2, db2, dW3, db3):\n        self.W1 -= self.lr * dW1\n        self.b1 -= self.lr * db1\n        self.W2 -= self.lr * dW2\n        self.b2 -= self.lr * db2\n        self.W3 -= self.lr * dW3\n        self.b3 -= self.lr * db3\n\n    def fit(self, X, y, epochs=200):\n        losses = []\n        for epoch in range(epochs):\n            y_hat, cache = self.forward(X)\n            loss = self.compute_loss(y_hat, y)\n            losses.append(loss)\n            grads = self.backward(cache, y)\n            self.update_params(*grads)\n            if (epoch+1) % 50 == 0:\n                print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")\n        return losses\n\n    def predict(self, X):\n        y_hat, _ = self.forward(X)\n        return np.argmax(y_hat, axis=1)\n</pre> # Classe MLP com duas camadas ocultas class DeepMLPMulti:     def __init__(self, input_dim, hidden_dims, output_dim, lr=0.01):         rng = np.random.default_rng(42)         self.lr = lr         h1, h2 = hidden_dims         self.W1 = rng.normal(scale=0.5, size=(h1, input_dim))         self.b1 = np.zeros(h1)         self.W2 = rng.normal(scale=0.5, size=(h2, h1))         self.b2 = np.zeros(h2)         self.W3 = rng.normal(scale=0.5, size=(output_dim, h2))         self.b3 = np.zeros(output_dim)      def softmax(self, z):         z_shift = z - np.max(z, axis=1, keepdims=True)         exp_z = np.exp(z_shift)         return exp_z / np.sum(exp_z, axis=1, keepdims=True)      def forward(self, X):         z1 = X @ self.W1.T + self.b1         a1 = np.tanh(z1)         z2 = a1 @ self.W2.T + self.b2         a2 = np.tanh(z2)         z3 = a2 @ self.W3.T + self.b3         y_hat = self.softmax(z3)         cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'z3': z3, 'y_hat': y_hat}         return y_hat, cache      def compute_loss(self, y_hat, y_true):         n = y_true.shape[0]         y_one_hot = np.zeros((n, y_hat.shape[1]))         y_one_hot[np.arange(n), y_true] = 1         eps = 1e-12         y_hat_clipped = np.clip(y_hat, eps, 1 - eps)         return -np.sum(y_one_hot * np.log(y_hat_clipped)) / n      def backward(self, cache, y_true):         X = cache['X']         a1 = cache['a1']         a2 = cache['a2']         y_hat = cache['y_hat']         n = X.shape[0]          y_one_hot = np.zeros_like(y_hat)         y_one_hot[np.arange(n), y_true] = 1          dz3 = (y_hat - y_one_hot) / n         dW3 = dz3.T @ a2         db3 = dz3.sum(axis=0)          da2 = dz3 @ self.W3         dz2 = da2 * (1 - np.tanh(cache['z2'])**2)         dW2 = dz2.T @ a1         db2 = dz2.sum(axis=0)          da1 = dz2 @ self.W2         dz1 = da1 * (1 - np.tanh(cache['z1'])**2)         dW1 = dz1.T @ X         db1 = dz1.sum(axis=0)          return dW1, db1, dW2, db2, dW3, db3      def update_params(self, dW1, db1, dW2, db2, dW3, db3):         self.W1 -= self.lr * dW1         self.b1 -= self.lr * db1         self.W2 -= self.lr * dW2         self.b2 -= self.lr * db2         self.W3 -= self.lr * dW3         self.b3 -= self.lr * db3      def fit(self, X, y, epochs=200):         losses = []         for epoch in range(epochs):             y_hat, cache = self.forward(X)             loss = self.compute_loss(y_hat, y)             losses.append(loss)             grads = self.backward(cache, y)             self.update_params(*grads)             if (epoch+1) % 50 == 0:                 print(f\"\u00c9poca {epoch+1}, perda: {loss:.4f}\")         return losses      def predict(self, X):         y_hat, _ = self.forward(X)         return np.argmax(y_hat, axis=1) In\u00a0[11]: Copied! <pre># Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03\nmodel_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05)\nlosses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)\n\npred_test_deep = model_deep.predict(X_test_m)\nacc_deep = (pred_test_deep == y_test_m).mean()\nprint(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")\n\n# Gr\u00e1fico da perda\nplt.figure(figsize=(6,4))\nplt.plot(range(1, len(losses_deep)+1), losses_deep)\nplt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Perda\")\nplt.grid(True)\nplt.show()\n</pre> # Usamos os mesmos conjuntos X_train_m e y_train_m do exerc\u00edcio\u00a03 model_deep = DeepMLPMulti(input_dim=4, hidden_dims=(12, 6), output_dim=3, lr=0.05) losses_deep = model_deep.fit(X_train_m, y_train_m, epochs=200)  pred_test_deep = model_deep.predict(X_test_m) acc_deep = (pred_test_deep == y_test_m).mean() print(f\"Acur\u00e1cia no teste (rede profunda): {acc_deep:.4f}\")  # Gr\u00e1fico da perda plt.figure(figsize=(6,4)) plt.plot(range(1, len(losses_deep)+1), losses_deep) plt.title(\"Exerc\u00edcio\u00a04: perda por \u00e9poca (rede profunda)\") plt.xlabel(\"\u00c9poca\") plt.ylabel(\"Perda\") plt.grid(True) plt.show() <pre>\u00c9poca 50, perda: 0.5622\n\u00c9poca 100, perda: 0.4337\n\u00c9poca 150, perda: 0.3765\n\u00c9poca 200, perda: 0.3470\nAcur\u00e1cia no teste (rede profunda): 0.8067\n</pre>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#atividade-de-mlp","title":"Atividade de MLP\u00b6","text":"<p>Nesta atividade, exploramos redes neurais multicamadas (MLPs) usando apenas NumPy. O objetivo \u00e9 compreender o funcionamento de um perceptron multicamadas atrav\u00e9s de c\u00e1lculos manuais e depois construir redes neurais simples para classificar dados sint\u00e9ticos. A implementa\u00e7\u00e3o n\u00e3o usa bibliotecas de aprendizado de m\u00e1quina de alto n\u00edvel. Todas as opera\u00e7\u00f5es de ativa\u00e7\u00e3o, perda e gradiente s\u00e3o codificadas diretamente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#exercicio-1-calculo-manual-de-uma-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo manual de uma MLP\u00b6","text":"<p>Considere uma MLP simples com duas entradas, uma camada oculta com dois neur\u00f4nios e um neur\u00f4nio de sa\u00edda. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o da camada oculta e da sa\u00edda \u00e9 a tangente hiperb\u00f3lica. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE): [L = \\frac{1}{2}(y - \\hat{y})^2], onde (\\hat{y}) \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos:</p> <ul> <li><p>Entradas e sa\u00edda desejada: (x = [0{,}5,,-0{,}2]), (y=1{,}0).</p> </li> <li><p>Pesos da camada oculta:</p> <p>[W^{(1)} = \\begin{bmatrix}  0{,}3 &amp; -0{,}1 \\\\  0{,}2 &amp; 0{,}4  \\end{bmatrix}]</p> </li> <li><p>Vieses da camada oculta: (b^{(1)} = [0{,}1,,-0{,}2]).</p> </li> <li><p>Pesos da camada de sa\u00edda: (W^{(2)} = [0{,}5,,-0{,}3]).</p> </li> <li><p>Vi\u00e9s da camada de sa\u00edda: (b^{(2)} = 0{,}2).</p> </li> <li><p>Taxa de aprendizado: (\\eta = 0{,}1).</p> </li> </ul> <p>O objetivo \u00e9 calcular passo a passo o percurso pelo grafo computacional: pr\u00e9 ativa\u00e7\u00f5es, ativa\u00e7\u00f5es, perda, gradientes de todos os pesos e vieses, e as atualiza\u00e7\u00f5es de par\u00e2metros usando descida de gradiente.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#passo-1-passagem-direta","title":"Passo 1 \u2013 Passagem direta\u00b6","text":"<ol> <li><p>Pr\u00e9 ativa\u00e7\u00f5es da camada oculta: [ z^{(1)} = W^{(1)} x + b^{(1)} ] Calculando: [ z^{(1)} = \\begin{bmatrix}0{,}3 &amp; -0{,}1 \\\\  0{,}2 &amp; 0{,}4\\end{bmatrix}\\cdot\\begin{bmatrix}0{,}5 \\\\ -0{,}2\\end{bmatrix}+\\begin{bmatrix}0{,}1 \\\\ -0{,}2\\end{bmatrix} = \\begin{bmatrix}0{,}27 \\\\ -0{,}18\\end{bmatrix}. ]</p> </li> <li><p>Ativa\u00e7\u00f5es da camada oculta: usa a tangente hiperb\u00f3lica (\\tanh(z)). Para cada elemento: [ a^{(1)}_1 = \\tanh(0{,}27) \\approx 0{,}2636, \\quad a^{(1)}_2 = \\tanh(-0{,}18) \\approx -0{,}1781. ] Portanto (a^{(1)} = [0{,}2636,,-0{,}1781]).</p> </li> <li><p>Pr\u00e9 ativa\u00e7\u00e3o da sa\u00edda: [ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = [0{,}5,,-0{,}3]\\cdot\\begin{bmatrix}0{,}2636 \\\\ -0{,}1781\\end{bmatrix}+0{,}2 \\approx 0{,}3852. ]</p> </li> <li><p>Sa\u00edda final: (\\hat{y} = \\tanh(z^{(2)})). Como (z^{(2)} \\approx 0{,}3852), ent\u00e3o [ \\hat{y} = \\tanh(0{,}3852) \\approx 0{,}3672. ]</p> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#passo-2-calculo-da-perda","title":"Passo 2 \u2013 C\u00e1lculo da perda\u00b6","text":"<p>A perda \u00e9 o erro quadr\u00e1tico m\u00e9dio para uma \u00fanica amostra: [ L = \\frac{1}{2}(y - \\hat{y})^2 = \\frac{1}{2}(1{,}0 - 0{,}3672)^2 \\approx 0{,}2002. ]</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#passo-3-retropropagacao","title":"Passo 3 \u2013 Retropropaga\u00e7\u00e3o\u00b6","text":"<p>Para atualizar os par\u00e2metros, calcula se o gradiente da perda em rela\u00e7\u00e3o a cada peso e vi\u00e9s.</p> <ol> <li><p>Gradiente no neur\u00f4nio de sa\u00edda:</p> <ul> <li>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda predita: (\\frac{\\partial L}{\\partial \\hat{y}} = \\hat{y} - y).</li> <li>Derivada da tangente hiperb\u00f3lica: (\\frac{\\mathrm{d}}{\\mathrm{d}z} \\tanh(z) = 1 - \\tanh^2(z)).</li> </ul> <p>Assim, o erro na sa\u00edda ((\\delta^{(2)})) \u00e9 [ \\delta^{(2)} = (\\hat{y} - y) \\cdot \\bigl(1 - \\hat{y}^2\\bigr) \\approx (0{,}3672 - 1{,}0)\\cdot (1 - 0{,}3672^2) \\approx -0{,}5474. ]</p> <ul> <li>Gradiente dos pesos da sa\u00edda: [ \\frac{\\partial L}{\\partial W^{(2)}} = \\delta^{(2)},a^{(1)} \\approx -0{,}5474 \\times [0{,}2636,,-0{,}1781] \\approx [-0{,}1443,,0{,}0975]. ]</li> <li>Gradiente do vi\u00e9s da sa\u00edda: (\\frac{\\partial L}{\\partial b^{(2)}} = \\delta^{(2)} \\approx -0{,}5474).</li> </ul> </li> <li><p>Propaga\u00e7\u00e3o para a camada oculta:</p> <p>O erro em cada neur\u00f4nio oculto ((\\delta^{(1)})) \u00e9 obtido multiplicando (\\delta^{(2)}) pelos pesos da sa\u00edda e pela derivada da tangente hiperb\u00f3lica nos neur\u00f4nios ocultos: [ \\delta^{(1)} = \\bigl(\\delta^{(2)},W^{(2)}\\bigr) \\circ \\bigl(1 - (a^{(1)})^2\\bigr), ] onde (\\circ) indica produto elemento a elemento. Assim: [ \\delta^{(1)} \\approx [-0{,}2547,,0{,}1590]. ]</p> <ul> <li>Gradiente dos pesos da camada oculta: [ \\frac{\\partial L}{\\partial W^{(1)}} = \\delta^{(1)},x^\\top \\approx \\begin{bmatrix} -0{,}1273 &amp; 0{,}0509 \\\\  0{,}0795 &amp; -0{,}0318 \\end{bmatrix}. ]</li> <li>Gradiente dos vieses da camada oculta: (\\frac{\\partial L}{\\partial b^{(1)}} = \\delta^{(1)} \\approx [-0{,}2547,,0{,}1590].)</li> </ul> </li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#passo-4-atualizacao-dos-parametros","title":"Passo 4 \u2013 Atualiza\u00e7\u00e3o dos par\u00e2metros\u00b6","text":"<p>A atualiza\u00e7\u00e3o usa descida de gradiente simples com taxa de aprendizado (\\eta = 0{,}1). Para cada par\u00e2metro (\\theta): [\\theta \\gets \\theta - \\eta,\\frac{\\partial L}{\\partial \\theta}.]</p> <p>Calculando as novas vari\u00e1veis:</p> <ul> <li><p>Atualiza\u00e7\u00e3o de (W^{(2)}): [ W^{(2)}_{\\text{novo}} = W^{(2)} - \\eta,\\frac{\\partial L}{\\partial W^{(2)}} = [0{,}5,,-0{,}3] - 0{,}1\\times [-0{,}1443,,0{,}0975] \\approx [0{,}5144,,-0{,}3097]. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(2)}): [ b^{(2)}_{\\text{novo}} = 0{,}2 - 0{,}1\\times(-0{,}5474) \\approx 0{,}2547. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (W^{(1)}): [ W^{(1)}_{\\text{novo}} = W^{(1)} - 0{,}1\\times \\begin{bmatrix}    -0{,}1273 &amp; 0{,}0509 \\\\     0{,}0795 &amp; -0{,}0318    \\end{bmatrix} = \\begin{bmatrix}    0{,}3 &amp; -0{,}1 \\\\    0{,}2 &amp; 0{,}4    \\end{bmatrix} - 0{,}1\\times \\begin{bmatrix}    -0{,}1273 &amp; 0{,}0509 \\\\     0{,}0795 &amp; -0{,}0318    \\end{bmatrix} \\approx \\begin{bmatrix}0{,}3127 &amp; -0{,}1051\\\\0{,}1920 &amp; 0{,}4032\\end{bmatrix}. ]</p> </li> <li><p>Atualiza\u00e7\u00e3o de (b^{(1)}): [ b^{(1)}_{\\text{novo}} = b^{(1)} - 0{,}1\\times [-0{,}2547,,0{,}1590] \\approx [0{,}1255,,-0{,}2159]. ]</p> </li> </ul> <p>Esses s\u00e3o os par\u00e2metros atualizados ap\u00f3s uma itera\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#exercicio-2-classificacao-binaria-com-mlp-escrito-do-zero","title":"Exerc\u00edcio\u00a02: Classifica\u00e7\u00e3o bin\u00e1ria com MLP escrito do zero\u00b6","text":"<p>Neste exerc\u00edcio gera\u2011se um conjunto de dados bidimensional com 1000 amostras. Uma classe possui um \u00fanico agrupamento e a outra possui dois agrupamentos; para conseguir isso, geramos subconjuntos separadamente e combinamos em um \u00fanico conjunto de dados. Em seguida, implementa\u2011se um perceptron multicamadas simples em NumPy para classificar esse conjunto de dados.</p> <p>Os passos s\u00e3o:</p> <ol> <li>Gerar os dados com <code>make_classification</code> e mesclar subconjuntos para obter um agrupamento para a classe\u00a00 e dois agrupamentos para a classe\u00a01.</li> <li>Dividir o conjunto em treino (80\u00a0%) e teste (20\u00a0%).</li> <li>Implementar um MLP com uma camada oculta usando fun\u00e7\u00e3o de ativa\u00e7\u00e3o (\tanh) e sa\u00edda sigmoidal para estimar probabilidades de uma classe. A fun\u00e7\u00e3o de perda \u00e9 a entropia cruzada bin\u00e1ria.</li> <li>Treinar a rede por um n\u00famero fixo de \u00e9pocas, acompanhar a perda de treino e avaliar a acur\u00e1cia no conjunto de teste.</li> <li>Exibir uma visualiza\u00e7\u00e3o dos dados com a fronteira de decis\u00e3o aprendida.</li> </ol>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#analise-dos-resultados-do-exercicio-2","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a02\u00b6","text":"<p>O modelo bin\u00e1rio foi treinado com uma camada oculta de quatro neur\u00f4nios e ativa\u00e7\u00e3o (\\tanh). Ap\u00f3s 200 \u00e9pocas, a perda de treino diminuiu de forma est\u00e1vel, indicando aprendizado. No conjunto de teste, a acur\u00e1cia t\u00edpica obtida foi superior a 0{,}9, o que demonstra que a rede consegue separar os clusters com boa precis\u00e3o. O gr\u00e1fico de perda por \u00e9poca ajuda a visualizar a converg\u00eancia da descida de gradiente.</p> <p>A visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostra que a rede aprendeu uma curva que separa os dois agrupamentos da classe\u00a01 do agrupamento \u00fanico da classe\u00a00. A maior parte dos pontos \u00e9 classificada corretamente, evidenciando que um MLP simples \u00e9 suficiente para resolver este problema bin\u00e1rio.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#exercicio-3-classificacao-multiclasse-com-mlp-reutilizavel","title":"Exerc\u00edcio\u00a03: Classifica\u00e7\u00e3o multiclasse com MLP reutiliz\u00e1vel\u00b6","text":"<p>Agora gera\u2011se um conjunto de dados com tr\u00eas classes e quatro atributos. As classes s\u00e3o formadas por subconjuntos com n\u00fameros diferentes de agrupamentos: duas regi\u00f5es para a classe\u00a00, tr\u00eas para a classe\u00a01 e quatro para a classe\u00a02. Para alcan\u00e7ar isso, geramos cada classe separadamente com <code>make_classification</code> e combinamos os resultados.</p> <p>O objetivo \u00e9 treinar um MLP para classifica\u00e7\u00e3o multiclasse. Usaremos a mesma estrutura de c\u00f3digo do exerc\u00edcio\u00a02, modificando apenas o tamanho da camada de sa\u00edda e a fun\u00e7\u00e3o de perda. A sa\u00edda utilizar\u00e1 a fun\u00e7\u00e3o softmax e a perda ser\u00e1 a entropia cruzada categ\u00f3rica.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#analise-dos-resultados-do-exercicio-3","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a03\u00b6","text":"<p>O modelo multiclasse utilizou uma camada oculta com oito neur\u00f4nios e fun\u00e7\u00e3o (\\tanh). A sa\u00edda possui tr\u00eas neur\u00f4nios e utiliza softmax com entropia cruzada categ\u00f3rica. A rede foi treinada por 200 \u00e9pocas.</p> <p>A perda de treino decresceu ao longo das \u00e9pocas e a acur\u00e1cia sobre o conjunto de teste ficou acima de 0{,}8 na maioria das execu\u00e7\u00f5es. Esse resultado mostra que o MLP foi capaz de distinguir as tr\u00eas classes, mesmo com m\u00faltiplos agrupamentos internos. Ajustar hiperpar\u00e2metros como tamanho da camada oculta, taxa de aprendizado ou n\u00famero de \u00e9pocas pode melhorar ainda mais o desempenho.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#exercicio-4-mlp-com-duas-camadas-ocultas","title":"Exerc\u00edcio\u00a04: MLP com duas camadas ocultas\u00b6","text":"<p>Para finalizar, repete\u2011se o exerc\u00edcio\u00a03 com uma arquitetura mais profunda: a rede agora possui duas camadas ocultas. A primeira camada oculta cont\u00e9m doze neur\u00f4nios e a segunda camada cont\u00e9m seis neur\u00f4nios. As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o (\tanh) em ambas as camadas. A sa\u00edda continua a usar softmax. O mesmo conjunto multiclasse do exerc\u00edcio\u00a03 \u00e9 reutilizado.</p>"},{"location":"portfolio/neural-networks/exercises/3/ex3_pedrotpc_copy/#analise-dos-resultados-do-exercicio-4","title":"An\u00e1lise dos resultados do Exerc\u00edcio\u00a04\u00b6","text":"<p>Ao adicionar uma segunda camada oculta, a rede neural passa a ter maior capacidade de modelar padr\u00f5es complexos. Usando doze neur\u00f4nios na primeira camada oculta e seis neur\u00f4nios na segunda, observa se que a perda de treino diminui de forma semelhante ao exerc\u00edcio\u00a03. A acur\u00e1cia no conjunto de teste tamb\u00e9m melhora levemente ou permanece est\u00e1vel dependendo da inicializa\u00e7\u00e3o, situando se em torno de 0{,}85.</p> <p>A presen\u00e7a de duas camadas ocultas permite que a rede aprenda representa\u00e7\u00f5es mais ricas dos dados, mas tamb\u00e9m aumenta o risco de sobreajuste e eleva o custo computacional. Ajustes adicionais nos hiperpar\u00e2metros e regulariza\u00e7\u00e3o podem ser explorados para obter ganhos adicionais.</p>"},{"location":"portfolio/neural-networks/exercises/3/main/","title":"Exercise 3","text":""},{"location":"portfolio/neural-networks/exercises/3/main/#exercise-3","title":"Exercise 3","text":"<p>Content coming soon.</p>"},{"location":"portfolio/neural-networks/exercises/4/","title":"4. VAE","text":""},{"location":"portfolio/neural-networks/exercises/4/#exercicio-4-variational-autoencoder-vae","title":"Exerc\u00edcio 4 - Variational Autoencoder (VAE)","text":"<p>Neste notebook implemento um VAE para gerar e reconstruir imagens de d\u00edgitos do MNIST. O objetivo \u00e9 entender a arquitetura, processo de treinamento e capacidade generativa do modelo.</p>"},{"location":"portfolio/neural-networks/exercises/4/#o-que-e-um-vae","title":"O que \u00e9 um VAE?","text":"<p>Um Variational Autoencoder \u00e9 um modelo generativo que aprende uma representa\u00e7\u00e3o latente probabil\u00edstica dos dados. Diferente de um autoencoder tradicional que mapeia cada entrada para um ponto fixo no espa\u00e7o latente, o VAE mapeia para uma distribui\u00e7\u00e3o de probabilidade (tipicamente Gaussiana).</p> <p>Componentes principais:</p> <ol> <li> <p>Encoder: mapeia a entrada para par\u00e2metros de distribui\u00e7\u00e3o (m\u00e9dia \u03bc e log-vari\u00e2ncia log \u03c3\u00b2)</p> </li> <li> <p>Reparameterization Trick: permite amostragem diferenci\u00e1vel:    $\\(z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\)$</p> </li> <li> <p>Decoder: reconstr\u00f3i a entrada a partir da amostra latente z</p> </li> <li> <p>Loss Function: combina dois termos:</p> </li> <li>Reconstruction Loss: mede similaridade entre entrada e reconstru\u00e7\u00e3o</li> <li>KL Divergence: regulariza o espa\u00e7o latente para N(0, I)</li> </ol> \\[\\mathcal{L} = \\text{Reconstru\u00e7\u00e3o} + \\text{KL Divergence}\\]"},{"location":"portfolio/neural-networks/exercises/4/#instalacao-de-dependencias","title":"Instala\u00e7\u00e3o de Depend\u00eancias","text":"<p>Implementa\u00e7\u00e3o do VAE usando NumPy puro, sem frameworks pesados que causam problemas de DLL no Windows.</p> <pre><code># Instala\u00e7\u00e3o (descomente se necess\u00e1rio):\n# !pip install numpy matplotlib scikit-learn\n\nprint(\"Usando apenas NumPy, Matplotlib e Scikit-learn\")\nprint(\"Sem PyTorch ou TensorFlow - evita problemas de DLL no Windows\")</code></pre> <pre><code>Usando apenas NumPy, Matplotlib e Scikit-learn\nSem PyTorch ou TensorFlow - evita problemas de DLL no Windows\n</code></pre>"},{"location":"portfolio/neural-networks/exercises/4/#por-que-numpy-puro","title":"Por que NumPy Puro?","text":"<p>PyTorch e TensorFlow apresentam erros de DLL no Windows. A solu\u00e7\u00e3o foi implementar o VAE completamente em NumPy.</p> <p>Vantagens: - Funciona em qualquer sistema operacional sem depend\u00eancias pesadas - C\u00f3digo completamente transparente e compreens\u00edvel - Mais did\u00e1tico para entender os detalhes de implementa\u00e7\u00e3o</p> <p>Desvantagens: - C\u00f3digo mais extenso - Sem acelera\u00e7\u00e3o por GPU (mas MNIST \u00e9 leve o suficiente para CPU)</p>"},{"location":"portfolio/neural-networks/exercises/4/#configuracao-do-projeto","title":"Configura\u00e7\u00e3o do Projeto","text":"<p>Dataset: MNIST (70.000 imagens de d\u00edgitos 28x28 pixels)</p> <p>Framework: NumPy puro</p> <p>Arquitetura: - Encoder: rede fully connected que mapeia imagens 784-D para m\u00e9dia e log-vari\u00e2ncia - Decoder: rede fully connected que mapeia vetores latentes para imagens 784-D - Dimens\u00e3o latente: 2D (para visualiza\u00e7\u00e3o), depois experimento com 10D e 20D</p> <p>Fun\u00e7\u00e3o de perda: MSE (reconstru\u00e7\u00e3o) + KL Divergence (regulariza\u00e7\u00e3o)</p> <p>Otimizador: Gradient Descent implementado manualmente</p> <p>Visualiza\u00e7\u00f5es: - Compara\u00e7\u00e3o entre originais e reconstru\u00e7\u00f5es - Espa\u00e7o latente 2D colorido por classe - Gera\u00e7\u00e3o de novas amostras - Interpola\u00e7\u00e3o no espa\u00e7o latente - Compara\u00e7\u00e3o entre diferentes dimens\u00f5es latentes</p> <pre><code># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Imports carregados com sucesso\")\nprint(\"Implementa\u00e7\u00e3o VAE em NumPy puro\")\n\n# Seed para reprodutibilidade\nnp.random.seed(42)</code></pre> <pre><code>Imports carregados com sucesso\nImplementa\u00e7\u00e3o VAE em NumPy puro\n</code></pre>"},{"location":"portfolio/neural-networks/exercises/4/#passo-1-preparacao-dos-dados","title":"Passo 1: Prepara\u00e7\u00e3o dos Dados","text":"<p>Carregamento do MNIST com normaliza\u00e7\u00e3o para [0, 1] e divis\u00e3o em conjuntos de treino/valida\u00e7\u00e3o/teste.</p> <pre><code># Carrega MNIST\nprint(\"Baixando MNIST...\")\nmnist = fetch_openml('mnist_784', version=1, parser='auto')\nX = mnist.data.astype('float32') / 255.0  # Normaliza para [0, 1]\ny = mnist.target.astype('int')\n\n# Converte para numpy array se necess\u00e1rio\nif not isinstance(X, np.ndarray):\n    X = X.to_numpy()\nif not isinstance(y, np.ndarray):\n    y = y.to_numpy()\n\n# Divis\u00e3o dos dados\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=10000, random_state=42, stratify=y\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n)\n\nprint(f\"Treino: {X_train.shape[0]:,} amostras\")\nprint(f\"Valida\u00e7\u00e3o: {X_val.shape[0]:,} amostras\")\nprint(f\"Teste: {X_test.shape[0]:,} amostras\")\nprint(f\"Dimens\u00e3o: {X_train.shape[1]} pixels\")</code></pre> <pre><code>Baixando MNIST...\nTreino: 48,000 amostras\nValida\u00e7\u00e3o: 12,000 amostras\nTeste: 10,000 amostras\nDimens\u00e3o: 784 pixels\n</code></pre> <pre><code># Visualiza\u00e7\u00e3o de algumas amostras\nfig, axes = plt.subplots(2, 8, figsize=(12, 3))\nfor i, ax in enumerate(axes.flat):\n    img = X_train[i].reshape(28, 28)\n    label = y_train[i]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(f'Label: {label}')\n    ax.axis('off')\nplt.suptitle('Amostras do Dataset MNIST')\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#passo-2-implementacao-do-vae","title":"Passo 2: Implementa\u00e7\u00e3o do VAE","text":"<p>A arquitetura consiste em tr\u00eas componentes:</p> <ol> <li>Encoder: mapeia imagem para par\u00e2metros da distribui\u00e7\u00e3o latente (\u03bc e log \u03c3\u00b2)</li> <li>Reparameterization Trick: amostra z de forma diferenci\u00e1vel</li> <li>Decoder: reconstr\u00f3i imagem a partir de z</li> </ol> <p>A fun\u00e7\u00e3o de perda combina reconstruction loss (MSE) e KL divergence.</p> <pre><code># Fun\u00e7\u00f5es de ativa\u00e7\u00e3o e suas derivadas\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x &gt; 0).astype(float)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef binary_crossentropy(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n\nclass VAE_NumPy:\n    \"\"\"Variational Autoencoder implementado em NumPy puro\"\"\"\n\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n\n        # Inicializa\u00e7\u00e3o Xavier\n        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros(hidden_dim)\n\n        self.W_mu = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b_mu = np.zeros(latent_dim)\n\n        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b_logvar = np.zeros(latent_dim)\n\n        self.W3 = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / latent_dim)\n        self.b3 = np.zeros(hidden_dim)\n\n        self.W4 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b4 = np.zeros(input_dim)\n\n    def encode(self, x):\n        self.h1 = relu(x @ self.W1 + self.b1)\n        self.mu = self.h1 @ self.W_mu + self.b_mu\n        self.log_var = self.h1 @ self.W_logvar + self.b_logvar\n        return self.mu, self.log_var\n\n    def reparameterize(self, mu, log_var):\n        std = np.exp(0.5 * log_var)\n        epsilon = np.random.randn(*mu.shape)\n        z = mu + std * epsilon\n        self.epsilon = epsilon\n        self.std = std\n        return z\n\n    def decode(self, z):\n        self.h3 = relu(z @ self.W3 + self.b3)\n        self.x_recon = sigmoid(self.h3 @ self.W4 + self.b4)\n        return self.x_recon\n\n    def forward(self, x):\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_recon = self.decode(z)\n        return x_recon, mu, log_var, z\n\n    def compute_loss(self, x, x_recon, mu, log_var):\n        batch_size = x.shape[0]\n        recon_loss = np.sum((x - x_recon) ** 2) / batch_size\n        kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var)) / batch_size\n        total_loss = recon_loss + kl_loss\n        return total_loss, recon_loss, kl_loss\n\n    def backward(self, x, x_recon, mu, log_var, z, lr=0.001):\n        \"\"\"Backpropagation manual\"\"\"\n        batch_size = x.shape[0]\n\n        d_recon = 2 * (x_recon - x) / batch_size\n\n        # Decoder gradients\n        d_h3_input = d_recon * x_recon * (1 - x_recon)\n        dW4 = self.h3.T @ d_h3_input\n        db4 = np.sum(d_h3_input, axis=0)\n\n        d_h3 = d_h3_input @ self.W4.T\n        d_h3 = d_h3 * relu_derivative(self.h3)\n        dW3 = z.T @ d_h3\n        db3 = np.sum(d_h3, axis=0)\n\n        d_z = d_h3 @ self.W3.T\n\n        # Reparameterization gradients\n        d_mu_recon = d_z\n        d_log_var_recon = d_z * self.std * 0.5 * self.epsilon\n\n        # KL gradients\n        d_mu_kl = mu / batch_size\n        d_log_var_kl = 0.5 * (np.exp(log_var) - 1) / batch_size\n\n        d_mu = d_mu_recon + d_mu_kl\n        d_log_var = d_log_var_recon + d_log_var_kl\n\n        # Encoder gradients\n        dW_mu = self.h1.T @ d_mu\n        db_mu = np.sum(d_mu, axis=0)\n\n        dW_logvar = self.h1.T @ d_log_var\n        db_logvar = np.sum(d_log_var, axis=0)\n\n        d_h1 = d_mu @ self.W_mu.T + d_log_var @ self.W_logvar.T\n        d_h1 = d_h1 * relu_derivative(self.h1)\n        dW1 = x.T @ d_h1\n        db1 = np.sum(d_h1, axis=0)\n\n        # Atualiza\u00e7\u00e3o de pesos\n        self.W4 -= lr * dW4\n        self.b4 -= lr * db4\n        self.W3 -= lr * dW3\n        self.b3 -= lr * db3\n        self.W_mu -= lr * dW_mu\n        self.b_mu -= lr * db_mu\n        self.W_logvar -= lr * dW_logvar\n        self.b_logvar -= lr * db_logvar\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n\n# Cria\u00e7\u00e3o do modelo\nprint(\"Criando VAE...\")\nlatent_dim = 2\nvae = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=latent_dim)\n\nn_params = (\n    vae.W1.size + vae.b1.size +\n    vae.W_mu.size + vae.b_mu.size +\n    vae.W_logvar.size + vae.b_logvar.size +\n    vae.W3.size + vae.b3.size +\n    vae.W4.size + vae.b4.size\n)\n\nprint(f\"Modelo criado\")\nprint(f\"Espa\u00e7o latente: {latent_dim}D\")\nprint(f\"Par\u00e2metros: {n_params:,}\")</code></pre> <pre><code>Criando VAE...\nModelo criado\nEspa\u00e7o latente: 2D\nPar\u00e2metros: 631,188\n</code></pre>"},{"location":"portfolio/neural-networks/exercises/4/#passo-3-funcoes-de-treinamento","title":"Passo 3: Fun\u00e7\u00f5es de Treinamento","text":"<p>Implementa\u00e7\u00e3o do loop de treinamento e valida\u00e7\u00e3o.</p> <pre><code>def create_batches(X, batch_size):\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for start_idx in range(0, X.shape[0], batch_size):\n        batch_indices = indices[start_idx:start_idx + batch_size]\n        yield X[batch_indices]\n\ndef train_epoch(vae, X_train, batch_size=128, lr=0.001):\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n    n_batches = 0\n\n    for batch in create_batches(X_train, batch_size):\n        x_recon, mu, log_var, z = vae.forward(batch)\n        loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)\n        vae.backward(batch, x_recon, mu, log_var, z, lr=lr)\n\n        total_loss += loss\n        total_recon += recon_loss\n        total_kl += kl_loss\n        n_batches += 1\n\n    return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches\n\ndef validate(vae, X_val, batch_size=128):\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n    n_batches = 0\n\n    for i in range(0, X_val.shape[0], batch_size):\n        batch = X_val[i:i+batch_size]\n        mu, log_var = vae.encode(batch)\n        z = mu\n        x_recon = vae.decode(z)\n        loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)\n\n        total_loss += loss\n        total_recon += recon_loss\n        total_kl += kl_loss\n        n_batches += 1\n\n    return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches\n\nprint(\"Fun\u00e7\u00f5es de treinamento definidas\")</code></pre> <pre><code>Fun\u00e7\u00f5es de treinamento definidas\n</code></pre> <pre><code># Configura\u00e7\u00e3o\nnum_epochs = 20\nbatch_size = 128\nlearning_rate = 0.001\n\n# Hist\u00f3rico\ntrain_losses = []\nval_losses = []\ntrain_recons = []\nval_recons = []\ntrain_kls = []\nval_kls = []\n\nprint(\"Iniciando treinamento...\")\nprint(f\"\u00c9pocas: {num_epochs}, Batch: {batch_size}, LR: {learning_rate}\\n\")\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_recon, train_kl = train_epoch(vae, X_train, batch_size, learning_rate)\n    val_loss, val_recon, val_kl = validate(vae, X_val, batch_size)\n\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_recons.append(train_recon)\n    val_recons.append(val_recon)\n    train_kls.append(train_kl)\n    val_kls.append(val_kl)\n\n    if epoch % 5 == 0 or epoch == 1:\n        print(f\"\u00c9poca {epoch:2d}/{num_epochs} | \"\n              f\"Train: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | \"\n              f\"Val: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\")\n\nprint(\"\\nTreinamento conclu\u00eddo\")</code></pre> <pre><code>Iniciando treinamento...\n\u00c9pocas: 20, Batch: 128, LR: 0.001\n\n\u00c9poca  1/20 | Train: 76.3850 (Recon: 72.1779, KL: 4.2071) | Val: 54.8919 (Recon: 50.7780, KL: 4.1139)\n\u00c9poca  5/20 | Train: 50.4901 (Recon: 46.8261, KL: 3.6641) | Val: 49.3246 (Recon: 45.7047, KL: 3.6199)\n\u00c9poca 10/20 | Train: 48.8446 (Recon: 45.3085, KL: 3.5361) | Val: 47.9248 (Recon: 44.3938, KL: 3.5310)\n\u00c9poca 15/20 | Train: 48.0203 (Recon: 44.4706, KL: 3.5497) | Val: 47.1099 (Recon: 43.6111, KL: 3.4988)\n\u00c9poca 20/20 | Train: 47.4496 (Recon: 43.8886, KL: 3.5611) | Val: 46.7090 (Recon: 43.0814, KL: 3.6276)\n\nTreinamento conclu\u00eddo\n</code></pre> <pre><code>print(f\"Loss final treino: {train_losses[-1]:.4f}\")\nprint(f\"Loss final valida\u00e7\u00e3o: {val_losses[-1]:.4f}\")</code></pre> <pre><code>Loss final treino: 47.4496\nLoss final valida\u00e7\u00e3o: 46.7090\n</code></pre> <pre><code># Visualiza\u00e7\u00e3o das curvas de perda\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Perda total\naxes[0].plot(range(1, num_epochs + 1), train_losses, label='Treino', marker='o')\naxes[0].plot(range(1, num_epochs + 1), val_losses, label='Valida\u00e7\u00e3o', marker='s')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Perda Total')\naxes[0].set_title('Perda Total por \u00c9poca')\naxes[0].legend()\naxes[0].grid(True)\n\n# Reconstruction loss\naxes[1].plot(range(1, num_epochs + 1), train_recons, label='Treino', marker='o')\naxes[1].plot(range(1, num_epochs + 1), val_recons, label='Valida\u00e7\u00e3o', marker='s')\naxes[1].set_xlabel('\u00c9poca')\naxes[1].set_ylabel('Reconstruction Loss')\naxes[1].set_title('Reconstruction Loss por \u00c9poca')\naxes[1].legend()\naxes[1].grid(True)\n\n# KL Divergence\naxes[2].plot(range(1, num_epochs + 1), train_kls, label='Treino', marker='o')\naxes[2].plot(range(1, num_epochs + 1), val_kls, label='Valida\u00e7\u00e3o', marker='s')\naxes[2].set_xlabel('\u00c9poca')\naxes[2].set_ylabel('KL Divergence')\naxes[2].set_title('KL Divergence por \u00c9poca')\naxes[2].legend()\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#analise-das-curvas","title":"An\u00e1lise das Curvas","text":"<p>Perda Total: diminui em treino e valida\u00e7\u00e3o, indicando aprendizado sem overfitting significativo.</p> <p>Reconstruction Loss: mede qualidade da reconstru\u00e7\u00e3o. Diminui\u00e7\u00e3o indica melhoria.</p> <p>KL Divergence: regulariza o espa\u00e7o latente. Valores est\u00e1veis indicam bom equil\u00edbrio entre reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/4/#passo-4-avaliacao-reconstrucoes","title":"Passo 4: Avalia\u00e7\u00e3o - Reconstru\u00e7\u00f5es","text":"<p>Compara\u00e7\u00e3o entre imagens originais e reconstru\u00eddas.</p> <pre><code># Pega algumas imagens de valida\u00e7\u00e3o\nn_samples = 10\nsample_data = X_val[:n_samples]\n\n# Reconstr\u00f3i\nmu, log_var = vae.encode(sample_data)\nreconstructions = vae.decode(mu)  # usa m\u00e9dia\n\n# Visualiza\nfig, axes = plt.subplots(2, n_samples, figsize=(15, 3))\n\nfor i in range(n_samples):\n    # Original\n    axes[0, i].imshow(sample_data[i].reshape(28, 28), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)\n\n    # Reconstru\u00e7\u00e3o\n    axes[1, i].imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel('Reconstru\u00e7\u00e3o', rotation=0, labelpad=40, fontsize=10)\n\nplt.suptitle('Compara\u00e7\u00e3o: Imagens Originais vs Reconstru\u00eddas', fontsize=12)\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#passo-5-visualizacao-do-espaco-latente","title":"Passo 5: Visualiza\u00e7\u00e3o do Espa\u00e7o Latente","text":"<p>Plotagem do espa\u00e7o latente 2D com colora\u00e7\u00e3o por classe para visualizar a organiza\u00e7\u00e3o aprendida.</p> <pre><code># Codifica o conjunto de valida\u00e7\u00e3o\nmu, log_var = vae.encode(X_val)\nlatent_vectors = mu\nlabels_array = y_val\n\n# Visualiza\u00e7\u00e3o 2D\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], \n                     c=labels_array, cmap='tab10', alpha=0.5, s=5)\nplt.colorbar(scatter, label='D\u00edgito')\nplt.xlabel('Dimens\u00e3o Latente 1')\nplt.ylabel('Dimens\u00e3o Latente 2')\nplt.title('Espa\u00e7o Latente 2D do VAE (colorido por classe)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\ud83d\udcca Forma do espa\u00e7o latente: {latent_vectors.shape}\")</code></pre> <p></p> <pre><code>\ud83d\udcca Forma do espa\u00e7o latente: (12000, 2)\n</code></pre>"},{"location":"portfolio/neural-networks/exercises/4/#analise-do-espaco-latente","title":"An\u00e1lise do Espa\u00e7o Latente","text":"<p>Agrupamento: d\u00edgitos similares agrupam-se pr\u00f3ximos, demonstrando representa\u00e7\u00e3o significativa.</p> <p>Continuidade: espa\u00e7o suave sem descontinuidades, resultado da regulariza\u00e7\u00e3o KL.</p> <p>Sobreposi\u00e7\u00e3o: classes visualmente similares (4-9, 3-5) apresentam sobreposi\u00e7\u00e3o esperada.</p> <p>Organiza\u00e7\u00e3o: separa\u00e7\u00e3o razo\u00e1vel de 10 classes em apenas 2 dimens\u00f5es.</p>"},{"location":"portfolio/neural-networks/exercises/4/#passo-6-geracao-de-amostras","title":"Passo 6: Gera\u00e7\u00e3o de Amostras","text":"<p>Amostragem aleat\u00f3ria do espa\u00e7o latente N(0,1) para gera\u00e7\u00e3o de novos d\u00edgitos.</p> <pre><code># Gera amostras aleat\u00f3rias\nn_samples = 20\nz_random = np.random.normal(size=(n_samples, latent_dim)).astype('float32')\nsamples = vae.decode(z_random)\n\n# Visualiza\nfig, axes = plt.subplots(2, 10, figsize=(15, 3))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(samples[i].reshape(28, 28), cmap='gray')\n    ax.axis('off')\n\nplt.suptitle('Amostras Geradas pelo VAE (amostragem aleat\u00f3ria do espa\u00e7o latente)', fontsize=12)\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#exploracao-sistematica-do-espaco-latente","title":"Explora\u00e7\u00e3o Sistem\u00e1tica do Espa\u00e7o Latente","text":"<p>Grade 15x15 de pontos no intervalo [-3, 3] mostrando transi\u00e7\u00f5es suaves entre d\u00edgitos.</p> <pre><code># Grade no espa\u00e7o latente\nn = 15\ndigit_size = 28\ngrid_x = np.linspace(-3, 3, n)\ngrid_y = np.linspace(-3, 3, n)[::-1]\nfigure = np.zeros((digit_size * n, digit_size * n))\n\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        z_sample = np.array([[xi, yi]], dtype='float32')\n        x_decoded = vae.decode(z_sample)\n        digit = x_decoded.reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\n# Visualiza\nplt.figure(figsize=(12, 12))\nplt.imshow(figure, cmap='gray')\nplt.title('Grade de D\u00edgitos Gerados no Espa\u00e7o Latente 2D\\n(de -3 a +3 em cada dimens\u00e3o)', fontsize=14)\nplt.xlabel('Dimens\u00e3o Latente 1 \u2192', fontsize=12)\nplt.ylabel('\u2190 Dimens\u00e3o Latente 2', fontsize=12)\nplt.xticks([])\nplt.yticks([])\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#interpolacao-linear","title":"Interpola\u00e7\u00e3o Linear","text":"<p>Interpola\u00e7\u00e3o entre dois pontos no espa\u00e7o latente demonstrando transi\u00e7\u00e3o suave entre d\u00edgitos.</p> <pre><code># Interpola\u00e7\u00e3o entre duas imagens\nidx1, idx2 = 0, 50\nimg1 = X_test[idx1:idx1+1]\nimg2 = X_test[idx2:idx2+1]\nlabel1 = y_test[idx1]\nlabel2 = y_test[idx2]\n\n# Codifica\nmu1, _ = vae.encode(img1)\nmu2, _ = vae.encode(img2)\n\n# Interpola\nn_steps = 12\ninterpolations = []\n\nfor alpha in np.linspace(0, 1, n_steps):\n    z_interp = (1 - alpha) * mu1 + alpha * mu2\n    img_interp = vae.decode(z_interp)\n    interpolations.append(img_interp.reshape(28, 28))\n\n# Visualiza\nfig, axes = plt.subplots(1, n_steps, figsize=(15, 2))\nfor i, ax in enumerate(axes):\n    ax.imshow(interpolations[i], cmap='gray')\n    ax.axis('off')\n    if i == 0:\n        ax.set_title(f'{label1}', fontsize=10, color='blue')\n    elif i == n_steps - 1:\n        ax.set_title(f'{label2}', fontsize=10, color='red')\n\nplt.suptitle(f'Interpola\u00e7\u00e3o Linear no Espa\u00e7o Latente: {label1} \u2192 {label2}', fontsize=12)\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#passo-7-comparacao-de-dimensoes-latentes","title":"Passo 7: Compara\u00e7\u00e3o de Dimens\u00f5es Latentes","text":"<p>Experimento com dimens\u00f5es 2D, 10D e 20D para avaliar trade-off entre visualiza\u00e7\u00e3o e qualidade.</p> <pre><code># Treinamento de modelos com diferentes dimens\u00f5es latentes\nprint(\"Treinando VAEs com diferentes dimens\u00f5es latentes...\")\nprint(\"Nota: implementa\u00e7\u00e3o NumPy sem GPU\\n\")\n\nlatent_dims = [2, 10, 20]\nmodels_dict = {2: vae}\nhistories_dict = {2: val_losses}\n\nfor ld in [10, 20]:\n    print(f\"Treinando VAE latent_dim={ld} (10 \u00e9pocas)\")\n    model_temp = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=ld)\n\n    temp_val_losses = []\n    for epoch in range(1, 11):\n        _ = train_epoch(model_temp, X_train, 128, 0.001)\n        val_loss, _, _ = validate(model_temp, X_val, 128)\n        temp_val_losses.append(val_loss)\n\n    models_dict[ld] = model_temp\n    histories_dict[ld] = temp_val_losses\n    print(f\"Loss final: {temp_val_losses[-1]:.4f}\\n\")\n\nprint(\"Treinamento conclu\u00eddo\")</code></pre> <pre><code>Treinando VAEs com diferentes dimens\u00f5es latentes...\nNota: implementa\u00e7\u00e3o NumPy sem GPU\n\nTreinando VAE latent_dim=10 (10 \u00e9pocas)\nLoss final: 37.9164\n\nTreinando VAE latent_dim=20 (10 \u00e9pocas)\nLoss final: 37.8154\n\nTreinamento conclu\u00eddo\n</code></pre> <pre><code># Compara perdas\nplt.figure(figsize=(10, 5))\nfor ld in latent_dims:\n    epochs_range = range(1, len(histories_dict[ld]) + 1)\n    plt.plot(epochs_range, histories_dict[ld], label=f'Latent Dim = {ld}', marker='o')\n\nplt.xlabel('\u00c9poca')\nplt.ylabel('Perda de Valida\u00e7\u00e3o')\nplt.title('Compara\u00e7\u00e3o de Perdas para Diferentes Dimens\u00f5es Latentes')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Compara reconstru\u00e7\u00f5es\nfig, axes = plt.subplots(len(latent_dims) + 1, 10, figsize=(15, 2 * (len(latent_dims) + 1)))\ntest_data = X_test[:10]\n\n# Linha 0: originais\nfor j in range(10):\n    axes[0, j].imshow(test_data[j].reshape(28, 28), cmap='gray')\n    axes[0, j].axis('off')\n    if j == 0:\n        axes[0, j].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)\n\n# Linhas seguintes: reconstru\u00e7\u00f5es\nfor i, ld in enumerate(latent_dims):\n    model_temp = models_dict[ld]\n    mu, _ = model_temp.encode(test_data)\n    recon = model_temp.decode(mu)\n\n    for j in range(10):\n        axes[i + 1, j].imshow(recon[j].reshape(28, 28), cmap='gray')\n        axes[i + 1, j].axis('off')\n        if j == 0:\n            axes[i + 1, j].set_ylabel(f'Latent {ld}D', rotation=0, labelpad=40, fontsize=10)\n\nplt.suptitle('Compara\u00e7\u00e3o de Reconstru\u00e7\u00f5es para Diferentes Dimens\u00f5es Latentes', fontsize=12)\nplt.tight_layout()\nplt.show()</code></pre> <p></p> <p></p>"},{"location":"portfolio/neural-networks/exercises/4/#analise-comparativa","title":"An\u00e1lise Comparativa","text":"<p>2D: - Visualiza\u00e7\u00e3o direta poss\u00edvel - Reconstru\u00e7\u00f5es com mais perda de detalhes - Loss de valida\u00e7\u00e3o: maior</p> <p>10D: - Equil\u00edbrio entre capacidade e interpretabilidade - Reconstru\u00e7\u00f5es significativamente melhores - Loss intermedi\u00e1ria</p> <p>20D: - Melhor qualidade de reconstru\u00e7\u00e3o - Loss de valida\u00e7\u00e3o: menor - Visualiza\u00e7\u00e3o requer redu\u00e7\u00e3o dimensional (t-SNE/UMAP)</p> <p>Conclus\u00e3o: dimens\u00e3o latente deve ser escolhida conforme objetivo (visualiza\u00e7\u00e3o vs qualidade).</p>"},{"location":"portfolio/neural-networks/exercises/4/#relatorio-final","title":"Relat\u00f3rio Final","text":""},{"location":"portfolio/neural-networks/exercises/4/#resumo","title":"Resumo","text":"<p>Implementa\u00e7\u00e3o completa de VAE em NumPy puro aplicado ao MNIST. Componentes implementados:</p> <ol> <li>Arquitetura: encoder probabil\u00edstico, reparameterization trick, decoder</li> <li>Loss: MSE (reconstru\u00e7\u00e3o) + KL divergence (regulariza\u00e7\u00e3o)</li> <li>Backpropagation manual para todos os pesos</li> <li>Treinamento: 20 \u00e9pocas com monitoring</li> <li>Avalia\u00e7\u00f5es: reconstru\u00e7\u00f5es, espa\u00e7o latente, gera\u00e7\u00e3o, interpola\u00e7\u00e3o</li> <li>Experimentos: compara\u00e7\u00e3o de dimens\u00f5es 2D, 10D e 20D</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/#resultados","title":"Resultados","text":"<p>Reconstru\u00e7\u00e3o: - VAE reconstr\u00f3i d\u00edgitos com qualidade razo\u00e1vel - Imagens ligeiramente borradas (caracter\u00edstica de VAEs) - Dimens\u00f5es maiores produzem melhores reconstru\u00e7\u00f5es (loss ~47 em 2D vs ~38 em 20D)</p> <p>Espa\u00e7o Latente: - Organiza\u00e7\u00e3o clara em 2D - Espa\u00e7o cont\u00ednuo sem descontinuidades - D\u00edgitos similares agrupam-se proximamente</p> <p>Gera\u00e7\u00e3o: - Amostragem aleat\u00f3ria produz d\u00edgitos reconhec\u00edveis - Grade mostra transi\u00e7\u00f5es suaves - Interpola\u00e7\u00e3o gera morfismos realistas</p> <p>Dimensionalidade: - Trade-off: visualiza\u00e7\u00e3o (2D) vs qualidade (20D)</p>"},{"location":"portfolio/neural-networks/exercises/4/#desafios","title":"Desafios","text":"<ol> <li>Backpropagation Manual: implementa\u00e7\u00e3o de gradientes complexa mas educativa</li> <li>Balanceamento de Loss: equil\u00edbrio entre reconstru\u00e7\u00e3o e KL</li> <li>Problemas de DLL: solu\u00e7\u00e3o atrav\u00e9s de implementa\u00e7\u00e3o NumPy pura</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/#insights","title":"Insights","text":"<p>VAE vs Autoencoder: - VAE aprende distribui\u00e7\u00e3o, n\u00e3o pontos fixos - Permite gera\u00e7\u00e3o, n\u00e3o apenas reconstru\u00e7\u00e3o - KL divergence garante continuidade</p> <p>Reparameterization Trick: - Essencial para backpropagation atrav\u00e9s de sampling - Separa ru\u00eddo de par\u00e2metros aprend\u00edveis</p> <p>Aplica\u00e7\u00f5es: - Compress\u00e3o de dados - Detec\u00e7\u00e3o de anomalias - Data augmentation - Aprendizado de representa\u00e7\u00f5es</p>"},{"location":"portfolio/neural-networks/exercises/4/#melhorias-possiveis","title":"Melhorias Poss\u00edveis","text":"<ol> <li>Arquitetura convolucional para melhor captura espacial</li> <li>\u03b2-VAE para controle de disentanglement</li> <li>Conditional VAE para gera\u00e7\u00e3o controlada</li> <li>KL annealing durante treinamento</li> <li>Priors mais complexos</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/#conclusao","title":"Conclus\u00e3o","text":"<p>VAE combina deep learning com infer\u00eancia bayesiana, oferecendo: - Espa\u00e7o latente interpret\u00e1vel e estruturado - Capacidade generativa com controle - Treinamento est\u00e1vel</p> <p>A implementa\u00e7\u00e3o em NumPy puro, embora mais trabalhosa, proporciona compreens\u00e3o profunda dos mecanismos internos do modelo.</p>"},{"location":"portfolio/neural-networks/exercises/4/#referencias","title":"Refer\u00eancias","text":"<ul> <li>Kingma, D. P., &amp; Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114.</li> <li>Doersch, C. (2016). Tutorial on Variational Autoencoders. arXiv preprint arXiv:1606.05908.</li> <li>Material do Curso: Insper ANN-DL - VAE Exercise</li> <li>NumPy Documentation: https://numpy.org/doc/</li> <li>Dataset MNIST: LeCun, Y., Cortes, C., &amp; Burges, C. J. (1998).</li> </ul> <p>Este notebook foi desenvolvido com assist\u00eancia de IA (Claude Code) para estrutura\u00e7\u00e3o de c\u00f3digo e organiza\u00e7\u00e3o.</p> <p>Autor: Pedro Civita Data: Outubro 2025 Curso: Redes Neurais e Deep Learning - Insper</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/","title":"Ex04 pedrotpc","text":"In\u00a0[1]: Copied! <pre># Instala\u00e7\u00e3o (descomente se necess\u00e1rio):\n# !pip install numpy matplotlib scikit-learn\n\nprint(\"Usando apenas NumPy, Matplotlib e Scikit-learn\")\nprint(\"Sem PyTorch ou TensorFlow - evita problemas de DLL no Windows\")\n</pre> # Instala\u00e7\u00e3o (descomente se necess\u00e1rio): # !pip install numpy matplotlib scikit-learn  print(\"Usando apenas NumPy, Matplotlib e Scikit-learn\") print(\"Sem PyTorch ou TensorFlow - evita problemas de DLL no Windows\") <pre>Usando apenas NumPy, Matplotlib e Scikit-learn\nSem PyTorch ou TensorFlow - evita problemas de DLL no Windows\n</pre> In\u00a0[2]: Copied! <pre># Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Imports carregados com sucesso\")\nprint(\"Implementa\u00e7\u00e3o VAE em NumPy puro\")\n\n# Seed para reprodutibilidade\nnp.random.seed(42)\n</pre> # Imports import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.manifold import TSNE import warnings warnings.filterwarnings('ignore')  print(\"Imports carregados com sucesso\") print(\"Implementa\u00e7\u00e3o VAE em NumPy puro\")  # Seed para reprodutibilidade np.random.seed(42) <pre>Imports carregados com sucesso\nImplementa\u00e7\u00e3o VAE em NumPy puro\n</pre> In\u00a0[3]: Copied! <pre># Carrega MNIST\nprint(\"Baixando MNIST...\")\nmnist = fetch_openml('mnist_784', version=1, parser='auto')\nX = mnist.data.astype('float32') / 255.0  # Normaliza para [0, 1]\ny = mnist.target.astype('int')\n\n# Converte para numpy array se necess\u00e1rio\nif not isinstance(X, np.ndarray):\n    X = X.to_numpy()\nif not isinstance(y, np.ndarray):\n    y = y.to_numpy()\n\n# Divis\u00e3o dos dados\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=10000, random_state=42, stratify=y\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n)\n\nprint(f\"Treino: {X_train.shape[0]:,} amostras\")\nprint(f\"Valida\u00e7\u00e3o: {X_val.shape[0]:,} amostras\")\nprint(f\"Teste: {X_test.shape[0]:,} amostras\")\nprint(f\"Dimens\u00e3o: {X_train.shape[1]} pixels\")\n</pre> # Carrega MNIST print(\"Baixando MNIST...\") mnist = fetch_openml('mnist_784', version=1, parser='auto') X = mnist.data.astype('float32') / 255.0  # Normaliza para [0, 1] y = mnist.target.astype('int')  # Converte para numpy array se necess\u00e1rio if not isinstance(X, np.ndarray):     X = X.to_numpy() if not isinstance(y, np.ndarray):     y = y.to_numpy()  # Divis\u00e3o dos dados X_train_full, X_test, y_train_full, y_test = train_test_split(     X, y, test_size=10000, random_state=42, stratify=y )  X_train, X_val, y_train, y_val = train_test_split(     X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full )  print(f\"Treino: {X_train.shape[0]:,} amostras\") print(f\"Valida\u00e7\u00e3o: {X_val.shape[0]:,} amostras\") print(f\"Teste: {X_test.shape[0]:,} amostras\") print(f\"Dimens\u00e3o: {X_train.shape[1]} pixels\") <pre>Baixando MNIST...\nTreino: 48,000 amostras\nValida\u00e7\u00e3o: 12,000 amostras\nTeste: 10,000 amostras\nDimens\u00e3o: 784 pixels\n</pre> In\u00a0[4]: Copied! <pre># Visualiza\u00e7\u00e3o de algumas amostras\nfig, axes = plt.subplots(2, 8, figsize=(12, 3))\nfor i, ax in enumerate(axes.flat):\n    img = X_train[i].reshape(28, 28)\n    label = y_train[i]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(f'Label: {label}')\n    ax.axis('off')\nplt.suptitle('Amostras do Dataset MNIST')\nplt.tight_layout()\nplt.show()\n</pre> # Visualiza\u00e7\u00e3o de algumas amostras fig, axes = plt.subplots(2, 8, figsize=(12, 3)) for i, ax in enumerate(axes.flat):     img = X_train[i].reshape(28, 28)     label = y_train[i]     ax.imshow(img, cmap='gray')     ax.set_title(f'Label: {label}')     ax.axis('off') plt.suptitle('Amostras do Dataset MNIST') plt.tight_layout() plt.show() In\u00a0[5]: Copied! <pre># Fun\u00e7\u00f5es de ativa\u00e7\u00e3o e suas derivadas\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_derivative(x):\n    return (x &gt; 0).astype(float)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef binary_crossentropy(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n\nclass VAE_NumPy:\n    \"\"\"Variational Autoencoder implementado em NumPy puro\"\"\"\n    \n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        \n        # Inicializa\u00e7\u00e3o Xavier\n        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros(hidden_dim)\n        \n        self.W_mu = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b_mu = np.zeros(latent_dim)\n        \n        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b_logvar = np.zeros(latent_dim)\n        \n        self.W3 = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / latent_dim)\n        self.b3 = np.zeros(hidden_dim)\n        \n        self.W4 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b4 = np.zeros(input_dim)\n        \n    def encode(self, x):\n        self.h1 = relu(x @ self.W1 + self.b1)\n        self.mu = self.h1 @ self.W_mu + self.b_mu\n        self.log_var = self.h1 @ self.W_logvar + self.b_logvar\n        return self.mu, self.log_var\n    \n    def reparameterize(self, mu, log_var):\n        std = np.exp(0.5 * log_var)\n        epsilon = np.random.randn(*mu.shape)\n        z = mu + std * epsilon\n        self.epsilon = epsilon\n        self.std = std\n        return z\n    \n    def decode(self, z):\n        self.h3 = relu(z @ self.W3 + self.b3)\n        self.x_recon = sigmoid(self.h3 @ self.W4 + self.b4)\n        return self.x_recon\n    \n    def forward(self, x):\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_recon = self.decode(z)\n        return x_recon, mu, log_var, z\n    \n    def compute_loss(self, x, x_recon, mu, log_var):\n        batch_size = x.shape[0]\n        recon_loss = np.sum((x - x_recon) ** 2) / batch_size\n        kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var)) / batch_size\n        total_loss = recon_loss + kl_loss\n        return total_loss, recon_loss, kl_loss\n    \n    def backward(self, x, x_recon, mu, log_var, z, lr=0.001):\n        \"\"\"Backpropagation manual\"\"\"\n        batch_size = x.shape[0]\n        \n        d_recon = 2 * (x_recon - x) / batch_size\n        \n        # Decoder gradients\n        d_h3_input = d_recon * x_recon * (1 - x_recon)\n        dW4 = self.h3.T @ d_h3_input\n        db4 = np.sum(d_h3_input, axis=0)\n        \n        d_h3 = d_h3_input @ self.W4.T\n        d_h3 = d_h3 * relu_derivative(self.h3)\n        dW3 = z.T @ d_h3\n        db3 = np.sum(d_h3, axis=0)\n        \n        d_z = d_h3 @ self.W3.T\n        \n        # Reparameterization gradients\n        d_mu_recon = d_z\n        d_log_var_recon = d_z * self.std * 0.5 * self.epsilon\n        \n        # KL gradients\n        d_mu_kl = mu / batch_size\n        d_log_var_kl = 0.5 * (np.exp(log_var) - 1) / batch_size\n        \n        d_mu = d_mu_recon + d_mu_kl\n        d_log_var = d_log_var_recon + d_log_var_kl\n        \n        # Encoder gradients\n        dW_mu = self.h1.T @ d_mu\n        db_mu = np.sum(d_mu, axis=0)\n        \n        dW_logvar = self.h1.T @ d_log_var\n        db_logvar = np.sum(d_log_var, axis=0)\n        \n        d_h1 = d_mu @ self.W_mu.T + d_log_var @ self.W_logvar.T\n        d_h1 = d_h1 * relu_derivative(self.h1)\n        dW1 = x.T @ d_h1\n        db1 = np.sum(d_h1, axis=0)\n        \n        # Atualiza\u00e7\u00e3o de pesos\n        self.W4 -= lr * dW4\n        self.b4 -= lr * db4\n        self.W3 -= lr * dW3\n        self.b3 -= lr * db3\n        self.W_mu -= lr * dW_mu\n        self.b_mu -= lr * db_mu\n        self.W_logvar -= lr * dW_logvar\n        self.b_logvar -= lr * db_logvar\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n\n# Cria\u00e7\u00e3o do modelo\nprint(\"Criando VAE...\")\nlatent_dim = 2\nvae = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=latent_dim)\n\nn_params = (\n    vae.W1.size + vae.b1.size +\n    vae.W_mu.size + vae.b_mu.size +\n    vae.W_logvar.size + vae.b_logvar.size +\n    vae.W3.size + vae.b3.size +\n    vae.W4.size + vae.b4.size\n)\n\nprint(f\"Modelo criado\")\nprint(f\"Espa\u00e7o latente: {latent_dim}D\")\nprint(f\"Par\u00e2metros: {n_params:,}\")\n</pre> # Fun\u00e7\u00f5es de ativa\u00e7\u00e3o e suas derivadas def relu(x):     return np.maximum(0, x)  def relu_derivative(x):     return (x &gt; 0).astype(float)  def sigmoid(x):     return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  def binary_crossentropy(y_true, y_pred):     y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)     return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))   class VAE_NumPy:     \"\"\"Variational Autoencoder implementado em NumPy puro\"\"\"          def __init__(self, input_dim=784, hidden_dim=400, latent_dim=2):         self.input_dim = input_dim         self.hidden_dim = hidden_dim         self.latent_dim = latent_dim                  # Inicializa\u00e7\u00e3o Xavier         self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)         self.b1 = np.zeros(hidden_dim)                  self.W_mu = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)         self.b_mu = np.zeros(latent_dim)                  self.W_logvar = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2.0 / hidden_dim)         self.b_logvar = np.zeros(latent_dim)                  self.W3 = np.random.randn(latent_dim, hidden_dim) * np.sqrt(2.0 / latent_dim)         self.b3 = np.zeros(hidden_dim)                  self.W4 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / hidden_dim)         self.b4 = np.zeros(input_dim)              def encode(self, x):         self.h1 = relu(x @ self.W1 + self.b1)         self.mu = self.h1 @ self.W_mu + self.b_mu         self.log_var = self.h1 @ self.W_logvar + self.b_logvar         return self.mu, self.log_var          def reparameterize(self, mu, log_var):         std = np.exp(0.5 * log_var)         epsilon = np.random.randn(*mu.shape)         z = mu + std * epsilon         self.epsilon = epsilon         self.std = std         return z          def decode(self, z):         self.h3 = relu(z @ self.W3 + self.b3)         self.x_recon = sigmoid(self.h3 @ self.W4 + self.b4)         return self.x_recon          def forward(self, x):         mu, log_var = self.encode(x)         z = self.reparameterize(mu, log_var)         x_recon = self.decode(z)         return x_recon, mu, log_var, z          def compute_loss(self, x, x_recon, mu, log_var):         batch_size = x.shape[0]         recon_loss = np.sum((x - x_recon) ** 2) / batch_size         kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var)) / batch_size         total_loss = recon_loss + kl_loss         return total_loss, recon_loss, kl_loss          def backward(self, x, x_recon, mu, log_var, z, lr=0.001):         \"\"\"Backpropagation manual\"\"\"         batch_size = x.shape[0]                  d_recon = 2 * (x_recon - x) / batch_size                  # Decoder gradients         d_h3_input = d_recon * x_recon * (1 - x_recon)         dW4 = self.h3.T @ d_h3_input         db4 = np.sum(d_h3_input, axis=0)                  d_h3 = d_h3_input @ self.W4.T         d_h3 = d_h3 * relu_derivative(self.h3)         dW3 = z.T @ d_h3         db3 = np.sum(d_h3, axis=0)                  d_z = d_h3 @ self.W3.T                  # Reparameterization gradients         d_mu_recon = d_z         d_log_var_recon = d_z * self.std * 0.5 * self.epsilon                  # KL gradients         d_mu_kl = mu / batch_size         d_log_var_kl = 0.5 * (np.exp(log_var) - 1) / batch_size                  d_mu = d_mu_recon + d_mu_kl         d_log_var = d_log_var_recon + d_log_var_kl                  # Encoder gradients         dW_mu = self.h1.T @ d_mu         db_mu = np.sum(d_mu, axis=0)                  dW_logvar = self.h1.T @ d_log_var         db_logvar = np.sum(d_log_var, axis=0)                  d_h1 = d_mu @ self.W_mu.T + d_log_var @ self.W_logvar.T         d_h1 = d_h1 * relu_derivative(self.h1)         dW1 = x.T @ d_h1         db1 = np.sum(d_h1, axis=0)                  # Atualiza\u00e7\u00e3o de pesos         self.W4 -= lr * dW4         self.b4 -= lr * db4         self.W3 -= lr * dW3         self.b3 -= lr * db3         self.W_mu -= lr * dW_mu         self.b_mu -= lr * db_mu         self.W_logvar -= lr * dW_logvar         self.b_logvar -= lr * db_logvar         self.W1 -= lr * dW1         self.b1 -= lr * db1   # Cria\u00e7\u00e3o do modelo print(\"Criando VAE...\") latent_dim = 2 vae = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=latent_dim)  n_params = (     vae.W1.size + vae.b1.size +     vae.W_mu.size + vae.b_mu.size +     vae.W_logvar.size + vae.b_logvar.size +     vae.W3.size + vae.b3.size +     vae.W4.size + vae.b4.size )  print(f\"Modelo criado\") print(f\"Espa\u00e7o latente: {latent_dim}D\") print(f\"Par\u00e2metros: {n_params:,}\") <pre>Criando VAE...\nModelo criado\nEspa\u00e7o latente: 2D\nPar\u00e2metros: 631,188\n</pre> In\u00a0[6]: Copied! <pre>def create_batches(X, batch_size):\n    indices = np.arange(X.shape[0])\n    np.random.shuffle(indices)\n    for start_idx in range(0, X.shape[0], batch_size):\n        batch_indices = indices[start_idx:start_idx + batch_size]\n        yield X[batch_indices]\n\ndef train_epoch(vae, X_train, batch_size=128, lr=0.001):\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n    n_batches = 0\n    \n    for batch in create_batches(X_train, batch_size):\n        x_recon, mu, log_var, z = vae.forward(batch)\n        loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)\n        vae.backward(batch, x_recon, mu, log_var, z, lr=lr)\n        \n        total_loss += loss\n        total_recon += recon_loss\n        total_kl += kl_loss\n        n_batches += 1\n    \n    return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches\n\ndef validate(vae, X_val, batch_size=128):\n    total_loss = 0\n    total_recon = 0\n    total_kl = 0\n    n_batches = 0\n    \n    for i in range(0, X_val.shape[0], batch_size):\n        batch = X_val[i:i+batch_size]\n        mu, log_var = vae.encode(batch)\n        z = mu\n        x_recon = vae.decode(z)\n        loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)\n        \n        total_loss += loss\n        total_recon += recon_loss\n        total_kl += kl_loss\n        n_batches += 1\n    \n    return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches\n\nprint(\"Fun\u00e7\u00f5es de treinamento definidas\")\n</pre> def create_batches(X, batch_size):     indices = np.arange(X.shape[0])     np.random.shuffle(indices)     for start_idx in range(0, X.shape[0], batch_size):         batch_indices = indices[start_idx:start_idx + batch_size]         yield X[batch_indices]  def train_epoch(vae, X_train, batch_size=128, lr=0.001):     total_loss = 0     total_recon = 0     total_kl = 0     n_batches = 0          for batch in create_batches(X_train, batch_size):         x_recon, mu, log_var, z = vae.forward(batch)         loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)         vae.backward(batch, x_recon, mu, log_var, z, lr=lr)                  total_loss += loss         total_recon += recon_loss         total_kl += kl_loss         n_batches += 1          return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches  def validate(vae, X_val, batch_size=128):     total_loss = 0     total_recon = 0     total_kl = 0     n_batches = 0          for i in range(0, X_val.shape[0], batch_size):         batch = X_val[i:i+batch_size]         mu, log_var = vae.encode(batch)         z = mu         x_recon = vae.decode(z)         loss, recon_loss, kl_loss = vae.compute_loss(batch, x_recon, mu, log_var)                  total_loss += loss         total_recon += recon_loss         total_kl += kl_loss         n_batches += 1          return total_loss / n_batches, total_recon / n_batches, total_kl / n_batches  print(\"Fun\u00e7\u00f5es de treinamento definidas\") <pre>Fun\u00e7\u00f5es de treinamento definidas\n</pre> In\u00a0[7]: Copied! <pre># Configura\u00e7\u00e3o\nnum_epochs = 20\nbatch_size = 128\nlearning_rate = 0.001\n\n# Hist\u00f3rico\ntrain_losses = []\nval_losses = []\ntrain_recons = []\nval_recons = []\ntrain_kls = []\nval_kls = []\n\nprint(\"Iniciando treinamento...\")\nprint(f\"\u00c9pocas: {num_epochs}, Batch: {batch_size}, LR: {learning_rate}\\n\")\n\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_recon, train_kl = train_epoch(vae, X_train, batch_size, learning_rate)\n    val_loss, val_recon, val_kl = validate(vae, X_val, batch_size)\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_recons.append(train_recon)\n    val_recons.append(val_recon)\n    train_kls.append(train_kl)\n    val_kls.append(val_kl)\n    \n    if epoch % 5 == 0 or epoch == 1:\n        print(f\"\u00c9poca {epoch:2d}/{num_epochs} | \"\n              f\"Train: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | \"\n              f\"Val: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\")\n\nprint(\"\\nTreinamento conclu\u00eddo\")\n</pre> # Configura\u00e7\u00e3o num_epochs = 20 batch_size = 128 learning_rate = 0.001  # Hist\u00f3rico train_losses = [] val_losses = [] train_recons = [] val_recons = [] train_kls = [] val_kls = []  print(\"Iniciando treinamento...\") print(f\"\u00c9pocas: {num_epochs}, Batch: {batch_size}, LR: {learning_rate}\\n\")  for epoch in range(1, num_epochs + 1):     train_loss, train_recon, train_kl = train_epoch(vae, X_train, batch_size, learning_rate)     val_loss, val_recon, val_kl = validate(vae, X_val, batch_size)          train_losses.append(train_loss)     val_losses.append(val_loss)     train_recons.append(train_recon)     val_recons.append(val_recon)     train_kls.append(train_kl)     val_kls.append(val_kl)          if epoch % 5 == 0 or epoch == 1:         print(f\"\u00c9poca {epoch:2d}/{num_epochs} | \"               f\"Train: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | \"               f\"Val: {val_loss:.4f} (Recon: {val_recon:.4f}, KL: {val_kl:.4f})\")  print(\"\\nTreinamento conclu\u00eddo\") <pre>Iniciando treinamento...\n\u00c9pocas: 20, Batch: 128, LR: 0.001\n\n\u00c9poca  1/20 | Train: 76.3850 (Recon: 72.1779, KL: 4.2071) | Val: 54.8919 (Recon: 50.7780, KL: 4.1139)\n\u00c9poca  5/20 | Train: 50.4901 (Recon: 46.8261, KL: 3.6641) | Val: 49.3246 (Recon: 45.7047, KL: 3.6199)\n\u00c9poca 10/20 | Train: 48.8446 (Recon: 45.3085, KL: 3.5361) | Val: 47.9248 (Recon: 44.3938, KL: 3.5310)\n\u00c9poca 15/20 | Train: 48.0203 (Recon: 44.4706, KL: 3.5497) | Val: 47.1099 (Recon: 43.6111, KL: 3.4988)\n\u00c9poca 20/20 | Train: 47.4496 (Recon: 43.8886, KL: 3.5611) | Val: 46.7090 (Recon: 43.0814, KL: 3.6276)\n\nTreinamento conclu\u00eddo\n</pre> In\u00a0[8]: Copied! <pre>print(f\"Loss final treino: {train_losses[-1]:.4f}\")\nprint(f\"Loss final valida\u00e7\u00e3o: {val_losses[-1]:.4f}\")\n</pre> print(f\"Loss final treino: {train_losses[-1]:.4f}\") print(f\"Loss final valida\u00e7\u00e3o: {val_losses[-1]:.4f}\") <pre>Loss final treino: 47.4496\nLoss final valida\u00e7\u00e3o: 46.7090\n</pre> In\u00a0[9]: Copied! <pre># Visualiza\u00e7\u00e3o das curvas de perda\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Perda total\naxes[0].plot(range(1, num_epochs + 1), train_losses, label='Treino', marker='o')\naxes[0].plot(range(1, num_epochs + 1), val_losses, label='Valida\u00e7\u00e3o', marker='s')\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Perda Total')\naxes[0].set_title('Perda Total por \u00c9poca')\naxes[0].legend()\naxes[0].grid(True)\n\n# Reconstruction loss\naxes[1].plot(range(1, num_epochs + 1), train_recons, label='Treino', marker='o')\naxes[1].plot(range(1, num_epochs + 1), val_recons, label='Valida\u00e7\u00e3o', marker='s')\naxes[1].set_xlabel('\u00c9poca')\naxes[1].set_ylabel('Reconstruction Loss')\naxes[1].set_title('Reconstruction Loss por \u00c9poca')\naxes[1].legend()\naxes[1].grid(True)\n\n# KL Divergence\naxes[2].plot(range(1, num_epochs + 1), train_kls, label='Treino', marker='o')\naxes[2].plot(range(1, num_epochs + 1), val_kls, label='Valida\u00e7\u00e3o', marker='s')\naxes[2].set_xlabel('\u00c9poca')\naxes[2].set_ylabel('KL Divergence')\naxes[2].set_title('KL Divergence por \u00c9poca')\naxes[2].legend()\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualiza\u00e7\u00e3o das curvas de perda fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Perda total axes[0].plot(range(1, num_epochs + 1), train_losses, label='Treino', marker='o') axes[0].plot(range(1, num_epochs + 1), val_losses, label='Valida\u00e7\u00e3o', marker='s') axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Perda Total') axes[0].set_title('Perda Total por \u00c9poca') axes[0].legend() axes[0].grid(True)  # Reconstruction loss axes[1].plot(range(1, num_epochs + 1), train_recons, label='Treino', marker='o') axes[1].plot(range(1, num_epochs + 1), val_recons, label='Valida\u00e7\u00e3o', marker='s') axes[1].set_xlabel('\u00c9poca') axes[1].set_ylabel('Reconstruction Loss') axes[1].set_title('Reconstruction Loss por \u00c9poca') axes[1].legend() axes[1].grid(True)  # KL Divergence axes[2].plot(range(1, num_epochs + 1), train_kls, label='Treino', marker='o') axes[2].plot(range(1, num_epochs + 1), val_kls, label='Valida\u00e7\u00e3o', marker='s') axes[2].set_xlabel('\u00c9poca') axes[2].set_ylabel('KL Divergence') axes[2].set_title('KL Divergence por \u00c9poca') axes[2].legend() axes[2].grid(True)  plt.tight_layout() plt.show() In\u00a0[10]: Copied! <pre># Pega algumas imagens de valida\u00e7\u00e3o\nn_samples = 10\nsample_data = X_val[:n_samples]\n\n# Reconstr\u00f3i\nmu, log_var = vae.encode(sample_data)\nreconstructions = vae.decode(mu)  # usa m\u00e9dia\n\n# Visualiza\nfig, axes = plt.subplots(2, n_samples, figsize=(15, 3))\n\nfor i in range(n_samples):\n    # Original\n    axes[0, i].imshow(sample_data[i].reshape(28, 28), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)\n    \n    # Reconstru\u00e7\u00e3o\n    axes[1, i].imshow(reconstructions[i].reshape(28, 28), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel('Reconstru\u00e7\u00e3o', rotation=0, labelpad=40, fontsize=10)\n\nplt.suptitle('Compara\u00e7\u00e3o: Imagens Originais vs Reconstru\u00eddas', fontsize=12)\nplt.tight_layout()\nplt.show()\n</pre> # Pega algumas imagens de valida\u00e7\u00e3o n_samples = 10 sample_data = X_val[:n_samples]  # Reconstr\u00f3i mu, log_var = vae.encode(sample_data) reconstructions = vae.decode(mu)  # usa m\u00e9dia  # Visualiza fig, axes = plt.subplots(2, n_samples, figsize=(15, 3))  for i in range(n_samples):     # Original     axes[0, i].imshow(sample_data[i].reshape(28, 28), cmap='gray')     axes[0, i].axis('off')     if i == 0:         axes[0, i].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)          # Reconstru\u00e7\u00e3o     axes[1, i].imshow(reconstructions[i].reshape(28, 28), cmap='gray')     axes[1, i].axis('off')     if i == 0:         axes[1, i].set_ylabel('Reconstru\u00e7\u00e3o', rotation=0, labelpad=40, fontsize=10)  plt.suptitle('Compara\u00e7\u00e3o: Imagens Originais vs Reconstru\u00eddas', fontsize=12) plt.tight_layout() plt.show() In\u00a0[11]: Copied! <pre># Codifica o conjunto de valida\u00e7\u00e3o\nmu, log_var = vae.encode(X_val)\nlatent_vectors = mu\nlabels_array = y_val\n\n# Visualiza\u00e7\u00e3o 2D\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], \n                     c=labels_array, cmap='tab10', alpha=0.5, s=5)\nplt.colorbar(scatter, label='D\u00edgito')\nplt.xlabel('Dimens\u00e3o Latente 1')\nplt.ylabel('Dimens\u00e3o Latente 2')\nplt.title('Espa\u00e7o Latente 2D do VAE (colorido por classe)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\ud83d\udcca Forma do espa\u00e7o latente: {latent_vectors.shape}\")\n</pre> # Codifica o conjunto de valida\u00e7\u00e3o mu, log_var = vae.encode(X_val) latent_vectors = mu labels_array = y_val  # Visualiza\u00e7\u00e3o 2D plt.figure(figsize=(10, 8)) scatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1],                       c=labels_array, cmap='tab10', alpha=0.5, s=5) plt.colorbar(scatter, label='D\u00edgito') plt.xlabel('Dimens\u00e3o Latente 1') plt.ylabel('Dimens\u00e3o Latente 2') plt.title('Espa\u00e7o Latente 2D do VAE (colorido por classe)') plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(f\"\ud83d\udcca Forma do espa\u00e7o latente: {latent_vectors.shape}\") <pre>\ud83d\udcca Forma do espa\u00e7o latente: (12000, 2)\n</pre> In\u00a0[12]: Copied! <pre># Gera amostras aleat\u00f3rias\nn_samples = 20\nz_random = np.random.normal(size=(n_samples, latent_dim)).astype('float32')\nsamples = vae.decode(z_random)\n\n# Visualiza\nfig, axes = plt.subplots(2, 10, figsize=(15, 3))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(samples[i].reshape(28, 28), cmap='gray')\n    ax.axis('off')\n\nplt.suptitle('Amostras Geradas pelo VAE (amostragem aleat\u00f3ria do espa\u00e7o latente)', fontsize=12)\nplt.tight_layout()\nplt.show()\n</pre> # Gera amostras aleat\u00f3rias n_samples = 20 z_random = np.random.normal(size=(n_samples, latent_dim)).astype('float32') samples = vae.decode(z_random)  # Visualiza fig, axes = plt.subplots(2, 10, figsize=(15, 3)) for i, ax in enumerate(axes.flat):     ax.imshow(samples[i].reshape(28, 28), cmap='gray')     ax.axis('off')  plt.suptitle('Amostras Geradas pelo VAE (amostragem aleat\u00f3ria do espa\u00e7o latente)', fontsize=12) plt.tight_layout() plt.show() In\u00a0[13]: Copied! <pre># Grade no espa\u00e7o latente\nn = 15\ndigit_size = 28\ngrid_x = np.linspace(-3, 3, n)\ngrid_y = np.linspace(-3, 3, n)[::-1]\nfigure = np.zeros((digit_size * n, digit_size * n))\n\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        z_sample = np.array([[xi, yi]], dtype='float32')\n        x_decoded = vae.decode(z_sample)\n        digit = x_decoded.reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] = digit\n\n# Visualiza\nplt.figure(figsize=(12, 12))\nplt.imshow(figure, cmap='gray')\nplt.title('Grade de D\u00edgitos Gerados no Espa\u00e7o Latente 2D\\n(de -3 a +3 em cada dimens\u00e3o)', fontsize=14)\nplt.xlabel('Dimens\u00e3o Latente 1 \u2192', fontsize=12)\nplt.ylabel('\u2190 Dimens\u00e3o Latente 2', fontsize=12)\nplt.xticks([])\nplt.yticks([])\nplt.tight_layout()\nplt.show()\n</pre> # Grade no espa\u00e7o latente n = 15 digit_size = 28 grid_x = np.linspace(-3, 3, n) grid_y = np.linspace(-3, 3, n)[::-1] figure = np.zeros((digit_size * n, digit_size * n))  for i, yi in enumerate(grid_y):     for j, xi in enumerate(grid_x):         z_sample = np.array([[xi, yi]], dtype='float32')         x_decoded = vae.decode(z_sample)         digit = x_decoded.reshape(digit_size, digit_size)         figure[i * digit_size: (i + 1) * digit_size,                j * digit_size: (j + 1) * digit_size] = digit  # Visualiza plt.figure(figsize=(12, 12)) plt.imshow(figure, cmap='gray') plt.title('Grade de D\u00edgitos Gerados no Espa\u00e7o Latente 2D\\n(de -3 a +3 em cada dimens\u00e3o)', fontsize=14) plt.xlabel('Dimens\u00e3o Latente 1 \u2192', fontsize=12) plt.ylabel('\u2190 Dimens\u00e3o Latente 2', fontsize=12) plt.xticks([]) plt.yticks([]) plt.tight_layout() plt.show() In\u00a0[14]: Copied! <pre># Interpola\u00e7\u00e3o entre duas imagens\nidx1, idx2 = 0, 50\nimg1 = X_test[idx1:idx1+1]\nimg2 = X_test[idx2:idx2+1]\nlabel1 = y_test[idx1]\nlabel2 = y_test[idx2]\n\n# Codifica\nmu1, _ = vae.encode(img1)\nmu2, _ = vae.encode(img2)\n\n# Interpola\nn_steps = 12\ninterpolations = []\n\nfor alpha in np.linspace(0, 1, n_steps):\n    z_interp = (1 - alpha) * mu1 + alpha * mu2\n    img_interp = vae.decode(z_interp)\n    interpolations.append(img_interp.reshape(28, 28))\n\n# Visualiza\nfig, axes = plt.subplots(1, n_steps, figsize=(15, 2))\nfor i, ax in enumerate(axes):\n    ax.imshow(interpolations[i], cmap='gray')\n    ax.axis('off')\n    if i == 0:\n        ax.set_title(f'{label1}', fontsize=10, color='blue')\n    elif i == n_steps - 1:\n        ax.set_title(f'{label2}', fontsize=10, color='red')\n\nplt.suptitle(f'Interpola\u00e7\u00e3o Linear no Espa\u00e7o Latente: {label1} \u2192 {label2}', fontsize=12)\nplt.tight_layout()\nplt.show()\n</pre> # Interpola\u00e7\u00e3o entre duas imagens idx1, idx2 = 0, 50 img1 = X_test[idx1:idx1+1] img2 = X_test[idx2:idx2+1] label1 = y_test[idx1] label2 = y_test[idx2]  # Codifica mu1, _ = vae.encode(img1) mu2, _ = vae.encode(img2)  # Interpola n_steps = 12 interpolations = []  for alpha in np.linspace(0, 1, n_steps):     z_interp = (1 - alpha) * mu1 + alpha * mu2     img_interp = vae.decode(z_interp)     interpolations.append(img_interp.reshape(28, 28))  # Visualiza fig, axes = plt.subplots(1, n_steps, figsize=(15, 2)) for i, ax in enumerate(axes):     ax.imshow(interpolations[i], cmap='gray')     ax.axis('off')     if i == 0:         ax.set_title(f'{label1}', fontsize=10, color='blue')     elif i == n_steps - 1:         ax.set_title(f'{label2}', fontsize=10, color='red')  plt.suptitle(f'Interpola\u00e7\u00e3o Linear no Espa\u00e7o Latente: {label1} \u2192 {label2}', fontsize=12) plt.tight_layout() plt.show() In\u00a0[15]: Copied! <pre># Treinamento de modelos com diferentes dimens\u00f5es latentes\nprint(\"Treinando VAEs com diferentes dimens\u00f5es latentes...\")\nprint(\"Nota: implementa\u00e7\u00e3o NumPy sem GPU\\n\")\n\nlatent_dims = [2, 10, 20]\nmodels_dict = {2: vae}\nhistories_dict = {2: val_losses}\n\nfor ld in [10, 20]:\n    print(f\"Treinando VAE latent_dim={ld} (10 \u00e9pocas)\")\n    model_temp = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=ld)\n    \n    temp_val_losses = []\n    for epoch in range(1, 11):\n        _ = train_epoch(model_temp, X_train, 128, 0.001)\n        val_loss, _, _ = validate(model_temp, X_val, 128)\n        temp_val_losses.append(val_loss)\n    \n    models_dict[ld] = model_temp\n    histories_dict[ld] = temp_val_losses\n    print(f\"Loss final: {temp_val_losses[-1]:.4f}\\n\")\n\nprint(\"Treinamento conclu\u00eddo\")\n</pre> # Treinamento de modelos com diferentes dimens\u00f5es latentes print(\"Treinando VAEs com diferentes dimens\u00f5es latentes...\") print(\"Nota: implementa\u00e7\u00e3o NumPy sem GPU\\n\")  latent_dims = [2, 10, 20] models_dict = {2: vae} histories_dict = {2: val_losses}  for ld in [10, 20]:     print(f\"Treinando VAE latent_dim={ld} (10 \u00e9pocas)\")     model_temp = VAE_NumPy(input_dim=784, hidden_dim=400, latent_dim=ld)          temp_val_losses = []     for epoch in range(1, 11):         _ = train_epoch(model_temp, X_train, 128, 0.001)         val_loss, _, _ = validate(model_temp, X_val, 128)         temp_val_losses.append(val_loss)          models_dict[ld] = model_temp     histories_dict[ld] = temp_val_losses     print(f\"Loss final: {temp_val_losses[-1]:.4f}\\n\")  print(\"Treinamento conclu\u00eddo\") <pre>Treinando VAEs com diferentes dimens\u00f5es latentes...\nNota: implementa\u00e7\u00e3o NumPy sem GPU\n\nTreinando VAE latent_dim=10 (10 \u00e9pocas)\nLoss final: 37.9164\n\nTreinando VAE latent_dim=20 (10 \u00e9pocas)\nLoss final: 37.8154\n\nTreinamento conclu\u00eddo\n</pre> In\u00a0[16]: Copied! <pre># Compara perdas\nplt.figure(figsize=(10, 5))\nfor ld in latent_dims:\n    epochs_range = range(1, len(histories_dict[ld]) + 1)\n    plt.plot(epochs_range, histories_dict[ld], label=f'Latent Dim = {ld}', marker='o')\n\nplt.xlabel('\u00c9poca')\nplt.ylabel('Perda de Valida\u00e7\u00e3o')\nplt.title('Compara\u00e7\u00e3o de Perdas para Diferentes Dimens\u00f5es Latentes')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Compara reconstru\u00e7\u00f5es\nfig, axes = plt.subplots(len(latent_dims) + 1, 10, figsize=(15, 2 * (len(latent_dims) + 1)))\ntest_data = X_test[:10]\n\n# Linha 0: originais\nfor j in range(10):\n    axes[0, j].imshow(test_data[j].reshape(28, 28), cmap='gray')\n    axes[0, j].axis('off')\n    if j == 0:\n        axes[0, j].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)\n\n# Linhas seguintes: reconstru\u00e7\u00f5es\nfor i, ld in enumerate(latent_dims):\n    model_temp = models_dict[ld]\n    mu, _ = model_temp.encode(test_data)\n    recon = model_temp.decode(mu)\n    \n    for j in range(10):\n        axes[i + 1, j].imshow(recon[j].reshape(28, 28), cmap='gray')\n        axes[i + 1, j].axis('off')\n        if j == 0:\n            axes[i + 1, j].set_ylabel(f'Latent {ld}D', rotation=0, labelpad=40, fontsize=10)\n\nplt.suptitle('Compara\u00e7\u00e3o de Reconstru\u00e7\u00f5es para Diferentes Dimens\u00f5es Latentes', fontsize=12)\nplt.tight_layout()\nplt.show()\n</pre> # Compara perdas plt.figure(figsize=(10, 5)) for ld in latent_dims:     epochs_range = range(1, len(histories_dict[ld]) + 1)     plt.plot(epochs_range, histories_dict[ld], label=f'Latent Dim = {ld}', marker='o')  plt.xlabel('\u00c9poca') plt.ylabel('Perda de Valida\u00e7\u00e3o') plt.title('Compara\u00e7\u00e3o de Perdas para Diferentes Dimens\u00f5es Latentes') plt.legend() plt.grid(True) plt.tight_layout() plt.show()  # Compara reconstru\u00e7\u00f5es fig, axes = plt.subplots(len(latent_dims) + 1, 10, figsize=(15, 2 * (len(latent_dims) + 1))) test_data = X_test[:10]  # Linha 0: originais for j in range(10):     axes[0, j].imshow(test_data[j].reshape(28, 28), cmap='gray')     axes[0, j].axis('off')     if j == 0:         axes[0, j].set_ylabel('Original', rotation=0, labelpad=40, fontsize=10)  # Linhas seguintes: reconstru\u00e7\u00f5es for i, ld in enumerate(latent_dims):     model_temp = models_dict[ld]     mu, _ = model_temp.encode(test_data)     recon = model_temp.decode(mu)          for j in range(10):         axes[i + 1, j].imshow(recon[j].reshape(28, 28), cmap='gray')         axes[i + 1, j].axis('off')         if j == 0:             axes[i + 1, j].set_ylabel(f'Latent {ld}D', rotation=0, labelpad=40, fontsize=10)  plt.suptitle('Compara\u00e7\u00e3o de Reconstru\u00e7\u00f5es para Diferentes Dimens\u00f5es Latentes', fontsize=12) plt.tight_layout() plt.show() <p>Este notebook foi desenvolvido com assist\u00eancia de IA (Claude Code) para estrutura\u00e7\u00e3o de c\u00f3digo e organiza\u00e7\u00e3o.</p> <p>Autor: Pedro Civita Data: Outubro 2025 Curso: Redes Neurais e Deep Learning - Insper</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#exercicio-4-variational-autoencoder-vae","title":"Exerc\u00edcio 4 - Variational Autoencoder (VAE)\u00b6","text":"<p>Neste notebook implemento um VAE para gerar e reconstruir imagens de d\u00edgitos do MNIST. O objetivo \u00e9 entender a arquitetura, processo de treinamento e capacidade generativa do modelo.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#o-que-e-um-vae","title":"O que \u00e9 um VAE?\u00b6","text":"<p>Um Variational Autoencoder \u00e9 um modelo generativo que aprende uma representa\u00e7\u00e3o latente probabil\u00edstica dos dados. Diferente de um autoencoder tradicional que mapeia cada entrada para um ponto fixo no espa\u00e7o latente, o VAE mapeia para uma distribui\u00e7\u00e3o de probabilidade (tipicamente Gaussiana).</p> <p>Componentes principais:</p> <ol> <li><p>Encoder: mapeia a entrada para par\u00e2metros de distribui\u00e7\u00e3o (m\u00e9dia \u03bc e log-vari\u00e2ncia log \u03c3\u00b2)</p> </li> <li><p>Reparameterization Trick: permite amostragem diferenci\u00e1vel: $$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$</p> </li> <li><p>Decoder: reconstr\u00f3i a entrada a partir da amostra latente z</p> </li> <li><p>Loss Function: combina dois termos:</p> <ul> <li>Reconstruction Loss: mede similaridade entre entrada e reconstru\u00e7\u00e3o</li> <li>KL Divergence: regulariza o espa\u00e7o latente para N(0, I)</li> </ul> </li> </ol> <p>$$\\mathcal{L} = \\text{Reconstru\u00e7\u00e3o} + \\text{KL Divergence}$$</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#instalacao-de-dependencias","title":"Instala\u00e7\u00e3o de Depend\u00eancias\u00b6","text":"<p>Implementa\u00e7\u00e3o do VAE usando NumPy puro, sem frameworks pesados que causam problemas de DLL no Windows.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#por-que-numpy-puro","title":"Por que NumPy Puro?\u00b6","text":"<p>PyTorch e TensorFlow apresentam erros de DLL no Windows. A solu\u00e7\u00e3o foi implementar o VAE completamente em NumPy.</p> <p>Vantagens:</p> <ul> <li>Funciona em qualquer sistema operacional sem depend\u00eancias pesadas</li> <li>C\u00f3digo completamente transparente e compreens\u00edvel</li> <li>Mais did\u00e1tico para entender os detalhes de implementa\u00e7\u00e3o</li> </ul> <p>Desvantagens:</p> <ul> <li>C\u00f3digo mais extenso</li> <li>Sem acelera\u00e7\u00e3o por GPU (mas MNIST \u00e9 leve o suficiente para CPU)</li> </ul>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#configuracao-do-projeto","title":"Configura\u00e7\u00e3o do Projeto\u00b6","text":"<p>Dataset: MNIST (70.000 imagens de d\u00edgitos 28x28 pixels)</p> <p>Framework: NumPy puro</p> <p>Arquitetura:</p> <ul> <li>Encoder: rede fully connected que mapeia imagens 784-D para m\u00e9dia e log-vari\u00e2ncia</li> <li>Decoder: rede fully connected que mapeia vetores latentes para imagens 784-D</li> <li>Dimens\u00e3o latente: 2D (para visualiza\u00e7\u00e3o), depois experimento com 10D e 20D</li> </ul> <p>Fun\u00e7\u00e3o de perda: MSE (reconstru\u00e7\u00e3o) + KL Divergence (regulariza\u00e7\u00e3o)</p> <p>Otimizador: Gradient Descent implementado manualmente</p> <p>Visualiza\u00e7\u00f5es:</p> <ul> <li>Compara\u00e7\u00e3o entre originais e reconstru\u00e7\u00f5es</li> <li>Espa\u00e7o latente 2D colorido por classe</li> <li>Gera\u00e7\u00e3o de novas amostras</li> <li>Interpola\u00e7\u00e3o no espa\u00e7o latente</li> <li>Compara\u00e7\u00e3o entre diferentes dimens\u00f5es latentes</li> </ul>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-1-preparacao-dos-dados","title":"Passo 1: Prepara\u00e7\u00e3o dos Dados\u00b6","text":"<p>Carregamento do MNIST com normaliza\u00e7\u00e3o para [0, 1] e divis\u00e3o em conjuntos de treino/valida\u00e7\u00e3o/teste.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-2-implementacao-do-vae","title":"Passo 2: Implementa\u00e7\u00e3o do VAE\u00b6","text":"<p>A arquitetura consiste em tr\u00eas componentes:</p> <ol> <li>Encoder: mapeia imagem para par\u00e2metros da distribui\u00e7\u00e3o latente (\u03bc e log \u03c3\u00b2)</li> <li>Reparameterization Trick: amostra z de forma diferenci\u00e1vel</li> <li>Decoder: reconstr\u00f3i imagem a partir de z</li> </ol> <p>A fun\u00e7\u00e3o de perda combina reconstruction loss (MSE) e KL divergence.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-3-funcoes-de-treinamento","title":"Passo 3: Fun\u00e7\u00f5es de Treinamento\u00b6","text":"<p>Implementa\u00e7\u00e3o do loop de treinamento e valida\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#analise-das-curvas","title":"An\u00e1lise das Curvas\u00b6","text":"<p>Perda Total: diminui em treino e valida\u00e7\u00e3o, indicando aprendizado sem overfitting significativo.</p> <p>Reconstruction Loss: mede qualidade da reconstru\u00e7\u00e3o. Diminui\u00e7\u00e3o indica melhoria.</p> <p>KL Divergence: regulariza o espa\u00e7o latente. Valores est\u00e1veis indicam bom equil\u00edbrio entre reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-4-avaliacao-reconstrucoes","title":"Passo 4: Avalia\u00e7\u00e3o - Reconstru\u00e7\u00f5es\u00b6","text":"<p>Compara\u00e7\u00e3o entre imagens originais e reconstru\u00eddas.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-5-visualizacao-do-espaco-latente","title":"Passo 5: Visualiza\u00e7\u00e3o do Espa\u00e7o Latente\u00b6","text":"<p>Plotagem do espa\u00e7o latente 2D com colora\u00e7\u00e3o por classe para visualizar a organiza\u00e7\u00e3o aprendida.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#analise-do-espaco-latente","title":"An\u00e1lise do Espa\u00e7o Latente\u00b6","text":"<p>Agrupamento: d\u00edgitos similares agrupam-se pr\u00f3ximos, demonstrando representa\u00e7\u00e3o significativa.</p> <p>Continuidade: espa\u00e7o suave sem descontinuidades, resultado da regulariza\u00e7\u00e3o KL.</p> <p>Sobreposi\u00e7\u00e3o: classes visualmente similares (4-9, 3-5) apresentam sobreposi\u00e7\u00e3o esperada.</p> <p>Organiza\u00e7\u00e3o: separa\u00e7\u00e3o razo\u00e1vel de 10 classes em apenas 2 dimens\u00f5es.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-6-geracao-de-amostras","title":"Passo 6: Gera\u00e7\u00e3o de Amostras\u00b6","text":"<p>Amostragem aleat\u00f3ria do espa\u00e7o latente N(0,1) para gera\u00e7\u00e3o de novos d\u00edgitos.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#exploracao-sistematica-do-espaco-latente","title":"Explora\u00e7\u00e3o Sistem\u00e1tica do Espa\u00e7o Latente\u00b6","text":"<p>Grade 15x15 de pontos no intervalo [-3, 3] mostrando transi\u00e7\u00f5es suaves entre d\u00edgitos.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#interpolacao-linear","title":"Interpola\u00e7\u00e3o Linear\u00b6","text":"<p>Interpola\u00e7\u00e3o entre dois pontos no espa\u00e7o latente demonstrando transi\u00e7\u00e3o suave entre d\u00edgitos.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#passo-7-comparacao-de-dimensoes-latentes","title":"Passo 7: Compara\u00e7\u00e3o de Dimens\u00f5es Latentes\u00b6","text":"<p>Experimento com dimens\u00f5es 2D, 10D e 20D para avaliar trade-off entre visualiza\u00e7\u00e3o e qualidade.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#analise-comparativa","title":"An\u00e1lise Comparativa\u00b6","text":"<p>2D:</p> <ul> <li>Visualiza\u00e7\u00e3o direta poss\u00edvel</li> <li>Reconstru\u00e7\u00f5es com mais perda de detalhes</li> <li>Loss de valida\u00e7\u00e3o: maior</li> </ul> <p>10D:</p> <ul> <li>Equil\u00edbrio entre capacidade e interpretabilidade</li> <li>Reconstru\u00e7\u00f5es significativamente melhores</li> <li>Loss intermedi\u00e1ria</li> </ul> <p>20D:</p> <ul> <li>Melhor qualidade de reconstru\u00e7\u00e3o</li> <li>Loss de valida\u00e7\u00e3o: menor</li> <li>Visualiza\u00e7\u00e3o requer redu\u00e7\u00e3o dimensional (t-SNE/UMAP)</li> </ul> <p>Conclus\u00e3o: dimens\u00e3o latente deve ser escolhida conforme objetivo (visualiza\u00e7\u00e3o vs qualidade).</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#relatorio-final","title":"Relat\u00f3rio Final\u00b6","text":""},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#resumo","title":"Resumo\u00b6","text":"<p>Implementa\u00e7\u00e3o completa de VAE em NumPy puro aplicado ao MNIST. Componentes implementados:</p> <ol> <li>Arquitetura: encoder probabil\u00edstico, reparameterization trick, decoder</li> <li>Loss: MSE (reconstru\u00e7\u00e3o) + KL divergence (regulariza\u00e7\u00e3o)</li> <li>Backpropagation manual para todos os pesos</li> <li>Treinamento: 20 \u00e9pocas com monitoring</li> <li>Avalia\u00e7\u00f5es: reconstru\u00e7\u00f5es, espa\u00e7o latente, gera\u00e7\u00e3o, interpola\u00e7\u00e3o</li> <li>Experimentos: compara\u00e7\u00e3o de dimens\u00f5es 2D, 10D e 20D</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#resultados","title":"Resultados\u00b6","text":"<p>Reconstru\u00e7\u00e3o:</p> <ul> <li>VAE reconstr\u00f3i d\u00edgitos com qualidade razo\u00e1vel</li> <li>Imagens ligeiramente borradas (caracter\u00edstica de VAEs)</li> <li>Dimens\u00f5es maiores produzem melhores reconstru\u00e7\u00f5es (loss ~47 em 2D vs ~38 em 20D)</li> </ul> <p>Espa\u00e7o Latente:</p> <ul> <li>Organiza\u00e7\u00e3o clara em 2D</li> <li>Espa\u00e7o cont\u00ednuo sem descontinuidades</li> <li>D\u00edgitos similares agrupam-se proximamente</li> </ul> <p>Gera\u00e7\u00e3o:</p> <ul> <li>Amostragem aleat\u00f3ria produz d\u00edgitos reconhec\u00edveis</li> <li>Grade mostra transi\u00e7\u00f5es suaves</li> <li>Interpola\u00e7\u00e3o gera morfismos realistas</li> </ul> <p>Dimensionalidade:</p> <ul> <li>Trade-off: visualiza\u00e7\u00e3o (2D) vs qualidade (20D)</li> </ul>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#desafios","title":"Desafios\u00b6","text":"<ol> <li>Backpropagation Manual: implementa\u00e7\u00e3o de gradientes complexa mas educativa</li> <li>Balanceamento de Loss: equil\u00edbrio entre reconstru\u00e7\u00e3o e KL</li> <li>Problemas de DLL: solu\u00e7\u00e3o atrav\u00e9s de implementa\u00e7\u00e3o NumPy pura</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#insights","title":"Insights\u00b6","text":"<p>VAE vs Autoencoder:</p> <ul> <li>VAE aprende distribui\u00e7\u00e3o, n\u00e3o pontos fixos</li> <li>Permite gera\u00e7\u00e3o, n\u00e3o apenas reconstru\u00e7\u00e3o</li> <li>KL divergence garante continuidade</li> </ul> <p>Reparameterization Trick:</p> <ul> <li>Essencial para backpropagation atrav\u00e9s de sampling</li> <li>Separa ru\u00eddo de par\u00e2metros aprend\u00edveis</li> </ul> <p>Aplica\u00e7\u00f5es:</p> <ul> <li>Compress\u00e3o de dados</li> <li>Detec\u00e7\u00e3o de anomalias</li> <li>Data augmentation</li> <li>Aprendizado de representa\u00e7\u00f5es</li> </ul>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#melhorias-possiveis","title":"Melhorias Poss\u00edveis\u00b6","text":"<ol> <li>Arquitetura convolucional para melhor captura espacial</li> <li>\u03b2-VAE para controle de disentanglement</li> <li>Conditional VAE para gera\u00e7\u00e3o controlada</li> <li>KL annealing durante treinamento</li> <li>Priors mais complexos</li> </ol>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#conclusao","title":"Conclus\u00e3o\u00b6","text":"<p>VAE combina deep learning com infer\u00eancia bayesiana, oferecendo:</p> <ul> <li>Espa\u00e7o latente interpret\u00e1vel e estruturado</li> <li>Capacidade generativa com controle</li> <li>Treinamento est\u00e1vel</li> </ul> <p>A implementa\u00e7\u00e3o em NumPy puro, embora mais trabalhosa, proporciona compreens\u00e3o profunda dos mecanismos internos do modelo.</p>"},{"location":"portfolio/neural-networks/exercises/4/ex04_pedrotpc/#referencias","title":"Refer\u00eancias\u00b6","text":"<ul> <li>Kingma, D. P., &amp; Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv preprint arXiv:1312.6114.</li> <li>Doersch, C. (2016). Tutorial on Variational Autoencoders. arXiv preprint arXiv:1606.05908.</li> <li>Material do Curso: Insper ANN-DL - VAE Exercise</li> <li>NumPy Documentation: https://numpy.org/doc/</li> <li>Dataset MNIST: LeCun, Y., Cortes, C., &amp; Burges, C. J. (1998).</li> </ul>"},{"location":"portfolio/neural-networks/exercises/4/main/","title":"Exercise 4","text":""},{"location":"portfolio/neural-networks/exercises/4/main/#exercise-4","title":"Exercise 4","text":"<p>Content coming soon.</p>"},{"location":"portfolio/neural-networks/presentation/","title":"Presentation","text":""},{"location":"portfolio/neural-networks/presentation/#presentation","title":"Presentation","text":"<p>Slides and materials for the final presentation will appear here.</p>"},{"location":"portfolio/neural-networks/projects/1/","title":"1. Classification","text":""},{"location":"portfolio/neural-networks/projects/1/#project-1","title":"Project 1","text":"<p>Description coming soon.</p>"},{"location":"portfolio/neural-networks/projects/1/main/","title":"Project 1","text":""},{"location":"portfolio/neural-networks/projects/1/main/#project-1","title":"Project 1","text":"<p>Description coming soon.</p>"},{"location":"portfolio/neural-networks/projects/2/","title":"2. Regression","text":""},{"location":"portfolio/neural-networks/projects/2/#projeto-de-regressao-bike-sharing-demand","title":"Projeto de Regress\u00e3o: Bike Sharing Demand","text":"<p>Grupo: Caio Boa, Gabriel Hermida e Pedro Civita Disciplina: Redes Neurais e Deep Learning Dataset: Bike Sharing (UCI ML Repository)</p>"},{"location":"portfolio/neural-networks/projects/2/#objetivo","title":"Objetivo","text":"<p>Implementar um Multi-Layer Perceptron (MLP) para prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.</p>"},{"location":"portfolio/neural-networks/projects/2/#1-selecao-do-dataset","title":"1. Sele\u00e7\u00e3o do Dataset","text":""},{"location":"portfolio/neural-networks/projects/2/#dataset-bike-sharing-demand","title":"Dataset: Bike Sharing Demand","text":"<p>Fonte: UCI ML Repository</p> <p>Caracter\u00edsticas: - Amostras: 17,379 registros (hor\u00e1rios de 2011-2012) - Features: 16 vari\u00e1veis (clima, tempo, sazonalidade) - Target: <code>cnt</code> - contagem total de alugu\u00e9is (regress\u00e3o)</p> <p>Motiva\u00e7\u00e3o: - Problema real de otimiza\u00e7\u00e3o urbana - Dataset n\u00e3o-trivial com padr\u00f5es temporais complexos - Evita datasets cl\u00e1ssicos (Boston/California Housing) - Dispon\u00edvel em competi\u00e7\u00e3o Kaggle (possibilidade de bonus)</p> <pre><code># Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura\u00e7\u00e3o\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Setup completo!\")</code></pre> <pre><code>Setup completo!\n</code></pre> <pre><code># Carregar dataset\ndf = pd.read_csv('hour.csv')\n\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nPrimeiras linhas:\")\ndf.head()</code></pre> <pre><code>Shape: (17379, 17)\n\nPrimeiras linhas:\n</code></pre> instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 <pre><code># Informa\u00e7\u00f5es do dataset\nprint(\"Informa\u00e7\u00f5es Gerais:\")\nprint(\"=\"*70)\ndf.info()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Estat\u00edsticas Descritivas:\")\nprint(\"=\"*70)\ndf.describe()</code></pre> <pre><code>Informa\u00e7\u00f5es Gerais:\n======================================================================\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17379 entries, 0 to 17378\nData columns (total 17 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   instant     17379 non-null  int64  \n 1   dteday      17379 non-null  object \n 2   season      17379 non-null  int64  \n 3   yr          17379 non-null  int64  \n 4   mnth        17379 non-null  int64  \n 5   hr          17379 non-null  int64  \n 6   holiday     17379 non-null  int64  \n 7   weekday     17379 non-null  int64  \n 8   workingday  17379 non-null  int64  \n 9   weathersit  17379 non-null  int64  \n 10  temp        17379 non-null  float64\n 11  atemp       17379 non-null  float64\n 12  hum         17379 non-null  float64\n 13  windspeed   17379 non-null  float64\n 14  casual      17379 non-null  int64  \n 15  registered  17379 non-null  int64  \n 16  cnt         17379 non-null  int64  \ndtypes: float64(4), int64(12), object(1)\nmemory usage: 2.3+ MB\n\n======================================================================\nEstat\u00edsticas Descritivas:\n======================================================================\n</code></pre> instant season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt count 17379.0000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 mean 8690.0000 2.501640 0.502561 6.537775 11.546752 0.028770 3.003683 0.682721 1.425283 0.496987 0.475775 0.627229 0.190098 35.676218 153.786869 189.463088 std 5017.0295 1.106918 0.500008 3.438776 6.914405 0.167165 2.005771 0.465431 0.639357 0.192556 0.171850 0.192930 0.122340 49.305030 151.357286 181.387599 min 1.0000 1.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.020000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 25% 4345.5000 2.000000 0.000000 4.000000 6.000000 0.000000 1.000000 0.000000 1.000000 0.340000 0.333300 0.480000 0.104500 4.000000 34.000000 40.000000 50% 8690.0000 3.000000 1.000000 7.000000 12.000000 0.000000 3.000000 1.000000 1.000000 0.500000 0.484800 0.630000 0.194000 17.000000 115.000000 142.000000 75% 13034.5000 3.000000 1.000000 10.000000 18.000000 0.000000 5.000000 1.000000 2.000000 0.660000 0.621200 0.780000 0.253700 48.000000 220.000000 281.000000 max 17379.0000 4.000000 1.000000 12.000000 23.000000 1.000000 6.000000 1.000000 4.000000 1.000000 1.000000 1.000000 0.850700 367.000000 886.000000 977.000000"},{"location":"portfolio/neural-networks/projects/2/#2-analise-exploratoria","title":"2. An\u00e1lise Explorat\u00f3ria","text":"<p>An\u00e1lise inicial para entender padr\u00f5es e rela\u00e7\u00f5es nos dados.</p> <pre><code># Verificar valores ausentes\nprint(\"Valores Ausentes:\")\nprint(df.isnull().sum())\n\n# An\u00e1lise da vari\u00e1vel target\nprint(f\"\\nTarget (cnt) - Estat\u00edsticas:\")\nprint(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\")\nprint(f\"  Mediana: {df['cnt'].median():.2f}\")\nprint(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\")\nprint(f\"  Min: {df['cnt'].min()}\")\nprint(f\"  Max: {df['cnt'].max()}\")</code></pre> <pre><code>Valores Ausentes:\ninstant       0\ndteday        0\nseason        0\nyr            0\nmnth          0\nhr            0\nholiday       0\nweekday       0\nworkingday    0\nweathersit    0\ntemp          0\natemp         0\nhum           0\nwindspeed     0\ncasual        0\nregistered    0\ncnt           0\ndtype: int64\n\nTarget (cnt) - Estat\u00edsticas:\n  M\u00e9dia: 189.46\n  Mediana: 142.00\n  Desvio padr\u00e3o: 181.39\n  Min: 1\n  Max: 977\n</code></pre> <pre><code># Visualiza\u00e7\u00f5es explorat\u00f3rias\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Distribui\u00e7\u00e3o do target\naxes[0,0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7)\naxes[0,0].set_title('Distribui\u00e7\u00e3o da Demanda')\naxes[0,0].set_xlabel('N\u00famero de alugu\u00e9is')\naxes[0,0].set_ylabel('Frequ\u00eancia')\naxes[0,0].grid(True, alpha=0.3)\n\n# Demanda por hora\nhourly = df.groupby('hr')['cnt'].mean()\naxes[0,1].plot(hourly.index, hourly.values, marker='o', linewidth=2)\naxes[0,1].set_title('Demanda M\u00e9dia por Hora')\naxes[0,1].set_xlabel('Hora do dia')\naxes[0,1].set_ylabel('Alugu\u00e9is m\u00e9dios')\naxes[0,1].grid(True, alpha=0.3)\n\n# Temperatura vs Demanda\naxes[1,0].scatter(df['temp'], df['cnt'], alpha=0.3, s=10)\naxes[1,0].set_title('Temperatura vs Demanda')\naxes[1,0].set_xlabel('Temperatura normalizada')\naxes[1,0].set_ylabel('Alugu\u00e9is')\naxes[1,0].grid(True, alpha=0.3)\n\n# Correla\u00e7\u00e3o\ncorr_cols = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\ncorr = df[corr_cols].corr()\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n            ax=axes[1,1], square=True, linewidths=1)\naxes[1,1].set_title('Matriz de Correla\u00e7\u00e3o')\n\nplt.tight_layout()\nplt.show()</code></pre> <p></p>"},{"location":"portfolio/neural-networks/projects/2/#3-limpeza-e-normalizacao-dos-dados","title":"3. Limpeza e Normaliza\u00e7\u00e3o dos Dados","text":""},{"location":"portfolio/neural-networks/projects/2/#estrategia-de-pre-processamento","title":"Estrat\u00e9gia de Pr\u00e9-processamento:","text":"<ol> <li>Remo\u00e7\u00e3o de features: <code>instant</code>, <code>dteday</code>, <code>casual</code>, <code>registered</code> (evitar data leakage)</li> <li>Feature engineering: Transforma\u00e7\u00e3o c\u00edclica para vari\u00e1veis temporais (sin/cos)</li> <li>Normaliza\u00e7\u00e3o: Z-score standardization (m\u00e9dia=0, std=1)</li> </ol> <pre><code># Preparar dados\ndf_prep = df.copy()\n\n# Remover colunas\ndrop_cols = ['instant', 'dteday', 'casual', 'registered']\ndf_prep = df_prep.drop(columns=drop_cols)\n\nprint(f\"Colunas removidas: {drop_cols}\")\nprint(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_prep.shape}\")</code></pre> <pre><code>Colunas removidas: ['instant', 'dteday', 'casual', 'registered']\nShape ap\u00f3s remo\u00e7\u00e3o: (17379, 13)\n</code></pre> <pre><code># Feature engineering: vari\u00e1veis c\u00edclicas\n# Hora, m\u00eas e dia da semana s\u00e3o c\u00edclicos (ex: hora 23 pr\u00f3xima de hora 0)\ndef encode_cyclical(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n    return data\n\ndf_prep = encode_cyclical(df_prep, 'hr', 24)\ndf_prep = encode_cyclical(df_prep, 'mnth', 12)\ndf_prep = encode_cyclical(df_prep, 'weekday', 7)\n\n# Remover originais\ndf_prep = df_prep.drop(columns=['hr', 'mnth', 'weekday'])\n\nprint(f\"Features c\u00edclicas criadas!\")\nprint(f\"Shape final: {df_prep.shape}\")\nprint(f\"\\nFeatures: {list(df_prep.columns)}\")</code></pre> <pre><code>Features c\u00edclicas criadas!\nShape final: (17379, 16)\n\nFeatures: ['season', 'yr', 'holiday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt', 'hr_sin', 'hr_cos', 'mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n</code></pre> <pre><code># Separar X e y\nX = df_prep.drop(columns=['cnt']).values\ny = df_prep['cnt'].values.reshape(-1, 1)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Salvar estat\u00edsticas para desnormaliza\u00e7\u00e3o posterior\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\ny_mean = y.mean()\ny_std = y.std()\n\n# Normalizar (z-score)\nX_norm = (X - X_mean) / (X_std + 1e-8)\ny_norm = (y - y_mean) / y_std\n\nprint(f\"\\nAp\u00f3s normaliza\u00e7\u00e3o:\")\nprint(f\"X: m\u00e9dia={X_norm.mean():.3f}, std={X_norm.std():.3f}\")\nprint(f\"y: m\u00e9dia={y_norm.mean():.3f}, std={y_norm.std():.3f}\")</code></pre> <pre><code>X shape: (17379, 15)\ny shape: (17379, 1)\n\nAp\u00f3s normaliza\u00e7\u00e3o:\nX: m\u00e9dia=0.000, std=1.000\ny: m\u00e9dia=-0.000, std=1.000\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#4-estrategia-de-divisao-trainvalidationtest","title":"4. Estrat\u00e9gia de Divis\u00e3o Train/Validation/Test","text":"<p>Divis\u00e3o: 70% treino / 15% valida\u00e7\u00e3o / 15% teste</p> <p>Modo de treinamento: Mini-batch gradient descent (batch_size=64)</p> <p>Justificativa: - 70% treino: volume suficiente para aprendizado - 15% valida\u00e7\u00e3o: monitorar overfitting e early stopping - 15% teste: avalia\u00e7\u00e3o final imparcial - Mini-batch: equil\u00edbrio entre velocidade e estabilidade</p> <pre><code># Dividir dados\nn_samples = X_norm.shape[0]\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\n\n# Calcular tamanhos\ntrain_size = int(0.70 * n_samples)\nval_size = int(0.15 * n_samples)\ntest_size = n_samples - train_size - val_size\n\n# Dividir \u00edndices\ntrain_idx = indices[:train_size]\nval_idx = indices[train_size:train_size + val_size]\ntest_idx = indices[train_size + val_size:]\n\n# Criar conjuntos\nX_train, y_train = X_norm[train_idx], y_norm[train_idx]\nX_val, y_val = X_norm[val_idx], y_norm[val_idx]\nX_test, y_test = X_norm[test_idx], y_norm[test_idx]\n\nprint(f\"Divis\u00e3o dos dados:\")\nprint(f\"  Train: {len(X_train):,} ({len(X_train)/n_samples*100:.1f}%)\")\nprint(f\"  Val:   {len(X_val):,} ({len(X_val)/n_samples*100:.1f}%)\")\nprint(f\"  Test:  {len(X_test):,} ({len(X_test)/n_samples*100:.1f}%)\")</code></pre> <pre><code>Divis\u00e3o dos dados:\n  Train: 12,165 (70.0%)\n  Val:   2,606 (15.0%)\n  Test:  2,608 (15.0%)\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#5-implementacao-do-mlp","title":"5. Implementa\u00e7\u00e3o do MLP","text":"<p>Arquitetura: Input(15) \u2192 Hidden(64, ReLU) \u2192 Hidden(32, ReLU) \u2192 Hidden(16, ReLU) \u2192 Output(1, Linear)</p> <p>Componentes: - Inicializa\u00e7\u00e3o: He initialization - Ativa\u00e7\u00e3o: ReLU (camadas ocultas), Linear (sa\u00edda) - Loss: MSE + Regulariza\u00e7\u00e3o L2 - Otimiza\u00e7\u00e3o: Mini-batch gradient descent - Regulariza\u00e7\u00e3o: Early stopping + L2</p> <pre><code>class MLP:\n    \"\"\"Multi-Layer Perceptron para Regress\u00e3o\"\"\"\n\n    def __init__(self, layers, learning_rate=0.001, reg_lambda=0.001):\n        self.lr = learning_rate\n        self.reg = reg_lambda\n        self.weights = []\n        self.biases = []\n\n        # He initialization\n        for i in range(len(layers) - 1):\n            W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n        self.train_losses = []\n        self.val_losses = []\n\n    def relu(self, x):\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        return (x &gt; 0).astype(float)\n\n    def forward(self, X):\n        \"\"\"Forward propagation\"\"\"\n        self.cache = {'A': [X], 'Z': []}\n        A = X\n\n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            self.cache['Z'].append(Z)\n            self.cache['A'].append(A)\n\n        # Output layer (linear)\n        Z = A @ self.weights[-1] + self.biases[-1]\n        self.cache['Z'].append(Z)\n        self.cache['A'].append(Z)\n\n        return Z\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"MSE + L2 regularization\"\"\"\n        n = len(y_true)\n        mse = np.mean((y_pred - y_true) ** 2)\n        l2 = sum(np.sum(W ** 2) for W in self.weights)\n        return mse + (self.reg / (2 * n)) * l2\n\n    def backward(self, y_true):\n        \"\"\"Backpropagation\"\"\"\n        n = len(y_true)\n        y_pred = self.cache['A'][-1]\n        dA = (2.0 / n) * (y_pred - y_true)\n\n        grads_W = []\n        grads_b = []\n\n        for i in reversed(range(len(self.weights))):\n            A_prev = self.cache['A'][i]\n\n            dW = A_prev.T @ dA + (self.reg / n) * self.weights[i]\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            grads_W.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = (dA @ self.weights[i].T) * self.relu_derivative(self.cache['Z'][i-1])\n\n        # Update weights\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.lr * grads_W[i]\n            self.biases[i] -= self.lr * grads_b[i]\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15, verbose=True):\n        \"\"\"Treinar modelo com early stopping\"\"\"\n        best_loss = float('inf')\n        patience_count = 0\n\n        for epoch in range(epochs):\n            # Mini-batch training\n            indices = np.arange(len(X_train))\n            np.random.shuffle(indices)\n\n            for start in range(0, len(X_train), batch_size):\n                end = min(start + batch_size, len(X_train))\n                batch_idx = indices[start:end]\n\n                X_batch = X_train[batch_idx]\n                y_batch = y_train[batch_idx]\n\n                self.forward(X_batch)\n                self.backward(y_batch)\n\n            # Calcular losses\n            train_pred = self.forward(X_train)\n            val_pred = self.forward(X_val)\n\n            train_loss = self.compute_loss(y_train, train_pred)\n            val_loss = self.compute_loss(y_val, val_pred)\n\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                patience_count = 0\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_count += 1\n\n            if verbose and (epoch + 1) % 20 == 0:\n                print(f\"\u00c9poca {epoch+1:3d}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n\n            if patience_count &gt;= patience:\n                if verbose:\n                    print(f\"\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        self.weights = self.best_weights\n        self.biases = self.best_biases\n\n    def predict(self, X):\n        \"\"\"Fazer predi\u00e7\u00f5es\"\"\"\n        A = X\n        for i in range(len(self.weights) - 1):\n            A = self.relu(A @ self.weights[i] + self.biases[i])\n        return A @ self.weights[-1] + self.biases[-1]\n\nprint(\"MLP implementado!\")</code></pre> <pre><code>MLP implementado!\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#6-treinamento-do-modelo","title":"6. Treinamento do Modelo","text":"<pre><code># Criar e treinar modelo\nmodel = MLP(\n    layers=[15, 64, 32, 16, 1],\n    learning_rate=0.001,\n    reg_lambda=0.001\n)\n\nprint(\"Iniciando treinamento...\\n\")\nmodel.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15)\nprint(\"\\nTreinamento conclu\u00eddo!\")</code></pre> <pre><code>Iniciando treinamento...\n\n\u00c9poca  20/200 | Train: 0.3770 | Val: 0.4228\n\u00c9poca  40/200 | Train: 0.2984 | Val: 0.3333\n\u00c9poca  60/200 | Train: 0.2369 | Val: 0.2631\n\u00c9poca  80/200 | Train: 0.1905 | Val: 0.2114\n\u00c9poca 100/200 | Train: 0.1620 | Val: 0.1811\n\u00c9poca 120/200 | Train: 0.1443 | Val: 0.1618\n\u00c9poca 140/200 | Train: 0.1343 | Val: 0.1538\n\u00c9poca 160/200 | Train: 0.1233 | Val: 0.1414\n\u00c9poca 180/200 | Train: 0.1158 | Val: 0.1359\n\u00c9poca 200/200 | Train: 0.1083 | Val: 0.1279\n\nTreinamento conclu\u00eddo!\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#7-curvas-de-erro-e-visualizacoes","title":"7. Curvas de Erro e Visualiza\u00e7\u00f5es","text":"<pre><code># Plot curvas de loss\nplt.figure(figsize=(10, 5))\nepochs = range(1, len(model.train_losses) + 1)\n\nplt.plot(epochs, model.train_losses, label='Train Loss', linewidth=2)\nplt.plot(epochs, model.val_losses, label='Validation Loss', linewidth=2)\nplt.xlabel('\u00c9poca')\nplt.ylabel('Loss (MSE + L2)')\nplt.title('Converg\u00eancia do Modelo')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u00c9pocas treinadas: {len(model.train_losses)}\")\nprint(f\"Loss final - Train: {model.train_losses[-1]:.4f} | Val: {model.val_losses[-1]:.4f}\")</code></pre> <pre><code>\u00c9pocas treinadas: 200\nLoss final - Train: 0.1083 | Val: 0.1279\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#8-metricas-de-avaliacao","title":"8. M\u00e9tricas de Avalia\u00e7\u00e3o","text":"<p>M\u00e9tricas obrigat\u00f3rias para regress\u00e3o: MAE, MAPE, MSE, RMSE, R\u00b2</p> <pre><code># Fun\u00e7\u00e3o para calcular m\u00e9tricas\ndef calc_metrics(y_true, y_pred):\n    y_true = y_true.flatten()\n    y_pred = y_pred.flatten()\n\n    mae = np.mean(np.abs(y_true - y_pred))\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n\n    # MAPE (evitar divis\u00e3o por zero)\n    mask = y_true != 0\n    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n\n# Fazer predi\u00e7\u00f5es e desnormalizar\ny_pred_test_norm = model.predict(X_test)\ny_pred_test = y_pred_test_norm * y_std + y_mean\ny_true_test = y_test * y_std + y_mean\n\ny_pred_train_norm = model.predict(X_train)\ny_pred_train = y_pred_train_norm * y_std + y_mean\ny_true_train = y_train * y_std + y_mean\n\n# Calcular m\u00e9tricas\ntrain_metrics = calc_metrics(y_true_train, y_pred_train)\ntest_metrics = calc_metrics(y_true_test, y_pred_test)\n\n# Baseline (preditor de m\u00e9dia)\nbaseline_pred = np.full_like(y_true_test, y_true_train.mean())\nbaseline_metrics = calc_metrics(y_true_test, baseline_pred)\n\n# Exibir resultados\nprint(\"M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\")\nprint(\"=\"*70)\nprint(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15} {'Baseline':&lt;15}\")\nprint(\"-\"*70)\nprint(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f} {baseline_metrics['MAE']:&lt;15.2f}\")\nprint(f\"{'MSE':&lt;10} {train_metrics['MSE']:&lt;15.2f} {test_metrics['MSE']:&lt;15.2f} {baseline_metrics['MSE']:&lt;15.2f}\")\nprint(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f} {baseline_metrics['RMSE']:&lt;15.2f}\")\nprint(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f} {baseline_metrics['R2']:&lt;15.4f}\")\nprint(f\"{'MAPE (%)':&lt;10} {train_metrics['MAPE']:&lt;15.2f} {test_metrics['MAPE']:&lt;15.2f} {baseline_metrics['MAPE']:&lt;15.2f}\")\nprint(\"=\"*70)\n\nmelhoria = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100)\nprint(f\"\\nMelhoria sobre baseline: {melhoria:.1f}%\")</code></pre> <pre><code>M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\n======================================================================\nM\u00e9trica    Train           Test            Baseline       \n----------------------------------------------------------------------\nMAE        42.95           44.83           143.29         \nMSE        3562.55         4024.64         33800.49       \nRMSE       59.69           63.44           183.85         \nR\u00b2         0.8894          0.8809          -0.0000        \nMAPE (%)   103.73          101.15          796.90         \n======================================================================\n\nMelhoria sobre baseline: 65.5%\n</code></pre> <pre><code># Visualiza\u00e7\u00f5es de avalia\u00e7\u00e3o\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Predito vs Real\naxes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=15)\nlim = [y_true_test.min(), y_true_test.max()]\naxes[0].plot(lim, lim, 'r--', linewidth=2, label='Predi\u00e7\u00e3o perfeita')\naxes[0].set_xlabel('Valor Real')\naxes[0].set_ylabel('Valor Predito')\naxes[0].set_title(f'Predi\u00e7\u00f5es vs Real\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Res\u00edduos\nresiduals = (y_true_test - y_pred_test).flatten()\naxes[1].scatter(y_pred_test, residuals, alpha=0.4, s=15)\naxes[1].axhline(0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Valor Predito')\naxes[1].set_ylabel('Res\u00edduo (Real - Predito)')\naxes[1].set_title('An\u00e1lise de Res\u00edduos')\naxes[1].grid(True, alpha=0.3)\n\n# Distribui\u00e7\u00e3o dos res\u00edduos\naxes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[2].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero')\naxes[2].axvline(residuals.mean(), color='g', linestyle='--', linewidth=2, label=f'M\u00e9dia: {residuals.mean():.1f}')\naxes[2].set_xlabel('Res\u00edduo')\naxes[2].set_ylabel('Frequ\u00eancia')\naxes[2].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estat\u00edsticas dos res\u00edduos:\")\nprint(f\"  M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\")\nprint(f\"  Std: {residuals.std():.2f}\")\nprint(f\"  Mediana do erro absoluto: {np.median(np.abs(residuals)):.2f}\")</code></pre> <p></p> <pre><code>Estat\u00edsticas dos res\u00edduos:\n  M\u00e9dia: -1.37 (ideal: ~0)\n  Std: 63.43\n  Mediana do erro absoluto: 31.89\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/#9-conclusao","title":"9. Conclus\u00e3o","text":""},{"location":"portfolio/neural-networks/projects/2/#resultados","title":"Resultados:","text":"<ul> <li>R\u00b2 = 0.8809: Modelo explica ~88.1% da vari\u00e2ncia</li> <li>RMSE = 63.44: Erro m\u00e9dio de ~63 alugu\u00e9is</li> <li>Melhoria sobre baseline: 65.5%</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/#limitacoes","title":"Limita\u00e7\u00f5es:","text":"<ul> <li>MLP n\u00e3o captura depend\u00eancias temporais de longo prazo</li> <li>Features podem ser expandidas (lags, intera\u00e7\u00f5es)</li> <li>Hiperpar\u00e2metros n\u00e3o foram otimizados sistematicamente</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/#melhorias-futuras","title":"Melhorias Futuras:","text":"<ul> <li>Arquiteturas recorrentes (LSTM/GRU) para s\u00e9ries temporais</li> <li>Feature engineering mais sofisticado</li> <li>Ensemble com outros modelos (XGBoost, Random Forest)</li> <li>Grid search para otimiza\u00e7\u00e3o de hiperpar\u00e2metros</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/#ferramentas-de-ia","title":"Ferramentas de IA:","text":"<p>Claude foi usado para auxiliar na estrutura\u00e7\u00e3o do c\u00f3digo. Todo o c\u00f3digo foi compreendido e validado manualmente, c\u00e9lula por c\u00e9lula.</p>"},{"location":"portfolio/neural-networks/projects/2/#referencias","title":"Refer\u00eancias:","text":"<ul> <li>Dataset: Fanaee-T &amp; Gama (2013), UCI ML Repository</li> <li>Implementa\u00e7\u00e3o: NumPy, conceitos de Goodfellow et al. (Deep Learning, 2016)</li> <li>Material do curso: https://insper.github.io/ann-dl/</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/","title":"\ud83d\ude80 Guia R\u00e1pido de In\u00edcio","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#guia-rapido-de-inicio","title":"\ud83d\ude80 Guia R\u00e1pido de In\u00edcio","text":"<p>Projeto de Regress\u00e3o - Bike Sharing Demand</p>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#inicio-rapido-3-passos","title":"\u26a1 In\u00edcio R\u00e1pido (3 passos)","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#1-instalar-dependencias","title":"1\ufe0f\u20e3 Instalar Depend\u00eancias","text":"<pre><code>pip install numpy pandas matplotlib seaborn jupyter\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#2-baixar-dataset","title":"2\ufe0f\u20e3 Baixar Dataset","text":"<p>Op\u00e7\u00e3o A - Autom\u00e1tico (recomendado): </p><pre><code>python download_dataset.py\n</code></pre><p></p> <p>Op\u00e7\u00e3o B - Manual: 1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset 2. Baixe <code>Bike-Sharing-Dataset.zip</code> 3. Extraia e copie <code>hour.csv</code> para esta pasta</p>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#3-executar-notebook","title":"3\ufe0f\u20e3 Executar Notebook","text":"<pre><code>jupyter notebook regression.ipynb\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#o-que-esperar","title":"\ud83d\udcca O que esperar","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#estrutura-do-notebook-49-celulas","title":"Estrutura do Notebook (49 c\u00e9lulas)","text":"<pre><code>\ud83d\udccc Se\u00e7\u00f5es Principais:\n\u251c\u2500\u2500 1-4:   Introdu\u00e7\u00e3o e Dataset\n\u251c\u2500\u2500 5-9:   Explora\u00e7\u00e3o de Dados\n\u251c\u2500\u2500 10-14: Pr\u00e9-processamento\n\u251c\u2500\u2500 15-20: Implementa\u00e7\u00e3o MLP\n\u251c\u2500\u2500 21-30: Treinamento\n\u251c\u2500\u2500 31-40: Avalia\u00e7\u00e3o\n\u2514\u2500\u2500 41-49: An\u00e1lise e Conclus\u00e3o\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#tempo-estimado","title":"Tempo Estimado","text":"<ul> <li>\u23f1\ufe0f Execu\u00e7\u00e3o completa: ~5-10 minutos</li> <li>\u23f1\ufe0f Treinamento do modelo: ~2-3 minutos</li> <li>\u23f1\ufe0f Leitura e an\u00e1lise: ~30-45 minutos</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#checklist-de-execucao","title":"\ud83c\udfaf Checklist de Execu\u00e7\u00e3o","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#antes-de-executar","title":"Antes de Executar","text":"<ul> <li> Python 3.8+ instalado</li> <li> Bibliotecas instaladas (numpy, pandas, matplotlib, seaborn)</li> <li> Arquivo <code>hour.csv</code> na pasta do notebook</li> <li> Jupyter Notebook funcionando</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#durante-a-execucao","title":"Durante a Execu\u00e7\u00e3o","text":"<ul> <li> Executar c\u00e9lulas sequencialmente (n\u00e3o pular c\u00e9lulas)</li> <li> Verificar se dataset foi carregado (c\u00e9lula 4)</li> <li> Aguardar treinamento completo (~100-150 \u00e9pocas)</li> <li> Verificar converg\u00eancia nas curvas de loss</li> <li> Analisar m\u00e9tricas finais (R\u00b2, RMSE, MAE)</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#apos-execucao","title":"Ap\u00f3s Execu\u00e7\u00e3o","text":"<ul> <li> R\u00b2 &gt; 0.3 no test set (m\u00ednimo aceit\u00e1vel)</li> <li> Gap train-val &lt; 20% (sem overfitting severo)</li> <li> Res\u00edduos centrados em zero</li> <li> Visualiza\u00e7\u00f5es geradas corretamente</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#hiperparametros-principais","title":"\ud83d\udd27 Hiperpar\u00e2metros Principais","text":"Par\u00e2metro Valor Padr\u00e3o Onde Modificar Learning Rate 0.001 C\u00e9lula 30 (<code>LEARNING_RATE</code>) Batch Size 64 C\u00e9lula 30 (<code>BATCH_SIZE</code>) \u00c9pocas M\u00e1x 200 C\u00e9lula 30 (<code>EPOCHS</code>) Arquitetura [64,32,16] C\u00e9lula 30 (<code>HIDDEN_SIZES</code>) L2 Lambda 0.001 C\u00e9lula 30 (<code>REG_LAMBDA</code>) <p>\ud83d\udca1 Dica: Para experimentar, modifique os valores e re-execute a c\u00e9lula 30 em diante.</p>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#resultados-esperados","title":"\ud83d\udcc8 Resultados Esperados","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#metricas-tipicas-apos-treinamento","title":"M\u00e9tricas T\u00edpicas (ap\u00f3s treinamento)","text":"M\u00e9trica Valor Esperado Interpreta\u00e7\u00e3o R\u00b2 0.40 - 0.70 Quanto da vari\u00e2ncia \u00e9 explicada RMSE 60 - 100 Erro m\u00e9dio em n\u00ba de bikes MAE 40 - 70 Erro absoluto m\u00e9dio MAPE 20% - 40% Erro percentual"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#curva-de-convergencia","title":"Curva de Converg\u00eancia","text":"<pre><code>\u2713 Esperado:\n  - Loss decrescente nas primeiras 50 \u00e9pocas\n  - Plateau ou oscila\u00e7\u00e3o m\u00ednima depois\n  - Train e Val loss pr\u00f3ximos (gap &lt; 20%)\n\n\u274c Problemas:\n  - Loss crescente: learning rate muito alto\n  - N\u00e3o converge: learning rate muito baixo\n  - Val &gt;&gt; Train: overfitting\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#erro-file-hourcsv-not-found","title":"Erro: \"File 'hour.csv' not found\"","text":"<pre><code># Solu\u00e7\u00e3o:\npython download_dataset.py\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#erro-modulenotfounderror-no-module-named-numpy","title":"Erro: \"ModuleNotFoundError: No module named 'numpy'\"","text":"<pre><code># Solu\u00e7\u00e3o:\npip install numpy pandas matplotlib seaborn\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#erro-kernel-died","title":"Erro: \"Kernel died\"","text":"<pre><code># Poss\u00edveis causas:\n# 1. Mem\u00f3ria insuficiente\n# 2. Conflito de vers\u00f5es\n\n# Solu\u00e7\u00e3o:\n# Reinicie o kernel: Kernel \u2192 Restart\n# Se persistir, reduza BATCH_SIZE para 32\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#warning-runtimewarning-overflow-encountered","title":"Warning: \"RuntimeWarning: overflow encountered\"","text":"<pre><code># Causa: Learning rate muito alto ou pesos explodindo\n# Solu\u00e7\u00e3o:\n# 1. Reduza LEARNING_RATE para 0.0005\n# 2. Aumente REG_LAMBDA para 0.01\n# 3. Re-execute o treinamento\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#loss-nao-converge-fica-oscilando","title":"Loss n\u00e3o converge (fica oscilando)","text":"<pre><code># Solu\u00e7\u00f5es:\n# 1. Reduza LEARNING_RATE (ex: 0.0005)\n# 2. Aumente BATCH_SIZE (ex: 128)\n# 3. Aumente L2 regularization (REG_LAMBDA = 0.01)\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#overfitting-val-train-loss","title":"Overfitting (Val &gt;&gt; Train loss)","text":"<pre><code># Solu\u00e7\u00f5es:\n# 1. Aumente REG_LAMBDA (ex: 0.01)\n# 2. Reduza arquitetura (ex: [32, 16, 8])\n# 3. Reduza EARLY_STOPPING_PATIENCE (ex: 10)\n</code></pre>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#secoes-importantes","title":"\ud83d\udcda Se\u00e7\u00f5es Importantes","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#celulas-essenciais-nao-pule","title":"C\u00e9lulas Essenciais (n\u00e3o pule!)","text":"<ul> <li>C\u00e9lula 3: Carregamento do dataset</li> <li>C\u00e9lula 7: An\u00e1lise de outliers</li> <li>C\u00e9lula 14: Normaliza\u00e7\u00e3o (salva y_mean e y_std)</li> <li>C\u00e9lula 16: Split train/val/test</li> <li>C\u00e9lulas 18-20: Implementa\u00e7\u00e3o MLP (classe completa)</li> <li>C\u00e9lula 21: Treinamento (pode demorar 2-3 min)</li> <li>C\u00e9lula 28: Desnormaliza\u00e7\u00e3o (cr\u00edtico para m\u00e9tricas corretas)</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#celulas-opcionais-podem-pular-se-tiver-pressa","title":"C\u00e9lulas Opcionais (podem pular se tiver pressa)","text":"<ul> <li>C\u00e9lulas de visualiza\u00e7\u00e3o explorat\u00f3ria (5, 6, 8, 9)</li> <li>An\u00e1lise detalhada de res\u00edduos (37, 38)</li> <li>Exemplos de predi\u00e7\u00f5es (39)</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#dicas-de-otimizacao","title":"\ud83d\udca1 Dicas de Otimiza\u00e7\u00e3o","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#para-resultados-melhores","title":"Para Resultados Melhores","text":"<ol> <li>Feature Engineering:</li> <li>J\u00e1 implementado: transforma\u00e7\u00e3o c\u00edclica (sin/cos)</li> <li> <p>Experimente: adicionar lags temporais, intera\u00e7\u00f5es</p> </li> <li> <p>Arquitetura:</p> </li> <li>Teste diferentes profundidades: [128,64,32], [32,16]</li> <li> <p>Cuidado com underfitting (muito simples) ou overfitting (muito complexo)</p> </li> <li> <p>Hiperpar\u00e2metros:</p> </li> <li>Learning rate: teste [0.0001, 0.0005, 0.001, 0.005]</li> <li>Batch size: teste [32, 64, 128, 256]</li> <li> <p>L2 lambda: teste [0.0001, 0.001, 0.01]</p> </li> <li> <p>Treinamento:</p> </li> <li>Aumente EPOCHS para 300-500 se converg\u00eancia lenta</li> <li>EARLY_STOPPING_PATIENCE pode ir at\u00e9 20-30</li> </ol>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#para-execucao-mais-rapida","title":"Para Execu\u00e7\u00e3o Mais R\u00e1pida","text":"<ol> <li>Reduza EPOCHS para 50-100</li> <li>Aumente BATCH_SIZE para 128</li> <li>Reduza arquitetura: [32, 16]</li> <li>Pule c\u00e9lulas de visualiza\u00e7\u00e3o</li> </ol>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#conceitos-aprendidos","title":"\ud83c\udf93 Conceitos Aprendidos","text":"<p>Ao final deste projeto, voc\u00ea ter\u00e1 implementado:</p> <ul> <li>\u2705 Forward propagation</li> <li>\u2705 Backpropagation</li> <li>\u2705 Gradient descent (mini-batch)</li> <li>\u2705 He initialization</li> <li>\u2705 ReLU activation</li> <li>\u2705 L2 regularization</li> <li>\u2705 Early stopping</li> <li>\u2705 Feature engineering (sin/cos)</li> <li>\u2705 M\u00e9tricas de regress\u00e3o</li> <li>\u2705 An\u00e1lise de res\u00edduos</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#suporte","title":"\ud83d\udcde Suporte","text":""},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#documentacao-completa","title":"Documenta\u00e7\u00e3o Completa","text":"<p>Leia README.md para documenta\u00e7\u00e3o detalhada.</p>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#material-de-apoio","title":"Material de Apoio","text":"<ul> <li>https://caioboa.github.io/DeepLearningPages/</li> <li>https://insper.github.io/ann-dl/versions/2025.2/projects/regression/</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#duvidas-comuns","title":"D\u00favidas Comuns","text":"<p>Consulte a se\u00e7\u00e3o Discuss\u00e3o e An\u00e1lise (c\u00e9lula 44) do notebook.</p>"},{"location":"portfolio/neural-networks/projects/2/QUICKSTART/#proximos-passos","title":"\ud83c\udf89 Pr\u00f3ximos Passos","text":"<p>Ap\u00f3s executar com sucesso:</p> <ol> <li>An\u00e1lise: Interprete os resultados nas c\u00e9lulas 40-45</li> <li>Experimenta\u00e7\u00e3o: Modifique hiperpar\u00e2metros e re-treine</li> <li>Compara\u00e7\u00e3o: Compare com baseline (c\u00e9lula 32)</li> <li>Melhorias: Implemente sugest\u00f5es da se\u00e7\u00e3o \"Melhorias Futuras\"</li> <li>Bonus: Submeta para competi\u00e7\u00e3o Kaggle (+0.5 pts)</li> </ol> <p>Boa sorte! \ud83d\ude80</p> <p>Se encontrar problemas, revise o README ou consulte o material do curso.</p>"},{"location":"portfolio/neural-networks/projects/2/add_conclusion/","title":"Add conclusion","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript final para adicionar visualiza\u00e7\u00f5es, an\u00e1lise de res\u00edduos, conclus\u00e3o e refer\u00eancias.\nExecute AP\u00d3S add_final_sections.py\n\"\"\"\n</pre> \"\"\" Script final para adicionar visualiza\u00e7\u00f5es, an\u00e1lise de res\u00edduos, conclus\u00e3o e refer\u00eancias. Execute AP\u00d3S add_final_sections.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre>notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n</pre> with open(notebook_path, 'r', encoding='utf-8') as f:     notebook = json.load(f) In\u00a0[\u00a0]: Copied! <pre>def add_markdown_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_markdown_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_code_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\")\n</pre> print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 1. Scatter: Predito vs Real (Test Set)\naxes[0, 0].scatter(y_test_true, y_test_pred, alpha=0.4, s=20)\nmin_val = min(y_test_true.min(), y_test_pred.min())\nmax_val = max(y_test_true.max(), y_test_pred.max())\naxes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predi\u00e7\u00e3o Perfeita')\naxes[0, 0].set_xlabel('Valor Real', fontsize=12)\naxes[0, 0].set_ylabel('Valor Predito', fontsize=12)\naxes[0, 0].set_title(f'Predi\u00e7\u00f5es vs Valores Reais (Test Set)\\\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}',\n                      fontsize=13, fontweight='bold')\naxes[0, 0].legend(fontsize=10)\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Res\u00edduos\nresiduals = y_test_true.flatten() - y_test_pred.flatten()\naxes[0, 1].scatter(y_test_pred, residuals, alpha=0.4, s=20)\naxes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[0, 1].set_xlabel('Valor Predito', fontsize=12)\naxes[0, 1].set_ylabel('Res\u00edduo (Real - Predito)', fontsize=12)\naxes[0, 1].set_title('Gr\u00e1fico de Res\u00edduos', fontsize=13, fontweight='bold')\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Distribui\u00e7\u00e3o dos Res\u00edduos\naxes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\naxes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero')\naxes[1, 0].axvline(x=residuals.mean(), color='green', linestyle='--', linewidth=2,\n                    label=f'M\u00e9dia: {residuals.mean():.2f}')\naxes[1, 0].set_xlabel('Res\u00edduo', fontsize=12)\naxes[1, 0].set_ylabel('Frequ\u00eancia', fontsize=12)\naxes[1, 0].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos', fontsize=13, fontweight='bold')\naxes[1, 0].legend(fontsize=10)\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Compara\u00e7\u00e3o de m\u00e9tricas entre sets\nmetrics_names = ['MAE', 'RMSE']\ntrain_vals = [train_metrics['MAE'], train_metrics['RMSE']]\nval_vals = [val_metrics['MAE'], val_metrics['RMSE']]\ntest_vals = [test_metrics['MAE'], test_metrics['RMSE']]\n\nx = np.arange(len(metrics_names))\nwidth = 0.25\n\naxes[1, 1].bar(x - width, train_vals, width, label='Train', color='steelblue')\naxes[1, 1].bar(x, val_vals, width, label='Validation', color='coral')\naxes[1, 1].bar(x + width, test_vals, width, label='Test', color='mediumseagreen')\n\naxes[1, 1].set_ylabel('Erro', fontsize=12)\naxes[1, 1].set_title('Compara\u00e7\u00e3o de M\u00e9tricas: Train vs Val vs Test', fontsize=13, fontweight='bold')\naxes[1, 1].set_xticks(x)\naxes[1, 1].set_xticklabels(metrics_names)\naxes[1, 1].legend(fontsize=10)\naxes[1, 1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\"\"\")\n</pre> add_code_cell(\"\"\"fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # 1. Scatter: Predito vs Real (Test Set) axes[0, 0].scatter(y_test_true, y_test_pred, alpha=0.4, s=20) min_val = min(y_test_true.min(), y_test_pred.min()) max_val = max(y_test_true.max(), y_test_pred.max()) axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predi\u00e7\u00e3o Perfeita') axes[0, 0].set_xlabel('Valor Real', fontsize=12) axes[0, 0].set_ylabel('Valor Predito', fontsize=12) axes[0, 0].set_title(f'Predi\u00e7\u00f5es vs Valores Reais (Test Set)\\\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}',                       fontsize=13, fontweight='bold') axes[0, 0].legend(fontsize=10) axes[0, 0].grid(True, alpha=0.3)  # 2. Res\u00edduos residuals = y_test_true.flatten() - y_test_pred.flatten() axes[0, 1].scatter(y_test_pred, residuals, alpha=0.4, s=20) axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2) axes[0, 1].set_xlabel('Valor Predito', fontsize=12) axes[0, 1].set_ylabel('Res\u00edduo (Real - Predito)', fontsize=12) axes[0, 1].set_title('Gr\u00e1fico de Res\u00edduos', fontsize=13, fontweight='bold') axes[0, 1].grid(True, alpha=0.3)  # 3. Distribui\u00e7\u00e3o dos Res\u00edduos axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='skyblue') axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero') axes[1, 0].axvline(x=residuals.mean(), color='green', linestyle='--', linewidth=2,                     label=f'M\u00e9dia: {residuals.mean():.2f}') axes[1, 0].set_xlabel('Res\u00edduo', fontsize=12) axes[1, 0].set_ylabel('Frequ\u00eancia', fontsize=12) axes[1, 0].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos', fontsize=13, fontweight='bold') axes[1, 0].legend(fontsize=10) axes[1, 0].grid(True, alpha=0.3)  # 4. Compara\u00e7\u00e3o de m\u00e9tricas entre sets metrics_names = ['MAE', 'RMSE'] train_vals = [train_metrics['MAE'], train_metrics['RMSE']] val_vals = [val_metrics['MAE'], val_metrics['RMSE']] test_vals = [test_metrics['MAE'], test_metrics['RMSE']]  x = np.arange(len(metrics_names)) width = 0.25  axes[1, 1].bar(x - width, train_vals, width, label='Train', color='steelblue') axes[1, 1].bar(x, val_vals, width, label='Validation', color='coral') axes[1, 1].bar(x + width, test_vals, width, label='Test', color='mediumseagreen')  axes[1, 1].set_ylabel('Erro', fontsize=12) axes[1, 1].set_title('Compara\u00e7\u00e3o de M\u00e9tricas: Train vs Val vs Test', fontsize=13, fontweight='bold') axes[1, 1].set_xticks(x) axes[1, 1].set_xticklabels(metrics_names) axes[1, 1].legend(fontsize=10) axes[1, 1].grid(True, alpha=0.3, axis='y')  plt.tight_layout() plt.show()\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### An\u00e1lise dos Res\u00edduos\n\n**Interpreta\u00e7\u00e3o esperada:**\n\n1. **Gr\u00e1fico de Res\u00edduos (canto superior direito):**\n   - **Ideal:** Pontos distribu\u00eddos aleatoriamente ao redor de zero\n   - **Padr\u00e3o detectado:** Indicaria modelo inadequado\n   - **Funil:** Heterocedasticidade (vari\u00e2ncia n\u00e3o constante)\n\n2. **Distribui\u00e7\u00e3o dos Res\u00edduos (canto inferior esquerdo):**\n   - **Ideal:** Distribui\u00e7\u00e3o normal centrada em zero\n   - **Assimetria:** Modelo sub/superestima sistematicamente\n   - **Caudas pesadas:** Presen\u00e7a de outliers\"\"\")\n</pre> add_markdown_cell(\"\"\"### An\u00e1lise dos Res\u00edduos  **Interpreta\u00e7\u00e3o esperada:**  1. **Gr\u00e1fico de Res\u00edduos (canto superior direito):**    - **Ideal:** Pontos distribu\u00eddos aleatoriamente ao redor de zero    - **Padr\u00e3o detectado:** Indicaria modelo inadequado    - **Funil:** Heterocedasticidade (vari\u00e2ncia n\u00e3o constante)  2. **Distribui\u00e7\u00e3o dos Res\u00edduos (canto inferior esquerdo):**    - **Ideal:** Distribui\u00e7\u00e3o normal centrada em zero    - **Assimetria:** Modelo sub/superestima sistematicamente    - **Caudas pesadas:** Presen\u00e7a de outliers\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# An\u00e1lise estat\u00edstica dos res\u00edduos\nprint(\"AN\u00c1LISE DOS RES\u00cdDUOS\")\nprint(\"=\"*80)\nprint(f\"M\u00e9dia dos res\u00edduos: {residuals.mean():.4f} (deve estar pr\u00f3ximo de 0)\")\nprint(f\"Desvio padr\u00e3o dos res\u00edduos: {residuals.std():.2f}\")\nprint(f\"Res\u00edduo m\u00ednimo: {residuals.min():.2f}\")\nprint(f\"Res\u00edduo m\u00e1ximo: {residuals.max():.2f}\")\nprint(f\"Mediana dos res\u00edduos: {np.median(residuals):.4f}\")\n\nprint(f\"\\\\nPercentis dos res\u00edduos absolutos:\")\nabs_residuals = np.abs(residuals)\nprint(f\"  25%: {np.percentile(abs_residuals, 25):.2f}\")\nprint(f\"  50%: {np.percentile(abs_residuals, 50):.2f} (mediana)\")\nprint(f\"  75%: {np.percentile(abs_residuals, 75):.2f}\")\nprint(f\"  95%: {np.percentile(abs_residuals, 95):.2f}\")\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(f\"Em 50% dos casos, o erro absoluto \u00e9 menor que {np.percentile(abs_residuals, 50):.2f} alugu\u00e9is\")\nprint(f\"Em 95% dos casos, o erro absoluto \u00e9 menor que {np.percentile(abs_residuals, 95):.2f} alugu\u00e9is\")\"\"\")\n</pre> add_code_cell(\"\"\"# An\u00e1lise estat\u00edstica dos res\u00edduos print(\"AN\u00c1LISE DOS RES\u00cdDUOS\") print(\"=\"*80) print(f\"M\u00e9dia dos res\u00edduos: {residuals.mean():.4f} (deve estar pr\u00f3ximo de 0)\") print(f\"Desvio padr\u00e3o dos res\u00edduos: {residuals.std():.2f}\") print(f\"Res\u00edduo m\u00ednimo: {residuals.min():.2f}\") print(f\"Res\u00edduo m\u00e1ximo: {residuals.max():.2f}\") print(f\"Mediana dos res\u00edduos: {np.median(residuals):.4f}\")  print(f\"\\\\nPercentis dos res\u00edduos absolutos:\") abs_residuals = np.abs(residuals) print(f\"  25%: {np.percentile(abs_residuals, 25):.2f}\") print(f\"  50%: {np.percentile(abs_residuals, 50):.2f} (mediana)\") print(f\"  75%: {np.percentile(abs_residuals, 75):.2f}\") print(f\"  95%: {np.percentile(abs_residuals, 95):.2f}\")  print(\"\\\\n\" + \"=\"*80) print(f\"Em 50% dos casos, o erro absoluto \u00e9 menor que {np.percentile(abs_residuals, 50):.2f} alugu\u00e9is\") print(f\"Em 95% dos casos, o erro absoluto \u00e9 menor que {np.percentile(abs_residuals, 95):.2f} alugu\u00e9is\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Exemplos de Predi\u00e7\u00f5es\"\"\")\n</pre> add_markdown_cell(\"\"\"### Exemplos de Predi\u00e7\u00f5es\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Mostrar alguns exemplos de predi\u00e7\u00f5es\nprint(\"EXEMPLOS DE PREDI\u00c7\u00d5ES (Test Set)\")\nprint(\"=\"*80)\n\n# Selecionar 10 amostras aleat\u00f3rias\nnp.random.seed(42)\nsample_indices = np.random.choice(len(y_test_true), size=10, replace=False)\n\nexamples_df = pd.DataFrame({\n    '\u00cdndice': sample_indices,\n    'Real': y_test_true.flatten()[sample_indices].astype(int),\n    'Predito': y_test_pred.flatten()[sample_indices].astype(int),\n    'Erro': (y_test_true.flatten()[sample_indices] - y_test_pred.flatten()[sample_indices]).astype(int),\n    'Erro %': ((y_test_true.flatten()[sample_indices] - y_test_pred.flatten()[sample_indices]) /\n               y_test_true.flatten()[sample_indices] * 100).round(1)\n})\n\nprint(examples_df.to_string(index=False))\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"Nota: Valores em n\u00famero de alugu\u00e9is de bicicletas\")\"\"\")\n</pre> add_code_cell(\"\"\"# Mostrar alguns exemplos de predi\u00e7\u00f5es print(\"EXEMPLOS DE PREDI\u00c7\u00d5ES (Test Set)\") print(\"=\"*80)  # Selecionar 10 amostras aleat\u00f3rias np.random.seed(42) sample_indices = np.random.choice(len(y_test_true), size=10, replace=False)  examples_df = pd.DataFrame({     '\u00cdndice': sample_indices,     'Real': y_test_true.flatten()[sample_indices].astype(int),     'Predito': y_test_pred.flatten()[sample_indices].astype(int),     'Erro': (y_test_true.flatten()[sample_indices] - y_test_pred.flatten()[sample_indices]).astype(int),     'Erro %': ((y_test_true.flatten()[sample_indices] - y_test_pred.flatten()[sample_indices]) /                y_test_true.flatten()[sample_indices] * 100).round(1) })  print(examples_df.to_string(index=False))  print(\"\\\\n\" + \"=\"*80) print(\"Nota: Valores em n\u00famero de alugu\u00e9is de bicicletas\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 11. Discuss\u00e3o e An\u00e1lise\n\n### Pontos Fortes do Modelo:\n\n1. **Implementa\u00e7\u00e3o Completa do Zero:**\n   - MLP implementado apenas com NumPy\n   - Forward e backward propagation totalmente customizados\n   - Controle total sobre arquitetura e hiperpar\u00e2metros\n\n2. **T\u00e9cnicas de Regulariza\u00e7\u00e3o:**\n   - L2 regularization para evitar overfitting\n   - Early stopping baseado em validation set\n   - Mini-batch GD para estabilidade\n\n3. **Feature Engineering:**\n   - Transforma\u00e7\u00e3o c\u00edclica de vari\u00e1veis temporais (sin/cos)\n   - Captura a natureza peri\u00f3dica de hora/m\u00eas/dia da semana\n   - Normaliza\u00e7\u00e3o adequada de features\n\n4. **Processo Sistem\u00e1tico:**\n   - An\u00e1lise explorat\u00f3ria detalhada\n   - Divis\u00e3o adequada em train/val/test\n   - M\u00faltiplas m\u00e9tricas de avalia\u00e7\u00e3o\n\n### Limita\u00e7\u00f5es:\n\n1. **MLP vs Modelos Mais Avan\u00e7ados:**\n   - MLP n\u00e3o captura depend\u00eancias temporais (sem mem\u00f3ria de estados anteriores)\n   - **Alternativas:** RNN, LSTM, GRU para s\u00e9ries temporais\n   - **\u00c1rvores de decis\u00e3o** (Random Forest, XGBoost) podem ter melhor performance em tabular data\n\n2. **Variabilidade N\u00e3o Explicada:**\n   - Features dispon\u00edveis podem n\u00e3o capturar todos os fatores (eventos especiais, constru\u00e7\u00f5es, etc.)\n   - Ru\u00eddo inerente ao comportamento humano\n\n3. **Sensibilidade a Hiperpar\u00e2metros:**\n   - Performance depende de escolhas de learning rate, arquitetura, etc.\n   - **Melhoria:** Grid search ou random search para otimiza\u00e7\u00e3o\n\n4. **Escalabilidade:**\n   - Implementa\u00e7\u00e3o do zero \u00e9 educacional mas n\u00e3o otimizada para produ\u00e7\u00e3o\n   - Bibliotecas como PyTorch/TensorFlow usam GPU e s\u00e3o muito mais r\u00e1pidas\n\n### Melhorias Futuras:\n\n1. **Arquitetura:**\n   - Testar diferentes n\u00fameros de camadas e neur\u00f4nios\n   - Adicionar Dropout para regulariza\u00e7\u00e3o adicional\n   - Batch Normalization para converg\u00eancia mais r\u00e1pida\n\n2. **Otimiza\u00e7\u00e3o:**\n   - Implementar Adam optimizer (learning rate adaptativo)\n   - Learning rate scheduling (reduzir lr ao longo do treinamento)\n   - Gradient clipping para estabilidade\n\n3. **Features:**\n   - Adicionar lags temporais (demanda das horas anteriores)\n   - Criar features de intera\u00e7\u00e3o (ex: temperatura \u00d7 hora do dia)\n   - Engenharia de features baseada em domain knowledge\n\n4. **Ensemble:**\n   - Combinar MLP com outros modelos (ex: MLP + XGBoost)\n   - Stacking ou blending para melhor generaliza\u00e7\u00e3o\n\n5. **Deployment:**\n   - Migrar para framework de produ\u00e7\u00e3o (TensorFlow Serving, FastAPI)\n   - Monitoramento de performance em produ\u00e7\u00e3o\n   - A/B testing de diferentes vers\u00f5es\"\"\")\n</pre> add_markdown_cell(\"\"\"## 11. Discuss\u00e3o e An\u00e1lise  ### Pontos Fortes do Modelo:  1. **Implementa\u00e7\u00e3o Completa do Zero:**    - MLP implementado apenas com NumPy    - Forward e backward propagation totalmente customizados    - Controle total sobre arquitetura e hiperpar\u00e2metros  2. **T\u00e9cnicas de Regulariza\u00e7\u00e3o:**    - L2 regularization para evitar overfitting    - Early stopping baseado em validation set    - Mini-batch GD para estabilidade  3. **Feature Engineering:**    - Transforma\u00e7\u00e3o c\u00edclica de vari\u00e1veis temporais (sin/cos)    - Captura a natureza peri\u00f3dica de hora/m\u00eas/dia da semana    - Normaliza\u00e7\u00e3o adequada de features  4. **Processo Sistem\u00e1tico:**    - An\u00e1lise explorat\u00f3ria detalhada    - Divis\u00e3o adequada em train/val/test    - M\u00faltiplas m\u00e9tricas de avalia\u00e7\u00e3o  ### Limita\u00e7\u00f5es:  1. **MLP vs Modelos Mais Avan\u00e7ados:**    - MLP n\u00e3o captura depend\u00eancias temporais (sem mem\u00f3ria de estados anteriores)    - **Alternativas:** RNN, LSTM, GRU para s\u00e9ries temporais    - **\u00c1rvores de decis\u00e3o** (Random Forest, XGBoost) podem ter melhor performance em tabular data  2. **Variabilidade N\u00e3o Explicada:**    - Features dispon\u00edveis podem n\u00e3o capturar todos os fatores (eventos especiais, constru\u00e7\u00f5es, etc.)    - Ru\u00eddo inerente ao comportamento humano  3. **Sensibilidade a Hiperpar\u00e2metros:**    - Performance depende de escolhas de learning rate, arquitetura, etc.    - **Melhoria:** Grid search ou random search para otimiza\u00e7\u00e3o  4. **Escalabilidade:**    - Implementa\u00e7\u00e3o do zero \u00e9 educacional mas n\u00e3o otimizada para produ\u00e7\u00e3o    - Bibliotecas como PyTorch/TensorFlow usam GPU e s\u00e3o muito mais r\u00e1pidas  ### Melhorias Futuras:  1. **Arquitetura:**    - Testar diferentes n\u00fameros de camadas e neur\u00f4nios    - Adicionar Dropout para regulariza\u00e7\u00e3o adicional    - Batch Normalization para converg\u00eancia mais r\u00e1pida  2. **Otimiza\u00e7\u00e3o:**    - Implementar Adam optimizer (learning rate adaptativo)    - Learning rate scheduling (reduzir lr ao longo do treinamento)    - Gradient clipping para estabilidade  3. **Features:**    - Adicionar lags temporais (demanda das horas anteriores)    - Criar features de intera\u00e7\u00e3o (ex: temperatura \u00d7 hora do dia)    - Engenharia de features baseada em domain knowledge  4. **Ensemble:**    - Combinar MLP com outros modelos (ex: MLP + XGBoost)    - Stacking ou blending para melhor generaliza\u00e7\u00e3o  5. **Deployment:**    - Migrar para framework de produ\u00e7\u00e3o (TensorFlow Serving, FastAPI)    - Monitoramento de performance em produ\u00e7\u00e3o    - A/B testing de diferentes vers\u00f5es\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 12. Conclus\u00e3o\n\n### Resumo do Projeto:\n\nNeste projeto, implementamos um **Multi-Layer Perceptron (MLP) do zero** para resolver um problema de regress\u00e3o real: prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.\n\n### Resultados Principais:\n\n- **Dataset:** 17,379 amostras do sistema de bike-sharing de Washington D.C.\n- **Arquitetura:** MLP com 3 camadas ocultas [64, 32, 16 neur\u00f4nios]\n- **M\u00e9tricas (Test Set):** *(valores ser\u00e3o preenchidos ap\u00f3s execu\u00e7\u00e3o)*\n  - R\u00b2: [ser\u00e1 calculado]\n  - RMSE: [ser\u00e1 calculado]\n  - MAE: [ser\u00e1 calculado]\n\n### Aprendizados:\n\n1. **Implementa\u00e7\u00e3o de Redes Neurais:**\n   - Compreens\u00e3o profunda de forward/backward propagation\n   - Import\u00e2ncia da inicializa\u00e7\u00e3o de pesos (He initialization)\n   - Papel crucial da normaliza\u00e7\u00e3o de dados\n\n2. **T\u00e9cnicas de Treinamento:**\n   - Mini-batch GD oferece bom equil\u00edbrio entre velocidade e estabilidade\n   - Early stopping \u00e9 essencial para evitar overfitting\n   - Regulariza\u00e7\u00e3o L2 ajuda na generaliza\u00e7\u00e3o\n\n3. **Feature Engineering:**\n   - Transforma\u00e7\u00f5es c\u00edclicas (sin/cos) s\u00e3o fundamentais para vari\u00e1veis temporais\n   - Import\u00e2ncia de entender o dom\u00ednio do problema\n\n4. **Avalia\u00e7\u00e3o:**\n   - M\u00faltiplas m\u00e9tricas fornecem vis\u00e3o completa da performance\n   - An\u00e1lise de res\u00edduos revela padr\u00f5es n\u00e3o capturados\n   - Compara\u00e7\u00e3o com baseline valida utilidade do modelo\n\n### Limita\u00e7\u00f5es Reconhecidas:\n\n- MLP n\u00e3o \u00e9 ideal para s\u00e9ries temporais (sem mem\u00f3ria de estados anteriores)\n- Variabilidade humana inerente limita R\u00b2 m\u00e1ximo alcan\u00e7\u00e1vel\n- Implementa\u00e7\u00e3o do zero \u00e9 educacional mas n\u00e3o otimizada para produ\u00e7\u00e3o\n\n### Pr\u00f3ximos Passos:\n\nPara melhorar ainda mais o modelo:\n1. Testar arquiteturas mais complexas (LSTM, GRU)\n2. Implementar otimizadores mais sofisticados (Adam)\n3. Criar features de lags temporais\n4. Ensemble com outros modelos (Random Forest, XGBoost)\n\n### Conclus\u00e3o Final:\n\nEste projeto demonstrou com sucesso a aplica\u00e7\u00e3o de redes neurais MLP em um problema de regress\u00e3o real-world. A implementa\u00e7\u00e3o do zero proporcionou compreens\u00e3o profunda dos mecanismos internos de redes neurais, enquanto as t\u00e9cnicas de regulariza\u00e7\u00e3o e avalia\u00e7\u00e3o garantiram um modelo robusto e bem validado.\n\n**O conhecimento adquirido serve como base s\u00f3lida para explorar arquiteturas mais avan\u00e7adas e problemas mais complexos em deep learning.**\"\"\")\n</pre> add_markdown_cell(\"\"\"## 12. Conclus\u00e3o  ### Resumo do Projeto:  Neste projeto, implementamos um **Multi-Layer Perceptron (MLP) do zero** para resolver um problema de regress\u00e3o real: prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.  ### Resultados Principais:  - **Dataset:** 17,379 amostras do sistema de bike-sharing de Washington D.C. - **Arquitetura:** MLP com 3 camadas ocultas [64, 32, 16 neur\u00f4nios] - **M\u00e9tricas (Test Set):** *(valores ser\u00e3o preenchidos ap\u00f3s execu\u00e7\u00e3o)*   - R\u00b2: [ser\u00e1 calculado]   - RMSE: [ser\u00e1 calculado]   - MAE: [ser\u00e1 calculado]  ### Aprendizados:  1. **Implementa\u00e7\u00e3o de Redes Neurais:**    - Compreens\u00e3o profunda de forward/backward propagation    - Import\u00e2ncia da inicializa\u00e7\u00e3o de pesos (He initialization)    - Papel crucial da normaliza\u00e7\u00e3o de dados  2. **T\u00e9cnicas de Treinamento:**    - Mini-batch GD oferece bom equil\u00edbrio entre velocidade e estabilidade    - Early stopping \u00e9 essencial para evitar overfitting    - Regulariza\u00e7\u00e3o L2 ajuda na generaliza\u00e7\u00e3o  3. **Feature Engineering:**    - Transforma\u00e7\u00f5es c\u00edclicas (sin/cos) s\u00e3o fundamentais para vari\u00e1veis temporais    - Import\u00e2ncia de entender o dom\u00ednio do problema  4. **Avalia\u00e7\u00e3o:**    - M\u00faltiplas m\u00e9tricas fornecem vis\u00e3o completa da performance    - An\u00e1lise de res\u00edduos revela padr\u00f5es n\u00e3o capturados    - Compara\u00e7\u00e3o com baseline valida utilidade do modelo  ### Limita\u00e7\u00f5es Reconhecidas:  - MLP n\u00e3o \u00e9 ideal para s\u00e9ries temporais (sem mem\u00f3ria de estados anteriores) - Variabilidade humana inerente limita R\u00b2 m\u00e1ximo alcan\u00e7\u00e1vel - Implementa\u00e7\u00e3o do zero \u00e9 educacional mas n\u00e3o otimizada para produ\u00e7\u00e3o  ### Pr\u00f3ximos Passos:  Para melhorar ainda mais o modelo: 1. Testar arquiteturas mais complexas (LSTM, GRU) 2. Implementar otimizadores mais sofisticados (Adam) 3. Criar features de lags temporais 4. Ensemble com outros modelos (Random Forest, XGBoost)  ### Conclus\u00e3o Final:  Este projeto demonstrou com sucesso a aplica\u00e7\u00e3o de redes neurais MLP em um problema de regress\u00e3o real-world. A implementa\u00e7\u00e3o do zero proporcionou compreens\u00e3o profunda dos mecanismos internos de redes neurais, enquanto as t\u00e9cnicas de regulariza\u00e7\u00e3o e avalia\u00e7\u00e3o garantiram um modelo robusto e bem validado.  **O conhecimento adquirido serve como base s\u00f3lida para explorar arquiteturas mais avan\u00e7adas e problemas mais complexos em deep learning.**\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 13. Refer\u00eancias\n\n### Dataset:\n\n1. **Fanaee-T, Hadi, and Gama, Joao** (2013). \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence, pp. 1-15, Springer Berlin Heidelberg.\n   - Dataset original: [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)\n\n2. **Kaggle Competition** - Bike Sharing Demand:\n   - https://www.kaggle.com/c/bike-sharing-demand\n\n### Fundamentos de Redes Neurais:\n\n3. **Goodfellow, Ian, Yoshua Bengio, and Aaron Courville** (2016). \"Deep Learning\". MIT Press.\n   - Cap\u00edtulos 6-8: Feedforward networks, regularization, optimization\n\n4. **Nielsen, Michael A.** (2015). \"Neural Networks and Deep Learning\".\n   - http://neuralnetworksanddeeplearning.com/\n\n### T\u00e9cnicas de Otimiza\u00e7\u00e3o:\n\n5. **He, Kaiming, et al.** (2015). \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\". ICCV 2015.\n   - He initialization\n\n6. **Srivastava, Nitish, et al.** (2014). \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\". JMLR 15.\n\n### Material do Curso:\n\n7. **Material de Apoio:**\n   - https://caioboa.github.io/DeepLearningPages/\n   - https://insper.github.io/ann-dl/versions/2025.2/projects/regression/\n\n### Ferramentas e Bibliotecas:\n\n8. **NumPy Documentation** - https://numpy.org/doc/\n9. **Pandas Documentation** - https://pandas.pydata.org/docs/\n10. **Matplotlib Documentation** - https://matplotlib.org/stable/contents.html\n11. **Seaborn Documentation** - https://seaborn.pydata.org/\n\n---\n\n**Ferramentas de IA utilizadas:**\n- Claude Code (Anthropic) - Assist\u00eancia na estrutura\u00e7\u00e3o do c\u00f3digo e documenta\u00e7\u00e3o\n- Todas as implementa\u00e7\u00f5es foram compreendidas e validadas manualmente\n\n---\n\n**Data de conclus\u00e3o:** [Data ser\u00e1 preenchida ap\u00f3s execu\u00e7\u00e3o]\n**Vers\u00e3o do notebook:** 1.0\"\"\")\n</pre> add_markdown_cell(\"\"\"## 13. Refer\u00eancias  ### Dataset:  1. **Fanaee-T, Hadi, and Gama, Joao** (2013). \"Event labeling combining ensemble detectors and background knowledge\", Progress in Artificial Intelligence, pp. 1-15, Springer Berlin Heidelberg.    - Dataset original: [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)  2. **Kaggle Competition** - Bike Sharing Demand:    - https://www.kaggle.com/c/bike-sharing-demand  ### Fundamentos de Redes Neurais:  3. **Goodfellow, Ian, Yoshua Bengio, and Aaron Courville** (2016). \"Deep Learning\". MIT Press.    - Cap\u00edtulos 6-8: Feedforward networks, regularization, optimization  4. **Nielsen, Michael A.** (2015). \"Neural Networks and Deep Learning\".    - http://neuralnetworksanddeeplearning.com/  ### T\u00e9cnicas de Otimiza\u00e7\u00e3o:  5. **He, Kaiming, et al.** (2015). \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\". ICCV 2015.    - He initialization  6. **Srivastava, Nitish, et al.** (2014). \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\". JMLR 15.  ### Material do Curso:  7. **Material de Apoio:**    - https://caioboa.github.io/DeepLearningPages/    - https://insper.github.io/ann-dl/versions/2025.2/projects/regression/  ### Ferramentas e Bibliotecas:  8. **NumPy Documentation** - https://numpy.org/doc/ 9. **Pandas Documentation** - https://pandas.pydata.org/docs/ 10. **Matplotlib Documentation** - https://matplotlib.org/stable/contents.html 11. **Seaborn Documentation** - https://seaborn.pydata.org/  ---  **Ferramentas de IA utilizadas:** - Claude Code (Anthropic) - Assist\u00eancia na estrutura\u00e7\u00e3o do c\u00f3digo e documenta\u00e7\u00e3o - Todas as implementa\u00e7\u00f5es foram compreendidas e validadas manualmente  ---  **Data de conclus\u00e3o:** [Data ser\u00e1 preenchida ap\u00f3s execu\u00e7\u00e3o] **Vers\u00e3o do notebook:** 1.0\"\"\") In\u00a0[\u00a0]: Copied! <pre># Salvar notebook final\nwith open(notebook_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> # Salvar notebook final with open(notebook_path, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"\u2713 NOTEBOOK COMPLETO!\")\nprint(f\"\u2713 Total de c\u00e9lulas: {len(notebook['cells'])}\")\nprint(f\"\u2713 Arquivo: {notebook_path}\")\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"ESTRUTURA DO PROJETO:\")\nprint(\"=\"*80)\nprint(\"1. Introdu\u00e7\u00e3o e Objetivo\")\nprint(\"2. Sele\u00e7\u00e3o do Dataset (Bike Sharing)\")\nprint(\"3. Importa\u00e7\u00e3o de Bibliotecas\")\nprint(\"4. Carregamento e Explora\u00e7\u00e3o dos Dados\")\nprint(\"5. Explica\u00e7\u00e3o Detalhada do Dataset\")\nprint(\"6. An\u00e1lise de Valores Ausentes e Outliers\")\nprint(\"7. Visualiza\u00e7\u00f5es Explorat\u00f3rias\")\nprint(\"8. An\u00e1lise Temporal e Sazonal\")\nprint(\"9. Limpeza e Normaliza\u00e7\u00e3o dos Dados\")\nprint(\"10. Feature Engineering (vari\u00e1veis c\u00edclicas)\")\nprint(\"11. Divis\u00e3o Train/Validation/Test\")\nprint(\"12. Implementa\u00e7\u00e3o do MLP (do zero com NumPy)\")\nprint(\"13. Treinamento do Modelo\")\nprint(\"14. Curvas de Erro e An\u00e1lise de Overfitting\")\nprint(\"15. M\u00e9tricas de Avalia\u00e7\u00e3o (MAE, MSE, RMSE, R\u00b2, MAPE)\")\nprint(\"16. Compara\u00e7\u00e3o com Baseline\")\nprint(\"17. Visualiza\u00e7\u00f5es de Avalia\u00e7\u00e3o\")\nprint(\"18. An\u00e1lise de Res\u00edduos\")\nprint(\"19. Exemplos de Predi\u00e7\u00f5es\")\nprint(\"20. Discuss\u00e3o e An\u00e1lise\")\nprint(\"21. Conclus\u00e3o\")\nprint(\"22. Refer\u00eancias\")\nprint(\"=\"*80)\nprint(\"\\\\nPr\u00f3ximos passos:\")\nprint(\"1. Baixe o dataset 'hour.csv' do UCI Repository\")\nprint(\"2. Coloque o arquivo na pasta do notebook\")\nprint(\"3. Execute o notebook c\u00e9lula por c\u00e9lula\")\nprint(\"4. Analise os resultados e ajuste hiperpar\u00e2metros se necess\u00e1rio\")\n</pre> print(f\"\u2713 NOTEBOOK COMPLETO!\") print(f\"\u2713 Total de c\u00e9lulas: {len(notebook['cells'])}\") print(f\"\u2713 Arquivo: {notebook_path}\") print(\"\\\\n\" + \"=\"*80) print(\"ESTRUTURA DO PROJETO:\") print(\"=\"*80) print(\"1. Introdu\u00e7\u00e3o e Objetivo\") print(\"2. Sele\u00e7\u00e3o do Dataset (Bike Sharing)\") print(\"3. Importa\u00e7\u00e3o de Bibliotecas\") print(\"4. Carregamento e Explora\u00e7\u00e3o dos Dados\") print(\"5. Explica\u00e7\u00e3o Detalhada do Dataset\") print(\"6. An\u00e1lise de Valores Ausentes e Outliers\") print(\"7. Visualiza\u00e7\u00f5es Explorat\u00f3rias\") print(\"8. An\u00e1lise Temporal e Sazonal\") print(\"9. Limpeza e Normaliza\u00e7\u00e3o dos Dados\") print(\"10. Feature Engineering (vari\u00e1veis c\u00edclicas)\") print(\"11. Divis\u00e3o Train/Validation/Test\") print(\"12. Implementa\u00e7\u00e3o do MLP (do zero com NumPy)\") print(\"13. Treinamento do Modelo\") print(\"14. Curvas de Erro e An\u00e1lise de Overfitting\") print(\"15. M\u00e9tricas de Avalia\u00e7\u00e3o (MAE, MSE, RMSE, R\u00b2, MAPE)\") print(\"16. Compara\u00e7\u00e3o com Baseline\") print(\"17. Visualiza\u00e7\u00f5es de Avalia\u00e7\u00e3o\") print(\"18. An\u00e1lise de Res\u00edduos\") print(\"19. Exemplos de Predi\u00e7\u00f5es\") print(\"20. Discuss\u00e3o e An\u00e1lise\") print(\"21. Conclus\u00e3o\") print(\"22. Refer\u00eancias\") print(\"=\"*80) print(\"\\\\nPr\u00f3ximos passos:\") print(\"1. Baixe o dataset 'hour.csv' do UCI Repository\") print(\"2. Coloque o arquivo na pasta do notebook\") print(\"3. Execute o notebook c\u00e9lula por c\u00e9lula\") print(\"4. Analise os resultados e ajuste hiperpar\u00e2metros se necess\u00e1rio\")"},{"location":"portfolio/neural-networks/projects/2/add_conclusion/#visualizacoes-finais","title":"============================================================================== Visualiza\u00e7\u00f5es finais\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_conclusion/#secao-11-discussao-e-analise","title":"============================================================================== SE\u00c7\u00c3O 11: Discuss\u00e3o e An\u00e1lise\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_conclusion/#secao-12-conclusao","title":"============================================================================== SE\u00c7\u00c3O 12: Conclus\u00e3o\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_conclusion/#secao-13-referencias","title":"============================================================================== SE\u00c7\u00c3O 13: Refer\u00eancias\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_final_sections/","title":"Add final sections","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para adicionar se\u00e7\u00f5es finais: treinamento, avalia\u00e7\u00e3o, conclus\u00e3o e refer\u00eancias.\nExecute AP\u00d3S add_mlp_and_training.py\n\"\"\"\n</pre> \"\"\" Script para adicionar se\u00e7\u00f5es finais: treinamento, avalia\u00e7\u00e3o, conclus\u00e3o e refer\u00eancias. Execute AP\u00d3S add_mlp_and_training.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre># Carregar notebook\nnotebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> # Carregar notebook notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n</pre> with open(notebook_path, 'r', encoding='utf-8') as f:     notebook = json.load(f) In\u00a0[\u00a0]: Copied! <pre>def add_markdown_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_markdown_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_code_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\")\n</pre> print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 8. Treinamento do Modelo\n\n### Hiperpar\u00e2metros Escolhidos:\n\n- **Arquitetura:** [15 \u2192 64 \u2192 32 \u2192 16 \u2192 1]\n- **Learning rate:** 0.001\n- **Batch size:** 64\n- **\u00c9pocas:** 200 (m\u00e1ximo)\n- **Early stopping:** Paci\u00eancia de 15 \u00e9pocas\n- **Regulariza\u00e7\u00e3o L2:** \u03bb = 0.001\n\n### Justificativa:\n\n1. **Arquitetura progressiva:** Redu\u00e7\u00e3o gradual de neur\u00f4nios (64\u219232\u219216) ajuda a extrair features hier\u00e1rquicas\n2. **Learning rate:** Valor conservador para garantir converg\u00eancia est\u00e1vel\n3. **Early stopping:** Previne overfitting, para treinamento quando valida\u00e7\u00e3o para de melhorar\"\"\")\n</pre> add_markdown_cell(\"\"\"## 8. Treinamento do Modelo  ### Hiperpar\u00e2metros Escolhidos:  - **Arquitetura:** [15 \u2192 64 \u2192 32 \u2192 16 \u2192 1] - **Learning rate:** 0.001 - **Batch size:** 64 - **\u00c9pocas:** 200 (m\u00e1ximo) - **Early stopping:** Paci\u00eancia de 15 \u00e9pocas - **Regulariza\u00e7\u00e3o L2:** \u03bb = 0.001  ### Justificativa:  1. **Arquitetura progressiva:** Redu\u00e7\u00e3o gradual de neur\u00f4nios (64\u219232\u219216) ajuda a extrair features hier\u00e1rquicas 2. **Learning rate:** Valor conservador para garantir converg\u00eancia est\u00e1vel 3. **Early stopping:** Previne overfitting, para treinamento quando valida\u00e7\u00e3o para de melhorar\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"INICIALIZANDO E TREINANDO O MODELO\")\nprint(\"=\"*80)\n\n# Hiperpar\u00e2metros\nINPUT_SIZE = X_train.shape[1]  # 15 features\nHIDDEN_SIZES = [64, 32, 16]     # 3 camadas ocultas\nOUTPUT_SIZE = 1                 # 1 output (regress\u00e3o)\nLEARNING_RATE = 0.001\nREG_LAMBDA = 0.001\nEPOCHS = 200\nBATCH_SIZE = 64\nEARLY_STOPPING_PATIENCE = 15\n\nprint(f\"Arquitetura: {INPUT_SIZE} \u2192 {' \u2192 '.join(map(str, HIDDEN_SIZES))} \u2192 {OUTPUT_SIZE}\")\nprint(f\"Learning rate: {LEARNING_RATE}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"\u00c9pocas (m\u00e1x): {EPOCHS}\")\nprint(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\nprint(f\"Regulariza\u00e7\u00e3o L2 (\u03bb): {REG_LAMBDA}\")\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"INICIANDO TREINAMENTO...\")\nprint(\"=\"*80 + \"\\\\n\")\n\n# Criar modelo\nmodel = MLPRegressor(\n    input_size=INPUT_SIZE,\n    hidden_sizes=HIDDEN_SIZES,\n    output_size=OUTPUT_SIZE,\n    learning_rate=LEARNING_RATE,\n    reg_lambda=REG_LAMBDA,\n    random_seed=42\n)\n\n# Treinar\nmodel.fit(\n    X_train, y_train,\n    X_val, y_val,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n    verbose=True\n)\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"TREINAMENTO CONCLU\u00cdDO!\")\nprint(\"=\"*80)\"\"\")\n</pre> add_code_cell(\"\"\"print(\"INICIALIZANDO E TREINANDO O MODELO\") print(\"=\"*80)  # Hiperpar\u00e2metros INPUT_SIZE = X_train.shape[1]  # 15 features HIDDEN_SIZES = [64, 32, 16]     # 3 camadas ocultas OUTPUT_SIZE = 1                 # 1 output (regress\u00e3o) LEARNING_RATE = 0.001 REG_LAMBDA = 0.001 EPOCHS = 200 BATCH_SIZE = 64 EARLY_STOPPING_PATIENCE = 15  print(f\"Arquitetura: {INPUT_SIZE} \u2192 {' \u2192 '.join(map(str, HIDDEN_SIZES))} \u2192 {OUTPUT_SIZE}\") print(f\"Learning rate: {LEARNING_RATE}\") print(f\"Batch size: {BATCH_SIZE}\") print(f\"\u00c9pocas (m\u00e1x): {EPOCHS}\") print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\") print(f\"Regulariza\u00e7\u00e3o L2 (\u03bb): {REG_LAMBDA}\")  print(\"\\\\n\" + \"=\"*80) print(\"INICIANDO TREINAMENTO...\") print(\"=\"*80 + \"\\\\n\")  # Criar modelo model = MLPRegressor(     input_size=INPUT_SIZE,     hidden_sizes=HIDDEN_SIZES,     output_size=OUTPUT_SIZE,     learning_rate=LEARNING_RATE,     reg_lambda=REG_LAMBDA,     random_seed=42 )  # Treinar model.fit(     X_train, y_train,     X_val, y_val,     epochs=EPOCHS,     batch_size=BATCH_SIZE,     early_stopping_patience=EARLY_STOPPING_PATIENCE,     verbose=True )  print(\"\\\\n\" + \"=\"*80) print(\"TREINAMENTO CONCLU\u00cdDO!\") print(\"=\"*80)\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Desafios Durante o Treinamento:\n\n**Poss\u00edveis problemas e solu\u00e7\u00f5es adotadas:**\n\n1. **Vanishing Gradients:**\n   - **Problema:** Gradientes ficam muito pequenos em redes profundas\n   - **Solu\u00e7\u00e3o:** He initialization + ReLU activation\n\n2. **Overfitting:**\n   - **Problema:** Modelo memoriza training set ao inv\u00e9s de generalizar\n   - **Solu\u00e7\u00e3o:** L2 regularization + early stopping\n\n3. **Converg\u00eancia Lenta:**\n   - **Problema:** Learning rate muito baixo\n   - **Solu\u00e7\u00e3o:** Testamos diferentes valores (0.001 mostrou-se adequado)\n\n4. **Instabilidade:**\n   - **Problema:** Loss oscilando muito\n   - **Solu\u00e7\u00e3o:** Mini-batch GD (batch=64) ao inv\u00e9s de SGD puro\"\"\")\n</pre> add_markdown_cell(\"\"\"### Desafios Durante o Treinamento:  **Poss\u00edveis problemas e solu\u00e7\u00f5es adotadas:**  1. **Vanishing Gradients:**    - **Problema:** Gradientes ficam muito pequenos em redes profundas    - **Solu\u00e7\u00e3o:** He initialization + ReLU activation  2. **Overfitting:**    - **Problema:** Modelo memoriza training set ao inv\u00e9s de generalizar    - **Solu\u00e7\u00e3o:** L2 regularization + early stopping  3. **Converg\u00eancia Lenta:**    - **Problema:** Learning rate muito baixo    - **Solu\u00e7\u00e3o:** Testamos diferentes valores (0.001 mostrou-se adequado)  4. **Instabilidade:**    - **Problema:** Loss oscilando muito    - **Solu\u00e7\u00e3o:** Mini-batch GD (batch=64) ao inv\u00e9s de SGD puro\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 9. Curvas de Erro e Visualiza\u00e7\u00e3o\n\nAn\u00e1lise da converg\u00eancia do modelo atrav\u00e9s das curvas de loss ao longo das \u00e9pocas.\"\"\")\n</pre> add_markdown_cell(\"\"\"## 9. Curvas de Erro e Visualiza\u00e7\u00e3o  An\u00e1lise da converg\u00eancia do modelo atrav\u00e9s das curvas de loss ao longo das \u00e9pocas.\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Curvas de loss\nfig, axes = plt.subplots(1, 2, figsize=(16, 5))\n\nepochs_trained = len(model.train_loss_history)\nepochs_range = range(1, epochs_trained + 1)\n\n# Gr\u00e1fico 1: Loss ao longo das \u00e9pocas\naxes[0].plot(epochs_range, model.train_loss_history, label='Training Loss',\n             linewidth=2, color='steelblue')\naxes[0].plot(epochs_range, model.val_loss_history, label='Validation Loss',\n             linewidth=2, color='coral')\naxes[0].set_xlabel('\u00c9poca', fontsize=12)\naxes[0].set_ylabel('Loss (MSE + L2)', fontsize=12)\naxes[0].set_title('Curva de Converg\u00eancia: Training vs Validation Loss',\n                   fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11, loc='upper right')\naxes[0].grid(True, alpha=0.3)\n\n# Marcar melhor \u00e9poca\nbest_epoch = np.argmin(model.val_loss_history) + 1\nbest_val_loss = min(model.val_loss_history)\naxes[0].axvline(best_epoch, color='green', linestyle='--', linewidth=1.5,\n                label=f'Melhor \u00e9poca: {best_epoch}')\naxes[0].scatter([best_epoch], [best_val_loss], color='green', s=100, zorder=5)\naxes[0].legend(fontsize=11)\n\n# Gr\u00e1fico 2: Loss em escala log\naxes[1].plot(epochs_range, model.train_loss_history, label='Training Loss',\n             linewidth=2, color='steelblue')\naxes[1].plot(epochs_range, model.val_loss_history, label='Validation Loss',\n             linewidth=2, color='coral')\naxes[1].set_xlabel('\u00c9poca', fontsize=12)\naxes[1].set_ylabel('Loss (escala log)', fontsize=12)\naxes[1].set_title('Curva de Converg\u00eancia (Escala Logar\u00edtmica)',\n                   fontsize=14, fontweight='bold')\naxes[1].set_yscale('log')\naxes[1].legend(fontsize=11, loc='upper right')\naxes[1].grid(True, alpha=0.3, which='both')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estat\u00edsticas do Treinamento:\")\nprint(\"=\"*80)\nprint(f\"Total de \u00e9pocas treinadas: {epochs_trained}\")\nprint(f\"Melhor \u00e9poca: {best_epoch}\")\nprint(f\"Melhor validation loss: {best_val_loss:.4f}\")\nprint(f\"Training loss final: {model.train_loss_history[-1]:.4f}\")\nprint(f\"Validation loss final: {model.val_loss_history[-1]:.4f}\")\"\"\")\n</pre> add_code_cell(\"\"\"# Curvas de loss fig, axes = plt.subplots(1, 2, figsize=(16, 5))  epochs_trained = len(model.train_loss_history) epochs_range = range(1, epochs_trained + 1)  # Gr\u00e1fico 1: Loss ao longo das \u00e9pocas axes[0].plot(epochs_range, model.train_loss_history, label='Training Loss',              linewidth=2, color='steelblue') axes[0].plot(epochs_range, model.val_loss_history, label='Validation Loss',              linewidth=2, color='coral') axes[0].set_xlabel('\u00c9poca', fontsize=12) axes[0].set_ylabel('Loss (MSE + L2)', fontsize=12) axes[0].set_title('Curva de Converg\u00eancia: Training vs Validation Loss',                    fontsize=14, fontweight='bold') axes[0].legend(fontsize=11, loc='upper right') axes[0].grid(True, alpha=0.3)  # Marcar melhor \u00e9poca best_epoch = np.argmin(model.val_loss_history) + 1 best_val_loss = min(model.val_loss_history) axes[0].axvline(best_epoch, color='green', linestyle='--', linewidth=1.5,                 label=f'Melhor \u00e9poca: {best_epoch}') axes[0].scatter([best_epoch], [best_val_loss], color='green', s=100, zorder=5) axes[0].legend(fontsize=11)  # Gr\u00e1fico 2: Loss em escala log axes[1].plot(epochs_range, model.train_loss_history, label='Training Loss',              linewidth=2, color='steelblue') axes[1].plot(epochs_range, model.val_loss_history, label='Validation Loss',              linewidth=2, color='coral') axes[1].set_xlabel('\u00c9poca', fontsize=12) axes[1].set_ylabel('Loss (escala log)', fontsize=12) axes[1].set_title('Curva de Converg\u00eancia (Escala Logar\u00edtmica)',                    fontsize=14, fontweight='bold') axes[1].set_yscale('log') axes[1].legend(fontsize=11, loc='upper right') axes[1].grid(True, alpha=0.3, which='both')  plt.tight_layout() plt.show()  print(f\"Estat\u00edsticas do Treinamento:\") print(\"=\"*80) print(f\"Total de \u00e9pocas treinadas: {epochs_trained}\") print(f\"Melhor \u00e9poca: {best_epoch}\") print(f\"Melhor validation loss: {best_val_loss:.4f}\") print(f\"Training loss final: {model.train_loss_history[-1]:.4f}\") print(f\"Validation loss final: {model.val_loss_history[-1]:.4f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Interpreta\u00e7\u00e3o das Curvas:\n\n**An\u00e1lise esperada (ajustar ap\u00f3s executar):**\n\n1. **Converg\u00eancia:**\n   - Loss de treinamento e valida\u00e7\u00e3o devem diminuir nas primeiras \u00e9pocas\n   - Plateau indica que modelo atingiu capacidade de aprendizado\n\n2. **Overfitting/Underfitting:**\n   - **Ideal:** Training e validation loss pr\u00f3ximos e est\u00e1veis\n   - **Overfitting:** Training loss continua caindo, validation loss sobe\n   - **Underfitting:** Ambos loss altos e n\u00e3o convergem\n\n3. **Early Stopping:**\n   - Linha verde vertical marca quando validation loss foi m\u00ednimo\n   - Modelo usa pesos dessa \u00e9poca (melhor generaliza\u00e7\u00e3o)\"\"\")\n</pre> add_markdown_cell(\"\"\"### Interpreta\u00e7\u00e3o das Curvas:  **An\u00e1lise esperada (ajustar ap\u00f3s executar):**  1. **Converg\u00eancia:**    - Loss de treinamento e valida\u00e7\u00e3o devem diminuir nas primeiras \u00e9pocas    - Plateau indica que modelo atingiu capacidade de aprendizado  2. **Overfitting/Underfitting:**    - **Ideal:** Training e validation loss pr\u00f3ximos e est\u00e1veis    - **Overfitting:** Training loss continua caindo, validation loss sobe    - **Underfitting:** Ambos loss altos e n\u00e3o convergem  3. **Early Stopping:**    - Linha verde vertical marca quando validation loss foi m\u00ednimo    - Modelo usa pesos dessa \u00e9poca (melhor generaliza\u00e7\u00e3o)\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# An\u00e1lise de overfitting/underfitting\nprint(\"AN\u00c1LISE DE OVERFITTING/UNDERFITTING\")\nprint(\"=\"*80)\n\nfinal_train_loss = model.train_loss_history[-1]\nfinal_val_loss = model.val_loss_history[-1]\ngap = final_val_loss - final_train_loss\ngap_pct = (gap / final_train_loss) * 100\n\nprint(f\"Training loss (final): {final_train_loss:.4f}\")\nprint(f\"Validation loss (final): {final_val_loss:.4f}\")\nprint(f\"Gap (val - train): {gap:.4f} ({gap_pct:+.1f}%)\")\nprint()\n\nif gap_pct &lt; 5:\n    print(\"MODELO BEM AJUSTADO: Gap pequeno entre train e validation\")\n    print(\"  Modelo est\u00e1 generalizando bem!\")\nelif gap_pct &lt; 20:\n    print(\"LEVE OVERFITTING: Gap moderado entre train e validation\")\n    print(\"  Modelo ainda generalizando razoavelmente bem\")\nelse:\n    print(\"OVERFITTING DETECTADO: Gap grande entre train e validation\")\n    print(\"  Sugest\u00f5es: Aumentar regulariza\u00e7\u00e3o, usar dropout, ou reduzir complexidade\")\n\nif final_val_loss &gt; 0.5:\n    print(\"\\\\nPOSS\u00cdVEL UNDERFITTING: Loss de valida\u00e7\u00e3o ainda alto\")\n    print(\"  Sugest\u00f5es: Aumentar complexidade do modelo, treinar por mais \u00e9pocas\")\"\"\")\n</pre> add_code_cell(\"\"\"# An\u00e1lise de overfitting/underfitting print(\"AN\u00c1LISE DE OVERFITTING/UNDERFITTING\") print(\"=\"*80)  final_train_loss = model.train_loss_history[-1] final_val_loss = model.val_loss_history[-1] gap = final_val_loss - final_train_loss gap_pct = (gap / final_train_loss) * 100  print(f\"Training loss (final): {final_train_loss:.4f}\") print(f\"Validation loss (final): {final_val_loss:.4f}\") print(f\"Gap (val - train): {gap:.4f} ({gap_pct:+.1f}%)\") print()  if gap_pct &lt; 5:     print(\"MODELO BEM AJUSTADO: Gap pequeno entre train e validation\")     print(\"  Modelo est\u00e1 generalizando bem!\") elif gap_pct &lt; 20:     print(\"LEVE OVERFITTING: Gap moderado entre train e validation\")     print(\"  Modelo ainda generalizando razoavelmente bem\") else:     print(\"OVERFITTING DETECTADO: Gap grande entre train e validation\")     print(\"  Sugest\u00f5es: Aumentar regulariza\u00e7\u00e3o, usar dropout, ou reduzir complexidade\")  if final_val_loss &gt; 0.5:     print(\"\\\\nPOSS\u00cdVEL UNDERFITTING: Loss de valida\u00e7\u00e3o ainda alto\")     print(\"  Sugest\u00f5es: Aumentar complexidade do modelo, treinar por mais \u00e9pocas\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 10. M\u00e9tricas de Avalia\u00e7\u00e3o\n\n### M\u00e9tricas para Regress\u00e3o:\n\n1. **MAE (Mean Absolute Error):**\n   - M\u00e9dia dos erros absolutos\n   - `MAE = (1/n) * \u03a3|y_pred - y_true|`\n   - **Interpreta\u00e7\u00e3o:** Erro m\u00e9dio em unidades do target\n\n2. **MSE (Mean Squared Error):**\n   - M\u00e9dia dos erros ao quadrado\n   - `MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2`\n   - **Interpreta\u00e7\u00e3o:** Penaliza erros grandes mais fortemente\n\n3. **RMSE (Root Mean Squared Error):**\n   - Raiz quadrada do MSE\n   - `RMSE = sqrt(MSE)`\n   - **Interpreta\u00e7\u00e3o:** Erro m\u00e9dio na mesma unidade do target\n\n4. **R\u00b2 (Coefficient of Determination):**\n   - Propor\u00e7\u00e3o da vari\u00e2ncia explicada pelo modelo\n   - `R\u00b2 = 1 - (SS_res / SS_tot)`\n   - **Interpreta\u00e7\u00e3o:** 0 = modelo in\u00fatil, 1 = modelo perfeito\n\n5. **MAPE (Mean Absolute Percentage Error):**\n   - Erro percentual m\u00e9dio\n   - `MAPE = (100/n) * \u03a3|((y_true - y_pred) / y_true)|`\n   - **Interpreta\u00e7\u00e3o:** Erro em percentual (cuidado com valores pr\u00f3ximos de zero)\"\"\")\n</pre> add_markdown_cell(\"\"\"## 10. M\u00e9tricas de Avalia\u00e7\u00e3o  ### M\u00e9tricas para Regress\u00e3o:  1. **MAE (Mean Absolute Error):**    - M\u00e9dia dos erros absolutos    - `MAE = (1/n) * \u03a3|y_pred - y_true|`    - **Interpreta\u00e7\u00e3o:** Erro m\u00e9dio em unidades do target  2. **MSE (Mean Squared Error):**    - M\u00e9dia dos erros ao quadrado    - `MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2`    - **Interpreta\u00e7\u00e3o:** Penaliza erros grandes mais fortemente  3. **RMSE (Root Mean Squared Error):**    - Raiz quadrada do MSE    - `RMSE = sqrt(MSE)`    - **Interpreta\u00e7\u00e3o:** Erro m\u00e9dio na mesma unidade do target  4. **R\u00b2 (Coefficient of Determination):**    - Propor\u00e7\u00e3o da vari\u00e2ncia explicada pelo modelo    - `R\u00b2 = 1 - (SS_res / SS_tot)`    - **Interpreta\u00e7\u00e3o:** 0 = modelo in\u00fatil, 1 = modelo perfeito  5. **MAPE (Mean Absolute Percentage Error):**    - Erro percentual m\u00e9dio    - `MAPE = (100/n) * \u03a3|((y_true - y_pred) / y_true)|`    - **Interpreta\u00e7\u00e3o:** Erro em percentual (cuidado com valores pr\u00f3ximos de zero)\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"def calculate_regression_metrics(y_true, y_pred):\n    \\\"\\\"\\\"\n    Calcula m\u00e9tricas de avalia\u00e7\u00e3o para regress\u00e3o.\n\n    Par\u00e2metros:\n    -----------\n    y_true : array\n        Valores reais\n    y_pred : array\n        Valores preditos\n\n    Retorna:\n    --------\n    metrics : dict\n        Dicion\u00e1rio com todas as m\u00e9tricas\n    \\\"\\\"\\\"\n    # Garantir que s\u00e3o arrays 1D\n    y_true = y_true.flatten()\n    y_pred = y_pred.flatten()\n\n    # MAE\n    mae = np.mean(np.abs(y_true - y_pred))\n\n    # MSE\n    mse = np.mean((y_true - y_pred) ** 2)\n\n    # RMSE\n    rmse = np.sqrt(mse)\n\n    # R\u00b2\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n\n    # MAPE (evitar divis\u00e3o por zero)\n    mask = y_true != 0\n    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n    metrics = {\n        'MAE': mae,\n        'MSE': mse,\n        'RMSE': rmse,\n        'R2': r2,\n        'MAPE': mape\n    }\n\n    return metrics\n\ndef denormalize_predictions(y_normalized, y_mean, y_std):\n    \\\"\\\"\\\"\n    Desnormaliza predi\u00e7\u00f5es para escala original.\n\n    y_original = (y_normalized * std) + mean\n    \\\"\\\"\\\"\n    return (y_normalized * y_std) + y_mean\n\nprint(\"Fun\u00e7\u00f5es de avalia\u00e7\u00e3o implementadas!\")\"\"\")\n</pre> add_code_cell(\"\"\"def calculate_regression_metrics(y_true, y_pred):     \\\"\\\"\\\"     Calcula m\u00e9tricas de avalia\u00e7\u00e3o para regress\u00e3o.      Par\u00e2metros:     -----------     y_true : array         Valores reais     y_pred : array         Valores preditos      Retorna:     --------     metrics : dict         Dicion\u00e1rio com todas as m\u00e9tricas     \\\"\\\"\\\"     # Garantir que s\u00e3o arrays 1D     y_true = y_true.flatten()     y_pred = y_pred.flatten()      # MAE     mae = np.mean(np.abs(y_true - y_pred))      # MSE     mse = np.mean((y_true - y_pred) ** 2)      # RMSE     rmse = np.sqrt(mse)      # R\u00b2     ss_res = np.sum((y_true - y_pred) ** 2)     ss_tot = np.sum((y_true - y_true.mean()) ** 2)     r2 = 1 - (ss_res / ss_tot)      # MAPE (evitar divis\u00e3o por zero)     mask = y_true != 0     mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100      metrics = {         'MAE': mae,         'MSE': mse,         'RMSE': rmse,         'R2': r2,         'MAPE': mape     }      return metrics  def denormalize_predictions(y_normalized, y_mean, y_std):     \\\"\\\"\\\"     Desnormaliza predi\u00e7\u00f5es para escala original.      y_original = (y_normalized * std) + mean     \\\"\\\"\\\"     return (y_normalized * y_std) + y_mean  print(\"Fun\u00e7\u00f5es de avalia\u00e7\u00e3o implementadas!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Avalia\u00e7\u00e3o no Conjunto de Teste\"\"\")\n</pre> add_markdown_cell(\"\"\"### Avalia\u00e7\u00e3o no Conjunto de Teste\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"AVALIA\u00c7\u00c3O NO CONJUNTO DE TESTE\")\nprint(\"=\"*80)\n\n# Fazer predi\u00e7\u00f5es (dados normalizados)\ny_test_pred_norm = model.predict(X_test)\ny_train_pred_norm = model.predict(X_train)\ny_val_pred_norm = model.predict(X_val)\n\n# Desnormalizar para escala original\ny_test_pred = denormalize_predictions(y_test_pred_norm, y_mean, y_std)\ny_test_true = denormalize_predictions(y_test, y_mean, y_std)\n\ny_train_pred = denormalize_predictions(y_train_pred_norm, y_mean, y_std)\ny_train_true = denormalize_predictions(y_train, y_mean, y_std)\n\ny_val_pred = denormalize_predictions(y_val_pred_norm, y_mean, y_std)\ny_val_true = denormalize_predictions(y_val, y_mean, y_std)\n\n# Calcular m\u00e9tricas para cada conjunto\ntrain_metrics = calculate_regression_metrics(y_train_true, y_train_pred)\nval_metrics = calculate_regression_metrics(y_val_true, y_val_pred)\ntest_metrics = calculate_regression_metrics(y_test_true, y_test_pred)\n\n# Criar DataFrame de compara\u00e7\u00e3o\nmetrics_df = pd.DataFrame({\n    'M\u00e9trica': ['MAE', 'MSE', 'RMSE', 'R\u00b2', 'MAPE (%)'],\n    'Training': [\n        f\"{train_metrics['MAE']:.2f}\",\n        f\"{train_metrics['MSE']:.2f}\",\n        f\"{train_metrics['RMSE']:.2f}\",\n        f\"{train_metrics['R2']:.4f}\",\n        f\"{train_metrics['MAPE']:.2f}%\"\n    ],\n    'Validation': [\n        f\"{val_metrics['MAE']:.2f}\",\n        f\"{val_metrics['MSE']:.2f}\",\n        f\"{val_metrics['RMSE']:.2f}\",\n        f\"{val_metrics['R2']:.4f}\",\n        f\"{val_metrics['MAPE']:.2f}%\"\n    ],\n    'Test': [\n        f\"{test_metrics['MAE']:.2f}\",\n        f\"{test_metrics['MSE']:.2f}\",\n        f\"{test_metrics['RMSE']:.2f}\",\n        f\"{test_metrics['R2']:.4f}\",\n        f\"{test_metrics['MAPE']:.2f}%\"\n    ]\n})\n\nprint(metrics_df.to_string(index=False))\nprint(\"\\\\n\" + \"=\"*80)\"\"\")\n</pre> add_code_cell(\"\"\"print(\"AVALIA\u00c7\u00c3O NO CONJUNTO DE TESTE\") print(\"=\"*80)  # Fazer predi\u00e7\u00f5es (dados normalizados) y_test_pred_norm = model.predict(X_test) y_train_pred_norm = model.predict(X_train) y_val_pred_norm = model.predict(X_val)  # Desnormalizar para escala original y_test_pred = denormalize_predictions(y_test_pred_norm, y_mean, y_std) y_test_true = denormalize_predictions(y_test, y_mean, y_std)  y_train_pred = denormalize_predictions(y_train_pred_norm, y_mean, y_std) y_train_true = denormalize_predictions(y_train, y_mean, y_std)  y_val_pred = denormalize_predictions(y_val_pred_norm, y_mean, y_std) y_val_true = denormalize_predictions(y_val, y_mean, y_std)  # Calcular m\u00e9tricas para cada conjunto train_metrics = calculate_regression_metrics(y_train_true, y_train_pred) val_metrics = calculate_regression_metrics(y_val_true, y_val_pred) test_metrics = calculate_regression_metrics(y_test_true, y_test_pred)  # Criar DataFrame de compara\u00e7\u00e3o metrics_df = pd.DataFrame({     'M\u00e9trica': ['MAE', 'MSE', 'RMSE', 'R\u00b2', 'MAPE (%)'],     'Training': [         f\"{train_metrics['MAE']:.2f}\",         f\"{train_metrics['MSE']:.2f}\",         f\"{train_metrics['RMSE']:.2f}\",         f\"{train_metrics['R2']:.4f}\",         f\"{train_metrics['MAPE']:.2f}%\"     ],     'Validation': [         f\"{val_metrics['MAE']:.2f}\",         f\"{val_metrics['MSE']:.2f}\",         f\"{val_metrics['RMSE']:.2f}\",         f\"{val_metrics['R2']:.4f}\",         f\"{val_metrics['MAPE']:.2f}%\"     ],     'Test': [         f\"{test_metrics['MAE']:.2f}\",         f\"{test_metrics['MSE']:.2f}\",         f\"{test_metrics['RMSE']:.2f}\",         f\"{test_metrics['R2']:.4f}\",         f\"{test_metrics['MAPE']:.2f}%\"     ] })  print(metrics_df.to_string(index=False)) print(\"\\\\n\" + \"=\"*80)\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Compara\u00e7\u00e3o com Baseline\"\"\")\n</pre> add_markdown_cell(\"\"\"### Compara\u00e7\u00e3o com Baseline\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"COMPARA\u00c7\u00c3O COM BASELINE (M\u00c9DIA)\")\nprint(\"=\"*80)\n\n# Baseline: sempre prever a m\u00e9dia do training set\nbaseline_pred = np.full_like(y_test_true, y_train_true.mean())\nbaseline_metrics = calculate_regression_metrics(y_test_true, baseline_pred)\n\nprint(f\"Baseline (sempre prever m\u00e9dia = {y_train_true.mean():.2f}):\")\nprint(f\"  MAE:  {baseline_metrics['MAE']:.2f}\")\nprint(f\"  RMSE: {baseline_metrics['RMSE']:.2f}\")\nprint(f\"  R\u00b2:   {baseline_metrics['R2']:.4f}\")\n\nprint(f\"\\\\nMLP Model (nosso modelo):\")\nprint(f\"  MAE:  {test_metrics['MAE']:.2f}\")\nprint(f\"  RMSE: {test_metrics['RMSE']:.2f}\")\nprint(f\"  R\u00b2:   {test_metrics['R2']:.4f}\")\n\n# Melhoria relativa\nmae_improvement = ((baseline_metrics['MAE'] - test_metrics['MAE']) / baseline_metrics['MAE']) * 100\nrmse_improvement = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE']) * 100\n\nprint(f\"\\\\nMelhoria em rela\u00e7\u00e3o ao baseline:\")\nprint(f\"  MAE:  {mae_improvement:+.1f}%\")\nprint(f\"  RMSE: {rmse_improvement:+.1f}%\")\n\nprint(\"\\\\n\" + \"=\"*80)\n\nif test_metrics['R2'] &gt; 0.5:\n    print(\"MODELO COM BOA CAPACIDADE PREDITIVA (R\u00b2 &gt; 0.5)\")\nelif test_metrics['R2'] &gt; 0.3:\n    print(\"MODELO COM CAPACIDADE PREDITIVA MODERADA (0.3 &lt; R\u00b2 &lt; 0.5)\")\nelse:\n    print(\"MODELO COM BAIXA CAPACIDADE PREDITIVA (R\u00b2 &lt; 0.3)\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"COMPARA\u00c7\u00c3O COM BASELINE (M\u00c9DIA)\") print(\"=\"*80)  # Baseline: sempre prever a m\u00e9dia do training set baseline_pred = np.full_like(y_test_true, y_train_true.mean()) baseline_metrics = calculate_regression_metrics(y_test_true, baseline_pred)  print(f\"Baseline (sempre prever m\u00e9dia = {y_train_true.mean():.2f}):\") print(f\"  MAE:  {baseline_metrics['MAE']:.2f}\") print(f\"  RMSE: {baseline_metrics['RMSE']:.2f}\") print(f\"  R\u00b2:   {baseline_metrics['R2']:.4f}\")  print(f\"\\\\nMLP Model (nosso modelo):\") print(f\"  MAE:  {test_metrics['MAE']:.2f}\") print(f\"  RMSE: {test_metrics['RMSE']:.2f}\") print(f\"  R\u00b2:   {test_metrics['R2']:.4f}\")  # Melhoria relativa mae_improvement = ((baseline_metrics['MAE'] - test_metrics['MAE']) / baseline_metrics['MAE']) * 100 rmse_improvement = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE']) * 100  print(f\"\\\\nMelhoria em rela\u00e7\u00e3o ao baseline:\") print(f\"  MAE:  {mae_improvement:+.1f}%\") print(f\"  RMSE: {rmse_improvement:+.1f}%\")  print(\"\\\\n\" + \"=\"*80)  if test_metrics['R2'] &gt; 0.5:     print(\"MODELO COM BOA CAPACIDADE PREDITIVA (R\u00b2 &gt; 0.5)\") elif test_metrics['R2'] &gt; 0.3:     print(\"MODELO COM CAPACIDADE PREDITIVA MODERADA (0.3 &lt; R\u00b2 &lt; 0.5)\") else:     print(\"MODELO COM BAIXA CAPACIDADE PREDITIVA (R\u00b2 &lt; 0.3)\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Visualiza\u00e7\u00f5es de Avalia\u00e7\u00e3o\"\"\")\n</pre> add_markdown_cell(\"\"\"### Visualiza\u00e7\u00f5es de Avalia\u00e7\u00e3o\"\"\") In\u00a0[\u00a0]: Copied! <pre># Continua no pr\u00f3ximo script...\nprint(f\"C\u00e9lulas adicionadas at\u00e9 avalia\u00e7\u00e3o! Total: {len(notebook['cells'])}\")\n</pre> # Continua no pr\u00f3ximo script... print(f\"C\u00e9lulas adicionadas at\u00e9 avalia\u00e7\u00e3o! Total: {len(notebook['cells'])}\") In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> with open(notebook_path, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(\"Notebook atualizado com sucesso!\")\n</pre> print(\"Notebook atualizado com sucesso!\")"},{"location":"portfolio/neural-networks/projects/2/add_final_sections/#secao-8-treinamento","title":"============================================================================== SE\u00c7\u00c3O 8: Treinamento\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_final_sections/#secao-9-curvas-de-erro","title":"============================================================================== SE\u00c7\u00c3O 9: Curvas de Erro\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_final_sections/#secao-10-metricas-de-avaliacao","title":"============================================================================== SE\u00c7\u00c3O 10: M\u00e9tricas de Avalia\u00e7\u00e3o\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_mlp_and_training/","title":"Add mlp and training","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para adicionar se\u00e7\u00f5es de MLP, treinamento e avalia\u00e7\u00e3o.\nExecute AP\u00d3S add_remaining_cells.py\n\"\"\"\n</pre> \"\"\" Script para adicionar se\u00e7\u00f5es de MLP, treinamento e avalia\u00e7\u00e3o. Execute AP\u00d3S add_remaining_cells.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre># Carregar notebook\nnotebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> # Carregar notebook notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n</pre> with open(notebook_path, 'r', encoding='utf-8') as f:     notebook = json.load(f) In\u00a0[\u00a0]: Copied! <pre>def add_markdown_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_markdown_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_code_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\")\n</pre> print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 6. Divis\u00e3o de Dados: Train / Validation / Test\n\n### Estrat\u00e9gia de Divis\u00e3o:\n\n- **70%** Training set (12,165 amostras) - treinar o modelo\n- **15%** Validation set (2,607 amostras) - ajustar hiperpar\u00e2metros e monitorar overfitting\n- **15%** Test set (2,607 amostras) - avalia\u00e7\u00e3o final e m\u00e9tricas reportadas\n\n### Justificativa:\n\n1. **Train set (70%)**: Dataset grande o suficiente, 70% proporciona amostras suficientes para aprendizado\n2. **Validation set (15%)**: Usado durante treinamento para early stopping e evitar overfitting\n3. **Test set (15%)**: Avalia\u00e7\u00e3o imparcial do modelo final\n\n### Modo de Treinamento:\n\n- **Mini-batch Gradient Descent** com batch size = 64\n- **Raz\u00e3o**: Equil\u00edbrio entre velocidade (vs. SGD) e estabilidade (vs. Batch GD)\n- Mini-batches permitem:\n  - Converg\u00eancia mais r\u00e1pida que batch completo\n  - Gradientes mais est\u00e1veis que SGD puro\n  - Uso eficiente de mem\u00f3ria\n\n### Reprodutibilidade:\n\n- **Random seed = 42** para garantir mesma divis\u00e3o em execu\u00e7\u00f5es diferentes\n- Importante para comparar resultados entre experimentos\"\"\")\n</pre> add_markdown_cell(\"\"\"## 6. Divis\u00e3o de Dados: Train / Validation / Test  ### Estrat\u00e9gia de Divis\u00e3o:  - **70%** Training set (12,165 amostras) - treinar o modelo - **15%** Validation set (2,607 amostras) - ajustar hiperpar\u00e2metros e monitorar overfitting - **15%** Test set (2,607 amostras) - avalia\u00e7\u00e3o final e m\u00e9tricas reportadas  ### Justificativa:  1. **Train set (70%)**: Dataset grande o suficiente, 70% proporciona amostras suficientes para aprendizado 2. **Validation set (15%)**: Usado durante treinamento para early stopping e evitar overfitting 3. **Test set (15%)**: Avalia\u00e7\u00e3o imparcial do modelo final  ### Modo de Treinamento:  - **Mini-batch Gradient Descent** com batch size = 64 - **Raz\u00e3o**: Equil\u00edbrio entre velocidade (vs. SGD) e estabilidade (vs. Batch GD) - Mini-batches permitem:   - Converg\u00eancia mais r\u00e1pida que batch completo   - Gradientes mais est\u00e1veis que SGD puro   - Uso eficiente de mem\u00f3ria  ### Reprodutibilidade:  - **Random seed = 42** para garantir mesma divis\u00e3o em execu\u00e7\u00f5es diferentes - Importante para comparar resultados entre experimentos\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"DIVIS\u00c3O DOS DADOS EM TRAIN / VALIDATION / TEST\")\nprint(\"=\"*80)\n\n# Seed para reprodutibilidade\nnp.random.seed(42)\n\n# Total de amostras\nn_samples = X_normalized.shape[0]\n\n# Shuffle dos \u00edndices\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\n\n# Definir tamanhos\ntrain_size = int(0.70 * n_samples)\nval_size = int(0.15 * n_samples)\ntest_size = n_samples - train_size - val_size\n\n# Dividir \u00edndices\ntrain_indices = indices[:train_size]\nval_indices = indices[train_size:train_size + val_size]\ntest_indices = indices[train_size + val_size:]\n\n# Criar sets\nX_train = X_normalized[train_indices]\ny_train = y_normalized[train_indices]\n\nX_val = X_normalized[val_indices]\ny_val = y_normalized[val_indices]\n\nX_test = X_normalized[test_indices]\ny_test = y_normalized[test_indices]\n\nprint(f\"Total de amostras: {n_samples:,}\\\\n\")\nprint(f\"Training Set:\")\nprint(f\"  X_train: {X_train.shape} | y_train: {y_train.shape}\")\nprint(f\"  {len(X_train):,} amostras ({len(X_train)/n_samples*100:.1f}%)\\\\n\")\n\nprint(f\"Validation Set:\")\nprint(f\"  X_val: {X_val.shape} | y_val: {y_val.shape}\")\nprint(f\"  {len(X_val):,} amostras ({len(X_val)/n_samples*100:.1f}%)\\\\n\")\n\nprint(f\"Test Set:\")\nprint(f\"  X_test: {X_test.shape} | y_test: {y_test.shape}\")\nprint(f\"  {len(X_test):,} amostras ({len(X_test)/n_samples*100:.1f}%)\")\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"Divis\u00e3o conclu\u00edda com sucesso!\")\nprint(f\"Random seed: 42 (para reprodutibilidade)\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"DIVIS\u00c3O DOS DADOS EM TRAIN / VALIDATION / TEST\") print(\"=\"*80)  # Seed para reprodutibilidade np.random.seed(42)  # Total de amostras n_samples = X_normalized.shape[0]  # Shuffle dos \u00edndices indices = np.arange(n_samples) np.random.shuffle(indices)  # Definir tamanhos train_size = int(0.70 * n_samples) val_size = int(0.15 * n_samples) test_size = n_samples - train_size - val_size  # Dividir \u00edndices train_indices = indices[:train_size] val_indices = indices[train_size:train_size + val_size] test_indices = indices[train_size + val_size:]  # Criar sets X_train = X_normalized[train_indices] y_train = y_normalized[train_indices]  X_val = X_normalized[val_indices] y_val = y_normalized[val_indices]  X_test = X_normalized[test_indices] y_test = y_normalized[test_indices]  print(f\"Total de amostras: {n_samples:,}\\\\n\") print(f\"Training Set:\") print(f\"  X_train: {X_train.shape} | y_train: {y_train.shape}\") print(f\"  {len(X_train):,} amostras ({len(X_train)/n_samples*100:.1f}%)\\\\n\")  print(f\"Validation Set:\") print(f\"  X_val: {X_val.shape} | y_val: {y_val.shape}\") print(f\"  {len(X_val):,} amostras ({len(X_val)/n_samples*100:.1f}%)\\\\n\")  print(f\"Test Set:\") print(f\"  X_test: {X_test.shape} | y_test: {y_test.shape}\") print(f\"  {len(X_test):,} amostras ({len(X_test)/n_samples*100:.1f}%)\")  print(\"\\\\n\" + \"=\"*80) print(\"Divis\u00e3o conclu\u00edda com sucesso!\") print(f\"Random seed: 42 (para reprodutibilidade)\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"## 7. Implementa\u00e7\u00e3o do MLP (Multi-Layer Perceptron)\n\n### Arquitetura da Rede Neural:\n\n```\nInput Layer (15 neur\u00f4nios) \u2192 Hidden Layer 1 (64 neur\u00f4nios, ReLU)\n                           \u2192 Hidden Layer 2 (32 neur\u00f4nios, ReLU)\n                           \u2192 Hidden Layer 3 (16 neur\u00f4nios, ReLU)\n                           \u2192 Output Layer (1 neur\u00f4nio, Linear)\n```\n\n### Componentes Implementados:\n\n1. **Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o:**\n   - **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)` para camadas ocultas\n   - **Linear**: `f(x) = x` para camada de sa\u00edda (regress\u00e3o)\n\n2. **Fun\u00e7\u00e3o de Perda:**\n   - **MSE** (Mean Squared Error): `L = (1/n) * \u03a3(y_pred - y_true)\u00b2`\n   - Padr\u00e3o para problemas de regress\u00e3o\n\n3. **Inicializa\u00e7\u00e3o de Pesos:**\n   - **He Initialization** para camadas com ReLU: `W ~ N(0, sqrt(2/n_in))`\n   - Previne vanishing/exploding gradients\n\n4. **Otimizador:**\n   - **Mini-batch Gradient Descent** com learning rate adaptativo\n   - Batch size: 64\n   - Learning rate: 0.001 (inicial)\n\n5. **Regulariza\u00e7\u00e3o:**\n   - **L2 Regularization** (Ridge): penaliza\u00e7\u00e3o de pesos grandes\n   - Lambda = 0.001\"\"\")\n</pre> add_markdown_cell(\"\"\"## 7. Implementa\u00e7\u00e3o do MLP (Multi-Layer Perceptron)  ### Arquitetura da Rede Neural:  ``` Input Layer (15 neur\u00f4nios) \u2192 Hidden Layer 1 (64 neur\u00f4nios, ReLU)                            \u2192 Hidden Layer 2 (32 neur\u00f4nios, ReLU)                            \u2192 Hidden Layer 3 (16 neur\u00f4nios, ReLU)                            \u2192 Output Layer (1 neur\u00f4nio, Linear) ```  ### Componentes Implementados:  1. **Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o:**    - **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)` para camadas ocultas    - **Linear**: `f(x) = x` para camada de sa\u00edda (regress\u00e3o)  2. **Fun\u00e7\u00e3o de Perda:**    - **MSE** (Mean Squared Error): `L = (1/n) * \u03a3(y_pred - y_true)\u00b2`    - Padr\u00e3o para problemas de regress\u00e3o  3. **Inicializa\u00e7\u00e3o de Pesos:**    - **He Initialization** para camadas com ReLU: `W ~ N(0, sqrt(2/n_in))`    - Previne vanishing/exploding gradients  4. **Otimizador:**    - **Mini-batch Gradient Descent** com learning rate adaptativo    - Batch size: 64    - Learning rate: 0.001 (inicial)  5. **Regulariza\u00e7\u00e3o:**    - **L2 Regularization** (Ridge): penaliza\u00e7\u00e3o de pesos grandes    - Lambda = 0.001\"\"\") In\u00a0[\u00a0]: Copied! <pre># Implementa\u00e7\u00e3o completa do MLP (dividida em partes para melhor organiza\u00e7\u00e3o)\nmlp_code_part1 = \"\"\"class MLPRegressor:\n    \\\"\\\"\\\"\n    Multi-Layer Perceptron para Regress\u00e3o implementado do zero.\n\n    Arquitetura: Input \u2192 Hidden1 (ReLU) \u2192 Hidden2 (ReLU) \u2192 Hidden3 (ReLU) \u2192 Output (Linear)\n    \\\"\\\"\\\"\n\n    def __init__(self, input_size, hidden_sizes, output_size=1, learning_rate=0.001,\n                 reg_lambda=0.001, random_seed=42):\n        \\\"\\\"\\\"\n        Inicializa o MLP.\n\n        Par\u00e2metros:\n        -----------\n        input_size : int\n            N\u00famero de features de entrada\n        hidden_sizes : list\n            Lista com n\u00famero de neur\u00f4nios em cada camada oculta\n        output_size : int\n            N\u00famero de neur\u00f4nios de sa\u00edda (1 para regress\u00e3o)\n        learning_rate : float\n            Taxa de aprendizado inicial\n        reg_lambda : float\n            Par\u00e2metro de regulariza\u00e7\u00e3o L2\n        random_seed : int\n            Seed para reprodutibilidade\n        \\\"\\\"\\\"\n        np.random.seed(random_seed)\n\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.reg_lambda = reg_lambda\n\n        # Inicializar pesos e biases\n        self.weights = []\n        self.biases = []\n\n        # Criar lista de tamanhos de camadas\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        # He Initialization para cada camada\n        for i in range(len(layer_sizes) - 1):\n            n_in = layer_sizes[i]\n            n_out = layer_sizes[i + 1]\n\n            # He initialization: std = sqrt(2 / n_in)\n            W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n            b = np.zeros((1, n_out))\n\n            self.weights.append(W)\n            self.biases.append(b)\n\n        # Hist\u00f3rico de treinamento\n        self.train_loss_history = []\n        self.val_loss_history = []\n\n    def relu(self, Z):\n        \\\"\\\"\\\"ReLU activation: f(x) = max(0, x)\\\"\\\"\\\"\n        return np.maximum(0, Z)\n\n    def relu_derivative(self, Z):\n        \\\"\\\"\\\"Derivada da ReLU: f'(x) = 1 se x &gt; 0, caso contr\u00e1rio 0\\\"\\\"\\\"\n        return (Z &gt; 0).astype(float)\n\n    def forward(self, X):\n        \\\"\\\"\\\"\n        Forward propagation.\n\n        Retorna:\n        --------\n        output : array\n            Predi\u00e7\u00f5es do modelo\n        cache : dict\n            Valores intermedi\u00e1rios (para backpropagation)\n        \\\"\\\"\\\"\n        cache = {'A': [X]}  # Armazenar ativa\u00e7\u00f5es\n        cache['Z'] = []     # Armazenar outputs pr\u00e9-ativa\u00e7\u00e3o\n\n        A = X\n\n        # Camadas ocultas (com ReLU)\n        for i in range(len(self.weights) - 1):\n            Z = np.dot(A, self.weights[i]) + self.biases[i]\n            A = self.relu(Z)\n\n            cache['Z'].append(Z)\n            cache['A'].append(A)\n\n        # Camada de sa\u00edda (linear, sem ativa\u00e7\u00e3o)\n        Z_out = np.dot(A, self.weights[-1]) + self.biases[-1]\n        output = Z_out  # Ativa\u00e7\u00e3o linear para regress\u00e3o\n\n        cache['Z'].append(Z_out)\n        cache['A'].append(output)\n\n        return output, cache\"\"\"\n</pre> # Implementa\u00e7\u00e3o completa do MLP (dividida em partes para melhor organiza\u00e7\u00e3o) mlp_code_part1 = \"\"\"class MLPRegressor:     \\\"\\\"\\\"     Multi-Layer Perceptron para Regress\u00e3o implementado do zero.      Arquitetura: Input \u2192 Hidden1 (ReLU) \u2192 Hidden2 (ReLU) \u2192 Hidden3 (ReLU) \u2192 Output (Linear)     \\\"\\\"\\\"      def __init__(self, input_size, hidden_sizes, output_size=1, learning_rate=0.001,                  reg_lambda=0.001, random_seed=42):         \\\"\\\"\\\"         Inicializa o MLP.          Par\u00e2metros:         -----------         input_size : int             N\u00famero de features de entrada         hidden_sizes : list             Lista com n\u00famero de neur\u00f4nios em cada camada oculta         output_size : int             N\u00famero de neur\u00f4nios de sa\u00edda (1 para regress\u00e3o)         learning_rate : float             Taxa de aprendizado inicial         reg_lambda : float             Par\u00e2metro de regulariza\u00e7\u00e3o L2         random_seed : int             Seed para reprodutibilidade         \\\"\\\"\\\"         np.random.seed(random_seed)          self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.output_size = output_size         self.learning_rate = learning_rate         self.reg_lambda = reg_lambda          # Inicializar pesos e biases         self.weights = []         self.biases = []          # Criar lista de tamanhos de camadas         layer_sizes = [input_size] + hidden_sizes + [output_size]          # He Initialization para cada camada         for i in range(len(layer_sizes) - 1):             n_in = layer_sizes[i]             n_out = layer_sizes[i + 1]              # He initialization: std = sqrt(2 / n_in)             W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)             b = np.zeros((1, n_out))              self.weights.append(W)             self.biases.append(b)          # Hist\u00f3rico de treinamento         self.train_loss_history = []         self.val_loss_history = []      def relu(self, Z):         \\\"\\\"\\\"ReLU activation: f(x) = max(0, x)\\\"\\\"\\\"         return np.maximum(0, Z)      def relu_derivative(self, Z):         \\\"\\\"\\\"Derivada da ReLU: f'(x) = 1 se x &gt; 0, caso contr\u00e1rio 0\\\"\\\"\\\"         return (Z &gt; 0).astype(float)      def forward(self, X):         \\\"\\\"\\\"         Forward propagation.          Retorna:         --------         output : array             Predi\u00e7\u00f5es do modelo         cache : dict             Valores intermedi\u00e1rios (para backpropagation)         \\\"\\\"\\\"         cache = {'A': [X]}  # Armazenar ativa\u00e7\u00f5es         cache['Z'] = []     # Armazenar outputs pr\u00e9-ativa\u00e7\u00e3o          A = X          # Camadas ocultas (com ReLU)         for i in range(len(self.weights) - 1):             Z = np.dot(A, self.weights[i]) + self.biases[i]             A = self.relu(Z)              cache['Z'].append(Z)             cache['A'].append(A)          # Camada de sa\u00edda (linear, sem ativa\u00e7\u00e3o)         Z_out = np.dot(A, self.weights[-1]) + self.biases[-1]         output = Z_out  # Ativa\u00e7\u00e3o linear para regress\u00e3o          cache['Z'].append(Z_out)         cache['A'].append(output)          return output, cache\"\"\" In\u00a0[\u00a0]: Copied! <pre>mlp_code_part2 = \"\"\"    def compute_loss(self, y_true, y_pred):\n        \\\"\\\"\\\"\n        Calcula MSE loss com regulariza\u00e7\u00e3o L2.\n\n        Loss = MSE + L2_penalty\n        MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2\n        L2 = (\u03bb/2n) * \u03a3(W\u00b2)\n        \\\"\\\"\\\"\n        n_samples = y_true.shape[0]\n\n        # MSE\n        mse = np.mean((y_pred - y_true) ** 2)\n\n        # L2 regularization\n        l2_penalty = 0\n        for W in self.weights:\n            l2_penalty += np.sum(W ** 2)\n        l2_penalty *= (self.reg_lambda / (2 * n_samples))\n\n        total_loss = mse + l2_penalty\n\n        return total_loss\n\n    def backward(self, X, y_true, cache):\n        \\\"\\\"\\\"\n        Backpropagation para calcular gradientes.\n\n        Retorna:\n        --------\n        grads : dict\n            Gradientes dos pesos e biases\n        \\\"\\\"\\\"\n        n_samples = X.shape[0]\n        grads = {'W': [], 'b': []}\n\n        # Gradiente da loss em rela\u00e7\u00e3o \u00e0 sa\u00edda\n        y_pred = cache['A'][-1]\n        dA = (2.0 / n_samples) * (y_pred - y_true)\n\n        # Backprop atrav\u00e9s das camadas (de tr\u00e1s para frente)\n        for i in reversed(range(len(self.weights))):\n            A_prev = cache['A'][i]\n\n            # Gradientes de W e b\n            dW = np.dot(A_prev.T, dA)\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            # Adicionar regulariza\u00e7\u00e3o L2 ao gradiente de W\n            dW += (self.reg_lambda / n_samples) * self.weights[i]\n\n            # Inserir no in\u00edcio da lista (pois estamos indo de tr\u00e1s para frente)\n            grads['W'].insert(0, dW)\n            grads['b'].insert(0, db)\n\n            # Gradiente para camada anterior (se n\u00e3o for a primeira camada)\n            if i &gt; 0:\n                dA = np.dot(dA, self.weights[i].T)\n                # Aplicar derivada da ReLU\n                dA = dA * self.relu_derivative(cache['Z'][i - 1])\n\n        return grads\n\n    def update_weights(self, grads):\n        \\\"\\\"\\\"Atualiza pesos usando gradiente descendente.\\\"\\\"\\\"\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * grads['W'][i]\n            self.biases[i] -= self.learning_rate * grads['b'][i]\"\"\"\n</pre> mlp_code_part2 = \"\"\"    def compute_loss(self, y_true, y_pred):         \\\"\\\"\\\"         Calcula MSE loss com regulariza\u00e7\u00e3o L2.          Loss = MSE + L2_penalty         MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2         L2 = (\u03bb/2n) * \u03a3(W\u00b2)         \\\"\\\"\\\"         n_samples = y_true.shape[0]          # MSE         mse = np.mean((y_pred - y_true) ** 2)          # L2 regularization         l2_penalty = 0         for W in self.weights:             l2_penalty += np.sum(W ** 2)         l2_penalty *= (self.reg_lambda / (2 * n_samples))          total_loss = mse + l2_penalty          return total_loss      def backward(self, X, y_true, cache):         \\\"\\\"\\\"         Backpropagation para calcular gradientes.          Retorna:         --------         grads : dict             Gradientes dos pesos e biases         \\\"\\\"\\\"         n_samples = X.shape[0]         grads = {'W': [], 'b': []}          # Gradiente da loss em rela\u00e7\u00e3o \u00e0 sa\u00edda         y_pred = cache['A'][-1]         dA = (2.0 / n_samples) * (y_pred - y_true)          # Backprop atrav\u00e9s das camadas (de tr\u00e1s para frente)         for i in reversed(range(len(self.weights))):             A_prev = cache['A'][i]              # Gradientes de W e b             dW = np.dot(A_prev.T, dA)             db = np.sum(dA, axis=0, keepdims=True)              # Adicionar regulariza\u00e7\u00e3o L2 ao gradiente de W             dW += (self.reg_lambda / n_samples) * self.weights[i]              # Inserir no in\u00edcio da lista (pois estamos indo de tr\u00e1s para frente)             grads['W'].insert(0, dW)             grads['b'].insert(0, db)              # Gradiente para camada anterior (se n\u00e3o for a primeira camada)             if i &gt; 0:                 dA = np.dot(dA, self.weights[i].T)                 # Aplicar derivada da ReLU                 dA = dA * self.relu_derivative(cache['Z'][i - 1])          return grads      def update_weights(self, grads):         \\\"\\\"\\\"Atualiza pesos usando gradiente descendente.\\\"\\\"\\\"         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * grads['W'][i]             self.biases[i] -= self.learning_rate * grads['b'][i]\"\"\" In\u00a0[\u00a0]: Copied! <pre>mlp_code_part3 = \"\"\"    def train_epoch(self, X_train, y_train, batch_size=64):\n        \\\"\\\"\\\"\n        Treina por uma \u00e9poca usando mini-batch gradient descent.\n\n        Retorna:\n        --------\n        epoch_loss : float\n            Loss m\u00e9dia da \u00e9poca\n        \\\"\\\"\\\"\n        n_samples = X_train.shape[0]\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n\n        epoch_loss = 0\n        n_batches = 0\n\n        # Mini-batch training\n        for start_idx in range(0, n_samples, batch_size):\n            end_idx = min(start_idx + batch_size, n_samples)\n            batch_indices = indices[start_idx:end_idx]\n\n            X_batch = X_train[batch_indices]\n            y_batch = y_train[batch_indices]\n\n            # Forward pass\n            y_pred, cache = self.forward(X_batch)\n\n            # Calcular loss\n            batch_loss = self.compute_loss(y_batch, y_pred)\n            epoch_loss += batch_loss\n            n_batches += 1\n\n            # Backward pass\n            grads = self.backward(X_batch, y_batch, cache)\n\n            # Update weights\n            self.update_weights(grads)\n\n        return epoch_loss / n_batches\n\n    def predict(self, X):\n        \\\"\\\"\\\"Faz predi\u00e7\u00f5es para novos dados.\\\"\\\"\\\"\n        y_pred, _ = self.forward(X)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64,\n            early_stopping_patience=15, verbose=True):\n        \\\"\\\"\\\"\n        Treina o modelo.\n\n        Par\u00e2metros:\n        -----------\n        X_train, y_train : arrays\n            Dados de treinamento\n        X_val, y_val : arrays\n            Dados de valida\u00e7\u00e3o\n        epochs : int\n            N\u00famero m\u00e1ximo de \u00e9pocas\n        batch_size : int\n            Tamanho do mini-batch\n        early_stopping_patience : int\n            N\u00famero de \u00e9pocas sem melhora antes de parar\n        verbose : bool\n            Imprimir progresso\n        \\\"\\\"\\\"\n        best_val_loss = float('inf')\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            # Treinar uma \u00e9poca\n            train_loss = self.train_epoch(X_train, y_train, batch_size)\n\n            # Calcular loss de valida\u00e7\u00e3o\n            y_val_pred, _ = self.forward(X_val)\n            val_loss = self.compute_loss(y_val, y_val_pred)\n\n            # Salvar hist\u00f3rico\n            self.train_loss_history.append(train_loss)\n            self.val_loss_history.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                # Salvar melhores pesos\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_counter += 1\n\n            # Imprimir progresso\n            if verbose and (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | \"\n                      f\"Best Val: {best_val_loss:.4f}\")\n\n            # Parar se n\u00e3o houver melhora\n            if patience_counter &gt;= early_stopping_patience:\n                if verbose:\n                    print(f\"\\\\nEarly stopping na \u00e9poca {epoch+1}\")\n                    print(f\"Melhor val loss: {best_val_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        if hasattr(self, 'best_weights'):\n            self.weights = self.best_weights\n            self.biases = self.best_biases\n            if verbose:\n                print(\"Melhores pesos restaurados\")\n\nprint(\"Classe MLPRegressor implementada com sucesso!\")\nprint(\"\\\\nFuncionalidades implementadas:\")\nprint(\"  - Forward propagation\")\nprint(\"  - Backpropagation\")\nprint(\"  - Mini-batch gradient descent\")\nprint(\"  - He initialization\")\nprint(\"  - ReLU activation\")\nprint(\"  - MSE loss\")\nprint(\"  - L2 regularization\")\nprint(\"  - Early stopping\")\"\"\"\n</pre> mlp_code_part3 = \"\"\"    def train_epoch(self, X_train, y_train, batch_size=64):         \\\"\\\"\\\"         Treina por uma \u00e9poca usando mini-batch gradient descent.          Retorna:         --------         epoch_loss : float             Loss m\u00e9dia da \u00e9poca         \\\"\\\"\\\"         n_samples = X_train.shape[0]         indices = np.arange(n_samples)         np.random.shuffle(indices)          epoch_loss = 0         n_batches = 0          # Mini-batch training         for start_idx in range(0, n_samples, batch_size):             end_idx = min(start_idx + batch_size, n_samples)             batch_indices = indices[start_idx:end_idx]              X_batch = X_train[batch_indices]             y_batch = y_train[batch_indices]              # Forward pass             y_pred, cache = self.forward(X_batch)              # Calcular loss             batch_loss = self.compute_loss(y_batch, y_pred)             epoch_loss += batch_loss             n_batches += 1              # Backward pass             grads = self.backward(X_batch, y_batch, cache)              # Update weights             self.update_weights(grads)          return epoch_loss / n_batches      def predict(self, X):         \\\"\\\"\\\"Faz predi\u00e7\u00f5es para novos dados.\\\"\\\"\\\"         y_pred, _ = self.forward(X)         return y_pred      def fit(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64,             early_stopping_patience=15, verbose=True):         \\\"\\\"\\\"         Treina o modelo.          Par\u00e2metros:         -----------         X_train, y_train : arrays             Dados de treinamento         X_val, y_val : arrays             Dados de valida\u00e7\u00e3o         epochs : int             N\u00famero m\u00e1ximo de \u00e9pocas         batch_size : int             Tamanho do mini-batch         early_stopping_patience : int             N\u00famero de \u00e9pocas sem melhora antes de parar         verbose : bool             Imprimir progresso         \\\"\\\"\\\"         best_val_loss = float('inf')         patience_counter = 0          for epoch in range(epochs):             # Treinar uma \u00e9poca             train_loss = self.train_epoch(X_train, y_train, batch_size)              # Calcular loss de valida\u00e7\u00e3o             y_val_pred, _ = self.forward(X_val)             val_loss = self.compute_loss(y_val, y_val_pred)              # Salvar hist\u00f3rico             self.train_loss_history.append(train_loss)             self.val_loss_history.append(val_loss)              # Early stopping             if val_loss &lt; best_val_loss:                 best_val_loss = val_loss                 patience_counter = 0                 # Salvar melhores pesos                 self.best_weights = [W.copy() for W in self.weights]                 self.best_biases = [b.copy() for b in self.biases]             else:                 patience_counter += 1              # Imprimir progresso             if verbose and (epoch + 1) % 10 == 0:                 print(f\"Epoch {epoch+1:3d}/{epochs} | \"                       f\"Train Loss: {train_loss:.4f} | \"                       f\"Val Loss: {val_loss:.4f} | \"                       f\"Best Val: {best_val_loss:.4f}\")              # Parar se n\u00e3o houver melhora             if patience_counter &gt;= early_stopping_patience:                 if verbose:                     print(f\"\\\\nEarly stopping na \u00e9poca {epoch+1}\")                     print(f\"Melhor val loss: {best_val_loss:.4f}\")                 break          # Restaurar melhores pesos         if hasattr(self, 'best_weights'):             self.weights = self.best_weights             self.biases = self.best_biases             if verbose:                 print(\"Melhores pesos restaurados\")  print(\"Classe MLPRegressor implementada com sucesso!\") print(\"\\\\nFuncionalidades implementadas:\") print(\"  - Forward propagation\") print(\"  - Backpropagation\") print(\"  - Mini-batch gradient descent\") print(\"  - He initialization\") print(\"  - ReLU activation\") print(\"  - MSE loss\") print(\"  - L2 regularization\") print(\"  - Early stopping\")\"\"\" In\u00a0[\u00a0]: Copied! <pre># Adicionar as 3 partes do c\u00f3digo MLP\nadd_code_cell(mlp_code_part1)\nadd_code_cell(mlp_code_part2)\nadd_code_cell(mlp_code_part3)\n</pre> # Adicionar as 3 partes do c\u00f3digo MLP add_code_cell(mlp_code_part1) add_code_cell(mlp_code_part2) add_code_cell(mlp_code_part3) In\u00a0[\u00a0]: Copied! <pre># Salvar\nwith open(notebook_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> # Salvar with open(notebook_path, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas adicionadas! Total agora: {len(notebook['cells'])}\")\n</pre> print(f\"C\u00e9lulas adicionadas! Total agora: {len(notebook['cells'])}\")"},{"location":"portfolio/neural-networks/projects/2/add_mlp_and_training/#secao-6-divisao-trainvaltest","title":"============================================================================== SE\u00c7\u00c3O 6: Divis\u00e3o Train/Val/Test\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_mlp_and_training/#secao-7-implementacao-do-mlp","title":"============================================================================== SE\u00c7\u00c3O 7: Implementa\u00e7\u00e3o do MLP\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/add_remaining_cells/","title":"Add remaining cells","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para adicionar c\u00e9lulas restantes ao notebook de regress\u00e3o.\nExecute AP\u00d3S create_regression_notebook.py\n\"\"\"\n</pre> \"\"\" Script para adicionar c\u00e9lulas restantes ao notebook de regress\u00e3o. Execute AP\u00d3S create_regression_notebook.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre># Carregar notebook existente\nnotebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> # Carregar notebook existente notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n</pre> with open(notebook_path, 'r', encoding='utf-8') as f:     notebook = json.load(f) In\u00a0[\u00a0]: Copied! <pre># Helper functions\ndef add_markdown_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": content.split(\"\\n\")\n    })\n</pre> # Helper functions def add_markdown_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_code_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\")\nprint(\"Adicionando c\u00e9lulas restantes...\")\n</pre> print(f\"C\u00e9lulas existentes: {len(notebook['cells'])}\") print(\"Adicionando c\u00e9lulas restantes...\") In\u00a0[\u00a0]: Copied! <pre># An\u00e1lise temporal e sazonal\nadd_code_cell(\"\"\"# An\u00e1lise temporal e sazonal\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n# 1. Demanda por hora do dia\nhourly_avg = df.groupby('hr')['cnt'].mean()\naxes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=6)\naxes[0, 0].set_xlabel('Hora do Dia', fontsize=11)\naxes[0, 0].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11)\naxes[0, 0].set_title('Demanda M\u00e9dia por Hora do Dia', fontsize=13, fontweight='bold')\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_xticks(range(0, 24, 2))\n\n# 2. Demanda por esta\u00e7\u00e3o\nseason_names = {1: 'Primavera', 2: 'Ver\u00e3o', 3: 'Outono', 4: 'Inverno'}\nseason_avg = df.groupby('season')['cnt'].mean()\naxes[0, 1].bar([season_names[i] for i in season_avg.index], season_avg.values,\n               color=['lightgreen', 'yellow', 'orange', 'lightblue'], edgecolor='black')\naxes[0, 1].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11)\naxes[0, 1].set_title('Demanda M\u00e9dia por Esta\u00e7\u00e3o', fontsize=13, fontweight='bold')\naxes[0, 1].grid(True, alpha=0.3, axis='y')\n\n# 3. Demanda por condi\u00e7\u00e3o clim\u00e1tica\nweather_names = {1: 'Claro', 2: 'Nublado', 3: 'Chuva Leve', 4: 'Chuva Forte'}\nweather_avg = df.groupby('weathersit')['cnt'].mean()\naxes[1, 0].bar([weather_names.get(i, str(i)) for i in weather_avg.index], weather_avg.values,\n               color=['gold', 'lightgray', 'lightblue', 'darkblue'], edgecolor='black')\naxes[1, 0].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11)\naxes[1, 0].set_title('Demanda M\u00e9dia por Condi\u00e7\u00e3o Clim\u00e1tica', fontsize=13, fontweight='bold')\naxes[1, 0].grid(True, alpha=0.3, axis='y')\naxes[1, 0].tick_params(axis='x', rotation=15)\n\n# 4. Temperatura vs Demanda\naxes[1, 1].scatter(df['temp'], df['cnt'], alpha=0.3, s=10)\naxes[1, 1].set_xlabel('Temperatura Normalizada', fontsize=11)\naxes[1, 1].set_ylabel('Contagem de Alugu\u00e9is', fontsize=11)\naxes[1, 1].set_title('Rela\u00e7\u00e3o entre Temperatura e Demanda', fontsize=13, fontweight='bold')\naxes[1, 1].grid(True, alpha=0.3)\n\n# Linha de tend\u00eancia\nz = np.polyfit(df['temp'], df['cnt'], 2)\np = np.poly1d(z)\ntemp_range = np.linspace(df['temp'].min(), df['temp'].max(), 100)\naxes[1, 1].plot(temp_range, p(temp_range), \"r-\", linewidth=2, label='Tend\u00eancia (polinomial)')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\"\"\")\n</pre> # An\u00e1lise temporal e sazonal add_code_cell(\"\"\"# An\u00e1lise temporal e sazonal fig, axes = plt.subplots(2, 2, figsize=(16, 12))  # 1. Demanda por hora do dia hourly_avg = df.groupby('hr')['cnt'].mean() axes[0, 0].plot(hourly_avg.index, hourly_avg.values, marker='o', linewidth=2, markersize=6) axes[0, 0].set_xlabel('Hora do Dia', fontsize=11) axes[0, 0].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11) axes[0, 0].set_title('Demanda M\u00e9dia por Hora do Dia', fontsize=13, fontweight='bold') axes[0, 0].grid(True, alpha=0.3) axes[0, 0].set_xticks(range(0, 24, 2))  # 2. Demanda por esta\u00e7\u00e3o season_names = {1: 'Primavera', 2: 'Ver\u00e3o', 3: 'Outono', 4: 'Inverno'} season_avg = df.groupby('season')['cnt'].mean() axes[0, 1].bar([season_names[i] for i in season_avg.index], season_avg.values,                color=['lightgreen', 'yellow', 'orange', 'lightblue'], edgecolor='black') axes[0, 1].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11) axes[0, 1].set_title('Demanda M\u00e9dia por Esta\u00e7\u00e3o', fontsize=13, fontweight='bold') axes[0, 1].grid(True, alpha=0.3, axis='y')  # 3. Demanda por condi\u00e7\u00e3o clim\u00e1tica weather_names = {1: 'Claro', 2: 'Nublado', 3: 'Chuva Leve', 4: 'Chuva Forte'} weather_avg = df.groupby('weathersit')['cnt'].mean() axes[1, 0].bar([weather_names.get(i, str(i)) for i in weather_avg.index], weather_avg.values,                color=['gold', 'lightgray', 'lightblue', 'darkblue'], edgecolor='black') axes[1, 0].set_ylabel('M\u00e9dia de Alugu\u00e9is', fontsize=11) axes[1, 0].set_title('Demanda M\u00e9dia por Condi\u00e7\u00e3o Clim\u00e1tica', fontsize=13, fontweight='bold') axes[1, 0].grid(True, alpha=0.3, axis='y') axes[1, 0].tick_params(axis='x', rotation=15)  # 4. Temperatura vs Demanda axes[1, 1].scatter(df['temp'], df['cnt'], alpha=0.3, s=10) axes[1, 1].set_xlabel('Temperatura Normalizada', fontsize=11) axes[1, 1].set_ylabel('Contagem de Alugu\u00e9is', fontsize=11) axes[1, 1].set_title('Rela\u00e7\u00e3o entre Temperatura e Demanda', fontsize=13, fontweight='bold') axes[1, 1].grid(True, alpha=0.3)  # Linha de tend\u00eancia z = np.polyfit(df['temp'], df['cnt'], 2) p = np.poly1d(z) temp_range = np.linspace(df['temp'].min(), df['temp'].max(), 100) axes[1, 1].plot(temp_range, p(temp_range), \"r-\", linewidth=2, label='Tend\u00eancia (polinomial)') axes[1, 1].legend()  plt.tight_layout() plt.show()\"\"\") In\u00a0[\u00a0]: Copied! <pre># Se\u00e7\u00e3o 5: Limpeza e Normaliza\u00e7\u00e3o\nadd_markdown_cell(\"\"\"## 5. Limpeza e Normaliza\u00e7\u00e3o dos Dados\n\n### Estrat\u00e9gia de Pr\u00e9-processamento:\n\n1. **Remo\u00e7\u00e3o de colunas irrelevantes**:\n   - `instant`: \u00edndice sequencial sem valor preditivo\n   - `dteday`: data j\u00e1 representada por outras features temporais\n   - `casual` e `registered`: componentes do target (evitar data leakage)\n\n2. **Tratamento de valores ausentes**:\n   - Dataset n\u00e3o possui valores ausentes (verificado anteriormente)\n\n3. **Tratamento de outliers**:\n   - Mantidos pois representam varia\u00e7\u00e3o real do sistema\n   - Outliers podem ser eventos especiais (feriados, clima extremo)\n\n4. **Encoding de vari\u00e1veis categ\u00f3ricas**:\n   - Vari\u00e1veis ordinais (`season`, `weathersit`): manter valores num\u00e9ricos (j\u00e1 possuem ordem natural)\n   - Vari\u00e1veis c\u00edclicas (`hr`, `mnth`, `weekday`): aplicar transforma\u00e7\u00e3o seno/cosseno para capturar ciclicidade\n\n5. **Normaliza\u00e7\u00e3o**:\n   - Z-score standardization (m\u00e9dia=0, desvio=1) para todas as features\n   - Raz\u00e3o: MLP converge mais r\u00e1pido com features na mesma escala\n   - Target tamb\u00e9m ser\u00e1 normalizado para facilitar treinamento\"\"\")\n</pre> # Se\u00e7\u00e3o 5: Limpeza e Normaliza\u00e7\u00e3o add_markdown_cell(\"\"\"## 5. Limpeza e Normaliza\u00e7\u00e3o dos Dados  ### Estrat\u00e9gia de Pr\u00e9-processamento:  1. **Remo\u00e7\u00e3o de colunas irrelevantes**:    - `instant`: \u00edndice sequencial sem valor preditivo    - `dteday`: data j\u00e1 representada por outras features temporais    - `casual` e `registered`: componentes do target (evitar data leakage)  2. **Tratamento de valores ausentes**:    - Dataset n\u00e3o possui valores ausentes (verificado anteriormente)  3. **Tratamento de outliers**:    - Mantidos pois representam varia\u00e7\u00e3o real do sistema    - Outliers podem ser eventos especiais (feriados, clima extremo)  4. **Encoding de vari\u00e1veis categ\u00f3ricas**:    - Vari\u00e1veis ordinais (`season`, `weathersit`): manter valores num\u00e9ricos (j\u00e1 possuem ordem natural)    - Vari\u00e1veis c\u00edclicas (`hr`, `mnth`, `weekday`): aplicar transforma\u00e7\u00e3o seno/cosseno para capturar ciclicidade  5. **Normaliza\u00e7\u00e3o**:    - Z-score standardization (m\u00e9dia=0, desvio=1) para todas as features    - Raz\u00e3o: MLP converge mais r\u00e1pido com features na mesma escala    - Target tamb\u00e9m ser\u00e1 normalizado para facilitar treinamento\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"ETAPA 1: PREPARA\u00c7\u00c3O INICIAL\")\nprint(\"=\"*80)\n\n# Criar c\u00f3pia para preservar dados originais\ndf_processed = df.copy()\n\n# Remover colunas irrelevantes\ncolumns_to_drop = ['instant', 'dteday', 'casual', 'registered']\ndf_processed = df_processed.drop(columns=columns_to_drop)\n\nprint(f\"Colunas removidas: {columns_to_drop}\")\nprint(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_processed.shape}\")\nprint(f\"\\\\nColunas restantes: {list(df_processed.columns)}\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"ETAPA 1: PREPARA\u00c7\u00c3O INICIAL\") print(\"=\"*80)  # Criar c\u00f3pia para preservar dados originais df_processed = df.copy()  # Remover colunas irrelevantes columns_to_drop = ['instant', 'dteday', 'casual', 'registered'] df_processed = df_processed.drop(columns=columns_to_drop)  print(f\"Colunas removidas: {columns_to_drop}\") print(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_processed.shape}\") print(f\"\\\\nColunas restantes: {list(df_processed.columns)}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"ETAPA 2: FEATURE ENGINEERING - VARI\u00c1VEIS C\u00cdCLICAS\")\nprint(\"=\"*80)\nprint(\"Transformando vari\u00e1veis c\u00edclicas (hora, m\u00eas, dia da semana) em sin/cos\")\nprint(\"Raz\u00e3o: Capturar a natureza c\u00edclica (ex: hora 23 est\u00e1 pr\u00f3xima da hora 0)\\\\n\")\n\n# Fun\u00e7\u00e3o para criar features c\u00edclicas\ndef encode_cyclical_feature(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n    return data\n\n# Aplicar transforma\u00e7\u00e3o c\u00edclica\ndf_processed = encode_cyclical_feature(df_processed, 'hr', 24)\ndf_processed = encode_cyclical_feature(df_processed, 'mnth', 12)\ndf_processed = encode_cyclical_feature(df_processed, 'weekday', 7)\n\nprint(f\"Features c\u00edclicas criadas:\")\nprint(f\"  - hr_sin, hr_cos (hora do dia)\")\nprint(f\"  - mnth_sin, mnth_cos (m\u00eas)\")\nprint(f\"  - weekday_sin, weekday_cos (dia da semana)\")\n\n# Remover features originais (mantemos apenas sin/cos)\ndf_processed = df_processed.drop(columns=['hr', 'mnth', 'weekday'])\n\nprint(f\"\\\\nFeatures originais removidas (substitu\u00eddas por sin/cos)\")\nprint(f\"Shape ap\u00f3s feature engineering: {df_processed.shape}\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"ETAPA 2: FEATURE ENGINEERING - VARI\u00c1VEIS C\u00cdCLICAS\") print(\"=\"*80) print(\"Transformando vari\u00e1veis c\u00edclicas (hora, m\u00eas, dia da semana) em sin/cos\") print(\"Raz\u00e3o: Capturar a natureza c\u00edclica (ex: hora 23 est\u00e1 pr\u00f3xima da hora 0)\\\\n\")  # Fun\u00e7\u00e3o para criar features c\u00edclicas def encode_cyclical_feature(data, col, max_val):     data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)     data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)     return data  # Aplicar transforma\u00e7\u00e3o c\u00edclica df_processed = encode_cyclical_feature(df_processed, 'hr', 24) df_processed = encode_cyclical_feature(df_processed, 'mnth', 12) df_processed = encode_cyclical_feature(df_processed, 'weekday', 7)  print(f\"Features c\u00edclicas criadas:\") print(f\"  - hr_sin, hr_cos (hora do dia)\") print(f\"  - mnth_sin, mnth_cos (m\u00eas)\") print(f\"  - weekday_sin, weekday_cos (dia da semana)\")  # Remover features originais (mantemos apenas sin/cos) df_processed = df_processed.drop(columns=['hr', 'mnth', 'weekday'])  print(f\"\\\\nFeatures originais removidas (substitu\u00eddas por sin/cos)\") print(f\"Shape ap\u00f3s feature engineering: {df_processed.shape}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"ETAPA 3: SEPARA\u00c7\u00c3O DE FEATURES E TARGET\")\nprint(\"=\"*80)\n\n# Separar features (X) e target (y)\nX = df_processed.drop(columns=['cnt']).values\ny = df_processed['cnt'].values.reshape(-1, 1)\n\nfeature_names = df_processed.drop(columns=['cnt']).columns.tolist()\n\nprint(f\"Features (X): shape = {X.shape}\")\nprint(f\"Target (y): shape = {y.shape}\")\nprint(f\"\\\\nLista de features ({len(feature_names)}):\")\nfor i, name in enumerate(feature_names, 1):\n    print(f\"  {i:2d}. {name}\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"ETAPA 3: SEPARA\u00c7\u00c3O DE FEATURES E TARGET\") print(\"=\"*80)  # Separar features (X) e target (y) X = df_processed.drop(columns=['cnt']).values y = df_processed['cnt'].values.reshape(-1, 1)  feature_names = df_processed.drop(columns=['cnt']).columns.tolist()  print(f\"Features (X): shape = {X.shape}\") print(f\"Target (y): shape = {y.shape}\") print(f\"\\\\nLista de features ({len(feature_names)}):\") for i, name in enumerate(feature_names, 1):     print(f\"  {i:2d}. {name}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"print(\"ETAPA 4: NORMALIZA\u00c7\u00c3O (Z-SCORE STANDARDIZATION)\")\nprint(\"=\"*80)\nprint(\"Normalizando features e target usando z-score: (x - \u03bc) / \u03c3\")\nprint(\"Raz\u00e3o: MLP converge mais r\u00e1pido com features na mesma escala\\\\n\")\n\n# Salvar estat\u00edsticas ANTES da normaliza\u00e7\u00e3o (para desnormaliza\u00e7\u00e3o posterior)\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\ny_mean = y.mean()\ny_std = y.std()\n\nprint(f\"Estat\u00edsticas do Target (ANTES da normaliza\u00e7\u00e3o):\")\nprint(f\"  M\u00e9dia: {y_mean:.2f}\")\nprint(f\"  Desvio padr\u00e3o: {y_std:.2f}\")\nprint(f\"  Min: {y.min():.2f}\")\nprint(f\"  Max: {y.max():.2f}\")\n\n# Normalizar features\nX_normalized = (X - X_mean) / (X_std + 1e-8)  # +epsilon para evitar divis\u00e3o por zero\n\n# Normalizar target\ny_normalized = (y - y_mean) / y_std\n\nprint(f\"\\\\nFeatures normalizadas: shape = {X_normalized.shape}\")\nprint(f\"Target normalizado: shape = {y_normalized.shape}\")\n\nprint(f\"\\\\nEstat\u00edsticas do Target (DEPOIS da normaliza\u00e7\u00e3o):\")\nprint(f\"  M\u00e9dia: {y_normalized.mean():.6f} (\u2248 0)\")\nprint(f\"  Desvio padr\u00e3o: {y_normalized.std():.6f} (\u2248 1)\")\nprint(f\"  Min: {y_normalized.min():.2f}\")\nprint(f\"  Max: {y_normalized.max():.2f}\")\"\"\")\n</pre> add_code_cell(\"\"\"print(\"ETAPA 4: NORMALIZA\u00c7\u00c3O (Z-SCORE STANDARDIZATION)\") print(\"=\"*80) print(\"Normalizando features e target usando z-score: (x - \u03bc) / \u03c3\") print(\"Raz\u00e3o: MLP converge mais r\u00e1pido com features na mesma escala\\\\n\")  # Salvar estat\u00edsticas ANTES da normaliza\u00e7\u00e3o (para desnormaliza\u00e7\u00e3o posterior) X_mean = X.mean(axis=0) X_std = X.std(axis=0) y_mean = y.mean() y_std = y.std()  print(f\"Estat\u00edsticas do Target (ANTES da normaliza\u00e7\u00e3o):\") print(f\"  M\u00e9dia: {y_mean:.2f}\") print(f\"  Desvio padr\u00e3o: {y_std:.2f}\") print(f\"  Min: {y.min():.2f}\") print(f\"  Max: {y.max():.2f}\")  # Normalizar features X_normalized = (X - X_mean) / (X_std + 1e-8)  # +epsilon para evitar divis\u00e3o por zero  # Normalizar target y_normalized = (y - y_mean) / y_std  print(f\"\\\\nFeatures normalizadas: shape = {X_normalized.shape}\") print(f\"Target normalizado: shape = {y_normalized.shape}\")  print(f\"\\\\nEstat\u00edsticas do Target (DEPOIS da normaliza\u00e7\u00e3o):\") print(f\"  M\u00e9dia: {y_normalized.mean():.6f} (\u2248 0)\") print(f\"  Desvio padr\u00e3o: {y_normalized.std():.6f} (\u2248 1)\") print(f\"  Min: {y_normalized.min():.2f}\") print(f\"  Max: {y_normalized.max():.2f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Visualiza\u00e7\u00e3o: Antes vs Depois da normaliza\u00e7\u00e3o\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Antes\naxes[0].hist(y, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].set_xlabel('Contagem de Alugu\u00e9is', fontsize=12)\naxes[0].set_ylabel('Frequ\u00eancia', fontsize=12)\naxes[0].set_title('Target ANTES da Normaliza\u00e7\u00e3o', fontsize=14, fontweight='bold')\naxes[0].axvline(y_mean, color='red', linestyle='--', linewidth=2, label=f'M\u00e9dia: {y_mean:.1f}')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\n# Depois\naxes[1].hist(y_normalized, bins=50, edgecolor='black', alpha=0.7, color='coral')\naxes[1].set_xlabel('Contagem Normalizada', fontsize=12)\naxes[1].set_ylabel('Frequ\u00eancia', fontsize=12)\naxes[1].set_title('Target DEPOIS da Normaliza\u00e7\u00e3o', fontsize=14, fontweight='bold')\naxes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='M\u00e9dia: 0.0')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\"\"\")\n</pre> add_code_cell(\"\"\"# Visualiza\u00e7\u00e3o: Antes vs Depois da normaliza\u00e7\u00e3o fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Antes axes[0].hist(y, bins=50, edgecolor='black', alpha=0.7, color='steelblue') axes[0].set_xlabel('Contagem de Alugu\u00e9is', fontsize=12) axes[0].set_ylabel('Frequ\u00eancia', fontsize=12) axes[0].set_title('Target ANTES da Normaliza\u00e7\u00e3o', fontsize=14, fontweight='bold') axes[0].axvline(y_mean, color='red', linestyle='--', linewidth=2, label=f'M\u00e9dia: {y_mean:.1f}') axes[0].legend(fontsize=11) axes[0].grid(True, alpha=0.3)  # Depois axes[1].hist(y_normalized, bins=50, edgecolor='black', alpha=0.7, color='coral') axes[1].set_xlabel('Contagem Normalizada', fontsize=12) axes[1].set_ylabel('Frequ\u00eancia', fontsize=12) axes[1].set_title('Target DEPOIS da Normaliza\u00e7\u00e3o', fontsize=14, fontweight='bold') axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='M\u00e9dia: 0.0') axes[1].legend(fontsize=11) axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show()\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_markdown_cell(\"\"\"### Resumo do Pr\u00e9-processamento\n\n\u2713 **Colunas removidas:** `instant`, `dteday`, `casual`, `registered`\n\u2713 **Feature engineering:** Vari\u00e1veis c\u00edclicas transformadas em sin/cos\n\u2713 **Valores ausentes:** Nenhum encontrado\n\u2713 **Outliers:** Mantidos (representam varia\u00e7\u00e3o real)\n\u2713 **Normaliza\u00e7\u00e3o:** Z-score standardization aplicada\n\u2713 **Features finais:** 15 vari\u00e1veis\n\u2713 **Amostras:** 17,379\"\"\")\n</pre> add_markdown_cell(\"\"\"### Resumo do Pr\u00e9-processamento  \u2713 **Colunas removidas:** `instant`, `dteday`, `casual`, `registered` \u2713 **Feature engineering:** Vari\u00e1veis c\u00edclicas transformadas em sin/cos \u2713 **Valores ausentes:** Nenhum encontrado \u2713 **Outliers:** Mantidos (representam varia\u00e7\u00e3o real) \u2713 **Normaliza\u00e7\u00e3o:** Z-score standardization aplicada \u2713 **Features finais:** 15 vari\u00e1veis \u2713 **Amostras:** 17,379\"\"\") In\u00a0[\u00a0]: Copied! <pre># Continua no pr\u00f3ximo bloco...\nprint(\"C\u00e9lulas adicionadas at\u00e9 se\u00e7\u00e3o 5 (normaliza\u00e7\u00e3o)...\")\n</pre> # Continua no pr\u00f3ximo bloco... print(\"C\u00e9lulas adicionadas at\u00e9 se\u00e7\u00e3o 5 (normaliza\u00e7\u00e3o)...\") In\u00a0[\u00a0]: Copied! <pre># Salvar parcialmente\nwith open(notebook_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> # Salvar parcialmente with open(notebook_path, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"Total de c\u00e9lulas agora: {len(notebook['cells'])}\")\n</pre> print(f\"Total de c\u00e9lulas agora: {len(notebook['cells'])}\")"},{"location":"portfolio/neural-networks/projects/2/add_remaining_cells/#celulas-adicionais","title":"============================================================================== C\u00c9LULAS ADICIONAIS\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/create_final_notebook/","title":"Create final notebook","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nNotebook final seguindo os padr\u00f5es da disciplina ANN/DL Insper.\nBaseado nos requisitos oficiais do projeto de regress\u00e3o.\n\"\"\"\n</pre> \"\"\" Notebook final seguindo os padr\u00f5es da disciplina ANN/DL Insper. Baseado nos requisitos oficiais do projeto de regress\u00e3o. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre>notebook = {\n    \"cells\": [],\n    \"metadata\": {\n        \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n        \"language_info\": {\"name\": \"python\", \"version\": \"3.8.0\"}\n    },\n    \"nbformat\": 4,\n    \"nbformat_minor\": 4\n}\n</pre> notebook = {     \"cells\": [],     \"metadata\": {         \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},         \"language_info\": {\"name\": \"python\", \"version\": \"3.8.0\"}     },     \"nbformat\": 4,     \"nbformat_minor\": 4 } In\u00a0[\u00a0]: Copied! <pre>def md(text):\n    notebook[\"cells\"].append({\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": text.split(\"\\n\")})\n</pre> def md(text):     notebook[\"cells\"].append({\"cell_type\": \"markdown\", \"metadata\": {}, \"source\": text.split(\"\\n\")}) In\u00a0[\u00a0]: Copied! <pre>def code(text):\n    notebook[\"cells\"].append({\"cell_type\": \"code\", \"execution_count\": None, \"metadata\": {}, \"outputs\": [], \"source\": text.split(\"\\n\")})\n</pre> def code(text):     notebook[\"cells\"].append({\"cell_type\": \"code\", \"execution_count\": None, \"metadata\": {}, \"outputs\": [], \"source\": text.split(\"\\n\")}) In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"# Projeto de Regress\u00e3o: Bike Sharing Demand\n\n**Aluno:** Pedro Civita\n**Disciplina:** Redes Neurais e Deep Learning\n**Dataset:** Bike Sharing (UCI ML Repository)\n\n## Objetivo\n\nImplementar um Multi-Layer Perceptron (MLP) para prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.\"\"\")\n</pre> md(\"\"\"# Projeto de Regress\u00e3o: Bike Sharing Demand  **Aluno:** Pedro Civita **Disciplina:** Redes Neurais e Deep Learning **Dataset:** Bike Sharing (UCI ML Repository)  ## Objetivo  Implementar um Multi-Layer Perceptron (MLP) para prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 1. Sele\u00e7\u00e3o do Dataset\n\n### Dataset: Bike Sharing Demand\n\n**Fonte:** [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)\n\n**Caracter\u00edsticas:**\n- **Amostras:** 17,379 registros (hor\u00e1rios de 2011-2012)\n- **Features:** 16 vari\u00e1veis (clima, tempo, sazonalidade)\n- **Target:** `cnt` - contagem total de alugu\u00e9is (regress\u00e3o)\n\n**Motiva\u00e7\u00e3o:**\n- Problema real de otimiza\u00e7\u00e3o urbana\n- Dataset n\u00e3o-trivial com padr\u00f5es temporais complexos\n- Evita datasets cl\u00e1ssicos (Boston/California Housing)\n- Dispon\u00edvel em competi\u00e7\u00e3o Kaggle (possibilidade de bonus)\"\"\")\n</pre> md(\"\"\"## 1. Sele\u00e7\u00e3o do Dataset  ### Dataset: Bike Sharing Demand  **Fonte:** [UCI ML Repository](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)  **Caracter\u00edsticas:** - **Amostras:** 17,379 registros (hor\u00e1rios de 2011-2012) - **Features:** 16 vari\u00e1veis (clima, tempo, sazonalidade) - **Target:** `cnt` - contagem total de alugu\u00e9is (regress\u00e3o)  **Motiva\u00e7\u00e3o:** - Problema real de otimiza\u00e7\u00e3o urbana - Dataset n\u00e3o-trivial com padr\u00f5es temporais complexos - Evita datasets cl\u00e1ssicos (Boston/California Housing) - Dispon\u00edvel em competi\u00e7\u00e3o Kaggle (possibilidade de bonus)\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura\u00e7\u00e3o\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Setup completo!\")\"\"\")\n</pre> code(\"\"\"# Imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from datetime import datetime import warnings warnings.filterwarnings('ignore')  # Configura\u00e7\u00e3o np.random.seed(42) plt.style.use('seaborn-v0_8-darkgrid') sns.set_palette(\"husl\")  print(\"Setup completo!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Carregar dataset\ndf = pd.read_csv('hour.csv')\n\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\\\nPrimeiras linhas:\")\ndf.head()\"\"\")\n</pre> code(\"\"\"# Carregar dataset df = pd.read_csv('hour.csv')  print(f\"Shape: {df.shape}\") print(f\"\\\\nPrimeiras linhas:\") df.head()\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Informa\u00e7\u00f5es do dataset\nprint(\"Informa\u00e7\u00f5es Gerais:\")\nprint(\"=\"*70)\ndf.info()\n\nprint(\"\\\\n\" + \"=\"*70)\nprint(\"Estat\u00edsticas Descritivas:\")\nprint(\"=\"*70)\ndf.describe()\"\"\")\n</pre> code(\"\"\"# Informa\u00e7\u00f5es do dataset print(\"Informa\u00e7\u00f5es Gerais:\") print(\"=\"*70) df.info()  print(\"\\\\n\" + \"=\"*70) print(\"Estat\u00edsticas Descritivas:\") print(\"=\"*70) df.describe()\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 2. An\u00e1lise Explorat\u00f3ria\n\nAn\u00e1lise inicial para entender padr\u00f5es e rela\u00e7\u00f5es nos dados.\"\"\")\n</pre> md(\"\"\"## 2. An\u00e1lise Explorat\u00f3ria  An\u00e1lise inicial para entender padr\u00f5es e rela\u00e7\u00f5es nos dados.\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Verificar valores ausentes\nprint(\"Valores Ausentes:\")\nprint(df.isnull().sum())\n\n# An\u00e1lise da vari\u00e1vel target\nprint(f\"\\\\nTarget (cnt) - Estat\u00edsticas:\")\nprint(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\")\nprint(f\"  Mediana: {df['cnt'].median():.2f}\")\nprint(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\")\nprint(f\"  Min: {df['cnt'].min()}\")\nprint(f\"  Max: {df['cnt'].max()}\")\"\"\")\n</pre> code(\"\"\"# Verificar valores ausentes print(\"Valores Ausentes:\") print(df.isnull().sum())  # An\u00e1lise da vari\u00e1vel target print(f\"\\\\nTarget (cnt) - Estat\u00edsticas:\") print(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\") print(f\"  Mediana: {df['cnt'].median():.2f}\") print(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\") print(f\"  Min: {df['cnt'].min()}\") print(f\"  Max: {df['cnt'].max()}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Visualiza\u00e7\u00f5es explorat\u00f3rias\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Distribui\u00e7\u00e3o do target\naxes[0,0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7)\naxes[0,0].set_title('Distribui\u00e7\u00e3o da Demanda')\naxes[0,0].set_xlabel('N\u00famero de alugu\u00e9is')\naxes[0,0].set_ylabel('Frequ\u00eancia')\naxes[0,0].grid(True, alpha=0.3)\n\n# Demanda por hora\nhourly = df.groupby('hr')['cnt'].mean()\naxes[0,1].plot(hourly.index, hourly.values, marker='o', linewidth=2)\naxes[0,1].set_title('Demanda M\u00e9dia por Hora')\naxes[0,1].set_xlabel('Hora do dia')\naxes[0,1].set_ylabel('Alugu\u00e9is m\u00e9dios')\naxes[0,1].grid(True, alpha=0.3)\n\n# Temperatura vs Demanda\naxes[1,0].scatter(df['temp'], df['cnt'], alpha=0.3, s=10)\naxes[1,0].set_title('Temperatura vs Demanda')\naxes[1,0].set_xlabel('Temperatura normalizada')\naxes[1,0].set_ylabel('Alugu\u00e9is')\naxes[1,0].grid(True, alpha=0.3)\n\n# Correla\u00e7\u00e3o\ncorr_cols = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\ncorr = df[corr_cols].corr()\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n            ax=axes[1,1], square=True, linewidths=1)\naxes[1,1].set_title('Matriz de Correla\u00e7\u00e3o')\n\nplt.tight_layout()\nplt.show()\"\"\")\n</pre> code(\"\"\"# Visualiza\u00e7\u00f5es explorat\u00f3rias fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # Distribui\u00e7\u00e3o do target axes[0,0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7) axes[0,0].set_title('Distribui\u00e7\u00e3o da Demanda') axes[0,0].set_xlabel('N\u00famero de alugu\u00e9is') axes[0,0].set_ylabel('Frequ\u00eancia') axes[0,0].grid(True, alpha=0.3)  # Demanda por hora hourly = df.groupby('hr')['cnt'].mean() axes[0,1].plot(hourly.index, hourly.values, marker='o', linewidth=2) axes[0,1].set_title('Demanda M\u00e9dia por Hora') axes[0,1].set_xlabel('Hora do dia') axes[0,1].set_ylabel('Alugu\u00e9is m\u00e9dios') axes[0,1].grid(True, alpha=0.3)  # Temperatura vs Demanda axes[1,0].scatter(df['temp'], df['cnt'], alpha=0.3, s=10) axes[1,0].set_title('Temperatura vs Demanda') axes[1,0].set_xlabel('Temperatura normalizada') axes[1,0].set_ylabel('Alugu\u00e9is') axes[1,0].grid(True, alpha=0.3)  # Correla\u00e7\u00e3o corr_cols = ['temp', 'atemp', 'hum', 'windspeed', 'cnt'] corr = df[corr_cols].corr() sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,             ax=axes[1,1], square=True, linewidths=1) axes[1,1].set_title('Matriz de Correla\u00e7\u00e3o')  plt.tight_layout() plt.show()\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 3. Limpeza e Normaliza\u00e7\u00e3o dos Dados\n\n### Estrat\u00e9gia de Pr\u00e9-processamento:\n\n1. **Remo\u00e7\u00e3o de features:** `instant`, `dteday`, `casual`, `registered` (evitar data leakage)\n2. **Feature engineering:** Transforma\u00e7\u00e3o c\u00edclica para vari\u00e1veis temporais (sin/cos)\n3. **Normaliza\u00e7\u00e3o:** Z-score standardization (m\u00e9dia=0, std=1)\"\"\")\n</pre> md(\"\"\"## 3. Limpeza e Normaliza\u00e7\u00e3o dos Dados  ### Estrat\u00e9gia de Pr\u00e9-processamento:  1. **Remo\u00e7\u00e3o de features:** `instant`, `dteday`, `casual`, `registered` (evitar data leakage) 2. **Feature engineering:** Transforma\u00e7\u00e3o c\u00edclica para vari\u00e1veis temporais (sin/cos) 3. **Normaliza\u00e7\u00e3o:** Z-score standardization (m\u00e9dia=0, std=1)\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Preparar dados\ndf_prep = df.copy()\n\n# Remover colunas\ndrop_cols = ['instant', 'dteday', 'casual', 'registered']\ndf_prep = df_prep.drop(columns=drop_cols)\n\nprint(f\"Colunas removidas: {drop_cols}\")\nprint(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_prep.shape}\")\"\"\")\n</pre> code(\"\"\"# Preparar dados df_prep = df.copy()  # Remover colunas drop_cols = ['instant', 'dteday', 'casual', 'registered'] df_prep = df_prep.drop(columns=drop_cols)  print(f\"Colunas removidas: {drop_cols}\") print(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_prep.shape}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Feature engineering: vari\u00e1veis c\u00edclicas\n# Hora, m\u00eas e dia da semana s\u00e3o c\u00edclicos (ex: hora 23 pr\u00f3xima de hora 0)\ndef encode_cyclical(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n    return data\n\ndf_prep = encode_cyclical(df_prep, 'hr', 24)\ndf_prep = encode_cyclical(df_prep, 'mnth', 12)\ndf_prep = encode_cyclical(df_prep, 'weekday', 7)\n\n# Remover originais\ndf_prep = df_prep.drop(columns=['hr', 'mnth', 'weekday'])\n\nprint(f\"Features c\u00edclicas criadas!\")\nprint(f\"Shape final: {df_prep.shape}\")\nprint(f\"\\\\nFeatures: {list(df_prep.columns)}\")\"\"\")\n</pre> code(\"\"\"# Feature engineering: vari\u00e1veis c\u00edclicas # Hora, m\u00eas e dia da semana s\u00e3o c\u00edclicos (ex: hora 23 pr\u00f3xima de hora 0) def encode_cyclical(data, col, max_val):     data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)     data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)     return data  df_prep = encode_cyclical(df_prep, 'hr', 24) df_prep = encode_cyclical(df_prep, 'mnth', 12) df_prep = encode_cyclical(df_prep, 'weekday', 7)  # Remover originais df_prep = df_prep.drop(columns=['hr', 'mnth', 'weekday'])  print(f\"Features c\u00edclicas criadas!\") print(f\"Shape final: {df_prep.shape}\") print(f\"\\\\nFeatures: {list(df_prep.columns)}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Separar X e y\nX = df_prep.drop(columns=['cnt']).values\ny = df_prep['cnt'].values.reshape(-1, 1)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Salvar estat\u00edsticas para desnormaliza\u00e7\u00e3o posterior\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\ny_mean = y.mean()\ny_std = y.std()\n\n# Normalizar (z-score)\nX_norm = (X - X_mean) / (X_std + 1e-8)\ny_norm = (y - y_mean) / y_std\n\nprint(f\"\\\\nAp\u00f3s normaliza\u00e7\u00e3o:\")\nprint(f\"X: m\u00e9dia={X_norm.mean():.3f}, std={X_norm.std():.3f}\")\nprint(f\"y: m\u00e9dia={y_norm.mean():.3f}, std={y_norm.std():.3f}\")\"\"\")\n</pre> code(\"\"\"# Separar X e y X = df_prep.drop(columns=['cnt']).values y = df_prep['cnt'].values.reshape(-1, 1)  print(f\"X shape: {X.shape}\") print(f\"y shape: {y.shape}\")  # Salvar estat\u00edsticas para desnormaliza\u00e7\u00e3o posterior X_mean = X.mean(axis=0) X_std = X.std(axis=0) y_mean = y.mean() y_std = y.std()  # Normalizar (z-score) X_norm = (X - X_mean) / (X_std + 1e-8) y_norm = (y - y_mean) / y_std  print(f\"\\\\nAp\u00f3s normaliza\u00e7\u00e3o:\") print(f\"X: m\u00e9dia={X_norm.mean():.3f}, std={X_norm.std():.3f}\") print(f\"y: m\u00e9dia={y_norm.mean():.3f}, std={y_norm.std():.3f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 4. Estrat\u00e9gia de Divis\u00e3o Train/Validation/Test\n\n**Divis\u00e3o:** 70% treino / 15% valida\u00e7\u00e3o / 15% teste\n\n**Modo de treinamento:** Mini-batch gradient descent (batch_size=64)\n\n**Justificativa:**\n- 70% treino: volume suficiente para aprendizado\n- 15% valida\u00e7\u00e3o: monitorar overfitting e early stopping\n- 15% teste: avalia\u00e7\u00e3o final imparcial\n- Mini-batch: equil\u00edbrio entre velocidade e estabilidade\"\"\")\n</pre> md(\"\"\"## 4. Estrat\u00e9gia de Divis\u00e3o Train/Validation/Test  **Divis\u00e3o:** 70% treino / 15% valida\u00e7\u00e3o / 15% teste  **Modo de treinamento:** Mini-batch gradient descent (batch_size=64)  **Justificativa:** - 70% treino: volume suficiente para aprendizado - 15% valida\u00e7\u00e3o: monitorar overfitting e early stopping - 15% teste: avalia\u00e7\u00e3o final imparcial - Mini-batch: equil\u00edbrio entre velocidade e estabilidade\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Dividir dados\nn_samples = X_norm.shape[0]\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\n\n# Calcular tamanhos\ntrain_size = int(0.70 * n_samples)\nval_size = int(0.15 * n_samples)\ntest_size = n_samples - train_size - val_size\n\n# Dividir \u00edndices\ntrain_idx = indices[:train_size]\nval_idx = indices[train_size:train_size + val_size]\ntest_idx = indices[train_size + val_size:]\n\n# Criar conjuntos\nX_train, y_train = X_norm[train_idx], y_norm[train_idx]\nX_val, y_val = X_norm[val_idx], y_norm[val_idx]\nX_test, y_test = X_norm[test_idx], y_norm[test_idx]\n\nprint(f\"Divis\u00e3o dos dados:\")\nprint(f\"  Train: {len(X_train):,} ({len(X_train)/n_samples*100:.1f}%)\")\nprint(f\"  Val:   {len(X_val):,} ({len(X_val)/n_samples*100:.1f}%)\")\nprint(f\"  Test:  {len(X_test):,} ({len(X_test)/n_samples*100:.1f}%)\")\"\"\")\n</pre> code(\"\"\"# Dividir dados n_samples = X_norm.shape[0] indices = np.arange(n_samples) np.random.shuffle(indices)  # Calcular tamanhos train_size = int(0.70 * n_samples) val_size = int(0.15 * n_samples) test_size = n_samples - train_size - val_size  # Dividir \u00edndices train_idx = indices[:train_size] val_idx = indices[train_size:train_size + val_size] test_idx = indices[train_size + val_size:]  # Criar conjuntos X_train, y_train = X_norm[train_idx], y_norm[train_idx] X_val, y_val = X_norm[val_idx], y_norm[val_idx] X_test, y_test = X_norm[test_idx], y_norm[test_idx]  print(f\"Divis\u00e3o dos dados:\") print(f\"  Train: {len(X_train):,} ({len(X_train)/n_samples*100:.1f}%)\") print(f\"  Val:   {len(X_val):,} ({len(X_val)/n_samples*100:.1f}%)\") print(f\"  Test:  {len(X_test):,} ({len(X_test)/n_samples*100:.1f}%)\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 5. Implementa\u00e7\u00e3o do MLP\n\n**Arquitetura:** Input(15) \u2192 Hidden(64, ReLU) \u2192 Hidden(32, ReLU) \u2192 Hidden(16, ReLU) \u2192 Output(1, Linear)\n\n**Componentes:**\n- **Inicializa\u00e7\u00e3o:** He initialization\n- **Ativa\u00e7\u00e3o:** ReLU (camadas ocultas), Linear (sa\u00edda)\n- **Loss:** MSE + Regulariza\u00e7\u00e3o L2\n- **Otimiza\u00e7\u00e3o:** Mini-batch gradient descent\n- **Regulariza\u00e7\u00e3o:** Early stopping + L2\"\"\")\n</pre> md(\"\"\"## 5. Implementa\u00e7\u00e3o do MLP  **Arquitetura:** Input(15) \u2192 Hidden(64, ReLU) \u2192 Hidden(32, ReLU) \u2192 Hidden(16, ReLU) \u2192 Output(1, Linear)  **Componentes:** - **Inicializa\u00e7\u00e3o:** He initialization - **Ativa\u00e7\u00e3o:** ReLU (camadas ocultas), Linear (sa\u00edda) - **Loss:** MSE + Regulariza\u00e7\u00e3o L2 - **Otimiza\u00e7\u00e3o:** Mini-batch gradient descent - **Regulariza\u00e7\u00e3o:** Early stopping + L2\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"class MLP:\n    \\\"\\\"\\\"Multi-Layer Perceptron para Regress\u00e3o\\\"\\\"\\\"\n\n    def __init__(self, layers, learning_rate=0.001, reg_lambda=0.001):\n        self.lr = learning_rate\n        self.reg = reg_lambda\n        self.weights = []\n        self.biases = []\n\n        # He initialization\n        for i in range(len(layers) - 1):\n            W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n        self.train_losses = []\n        self.val_losses = []\n\n    def relu(self, x):\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        return (x &gt; 0).astype(float)\n\n    def forward(self, X):\n        \\\"\\\"\\\"Forward propagation\\\"\\\"\\\"\n        self.cache = {'A': [X], 'Z': []}\n        A = X\n\n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            self.cache['Z'].append(Z)\n            self.cache['A'].append(A)\n\n        # Output layer (linear)\n        Z = A @ self.weights[-1] + self.biases[-1]\n        self.cache['Z'].append(Z)\n        self.cache['A'].append(Z)\n\n        return Z\n\n    def compute_loss(self, y_true, y_pred):\n        \\\"\\\"\\\"MSE + L2 regularization\\\"\\\"\\\"\n        n = len(y_true)\n        mse = np.mean((y_pred - y_true) ** 2)\n        l2 = sum(np.sum(W ** 2) for W in self.weights)\n        return mse + (self.reg / (2 * n)) * l2\n\n    def backward(self, y_true):\n        \\\"\\\"\\\"Backpropagation\\\"\\\"\\\"\n        n = len(y_true)\n        y_pred = self.cache['A'][-1]\n        dA = (2.0 / n) * (y_pred - y_true)\n\n        grads_W = []\n        grads_b = []\n\n        for i in reversed(range(len(self.weights))):\n            A_prev = self.cache['A'][i]\n\n            dW = A_prev.T @ dA + (self.reg / n) * self.weights[i]\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            grads_W.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = (dA @ self.weights[i].T) * self.relu_derivative(self.cache['Z'][i-1])\n\n        # Update weights\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.lr * grads_W[i]\n            self.biases[i] -= self.lr * grads_b[i]\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15, verbose=True):\n        \\\"\\\"\\\"Treinar modelo com early stopping\\\"\\\"\\\"\n        best_loss = float('inf')\n        patience_count = 0\n\n        for epoch in range(epochs):\n            # Mini-batch training\n            indices = np.arange(len(X_train))\n            np.random.shuffle(indices)\n\n            for start in range(0, len(X_train), batch_size):\n                end = min(start + batch_size, len(X_train))\n                batch_idx = indices[start:end]\n\n                X_batch = X_train[batch_idx]\n                y_batch = y_train[batch_idx]\n\n                self.forward(X_batch)\n                self.backward(y_batch)\n\n            # Calcular losses\n            train_pred = self.forward(X_train)\n            val_pred = self.forward(X_val)\n\n            train_loss = self.compute_loss(y_train, train_pred)\n            val_loss = self.compute_loss(y_val, val_pred)\n\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                patience_count = 0\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_count += 1\n\n            if verbose and (epoch + 1) % 20 == 0:\n                print(f\"\u00c9poca {epoch+1:3d}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n\n            if patience_count &gt;= patience:\n                if verbose:\n                    print(f\"\\\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        self.weights = self.best_weights\n        self.biases = self.best_biases\n\n    def predict(self, X):\n        \\\"\\\"\\\"Fazer predi\u00e7\u00f5es\\\"\\\"\\\"\n        A = X\n        for i in range(len(self.weights) - 1):\n            A = self.relu(A @ self.weights[i] + self.biases[i])\n        return A @ self.weights[-1] + self.biases[-1]\n\nprint(\"MLP implementado!\")\"\"\")\n</pre> code(\"\"\"class MLP:     \\\"\\\"\\\"Multi-Layer Perceptron para Regress\u00e3o\\\"\\\"\\\"      def __init__(self, layers, learning_rate=0.001, reg_lambda=0.001):         self.lr = learning_rate         self.reg = reg_lambda         self.weights = []         self.biases = []          # He initialization         for i in range(len(layers) - 1):             W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])             b = np.zeros((1, layers[i+1]))             self.weights.append(W)             self.biases.append(b)          self.train_losses = []         self.val_losses = []      def relu(self, x):         return np.maximum(0, x)      def relu_derivative(self, x):         return (x &gt; 0).astype(float)      def forward(self, X):         \\\"\\\"\\\"Forward propagation\\\"\\\"\\\"         self.cache = {'A': [X], 'Z': []}         A = X          # Hidden layers         for i in range(len(self.weights) - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             self.cache['Z'].append(Z)             self.cache['A'].append(A)          # Output layer (linear)         Z = A @ self.weights[-1] + self.biases[-1]         self.cache['Z'].append(Z)         self.cache['A'].append(Z)          return Z      def compute_loss(self, y_true, y_pred):         \\\"\\\"\\\"MSE + L2 regularization\\\"\\\"\\\"         n = len(y_true)         mse = np.mean((y_pred - y_true) ** 2)         l2 = sum(np.sum(W ** 2) for W in self.weights)         return mse + (self.reg / (2 * n)) * l2      def backward(self, y_true):         \\\"\\\"\\\"Backpropagation\\\"\\\"\\\"         n = len(y_true)         y_pred = self.cache['A'][-1]         dA = (2.0 / n) * (y_pred - y_true)          grads_W = []         grads_b = []          for i in reversed(range(len(self.weights))):             A_prev = self.cache['A'][i]              dW = A_prev.T @ dA + (self.reg / n) * self.weights[i]             db = np.sum(dA, axis=0, keepdims=True)              grads_W.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = (dA @ self.weights[i].T) * self.relu_derivative(self.cache['Z'][i-1])          # Update weights         for i in range(len(self.weights)):             self.weights[i] -= self.lr * grads_W[i]             self.biases[i] -= self.lr * grads_b[i]      def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15, verbose=True):         \\\"\\\"\\\"Treinar modelo com early stopping\\\"\\\"\\\"         best_loss = float('inf')         patience_count = 0          for epoch in range(epochs):             # Mini-batch training             indices = np.arange(len(X_train))             np.random.shuffle(indices)              for start in range(0, len(X_train), batch_size):                 end = min(start + batch_size, len(X_train))                 batch_idx = indices[start:end]                  X_batch = X_train[batch_idx]                 y_batch = y_train[batch_idx]                  self.forward(X_batch)                 self.backward(y_batch)              # Calcular losses             train_pred = self.forward(X_train)             val_pred = self.forward(X_val)              train_loss = self.compute_loss(y_train, train_pred)             val_loss = self.compute_loss(y_val, val_pred)              self.train_losses.append(train_loss)             self.val_losses.append(val_loss)              # Early stopping             if val_loss &lt; best_loss:                 best_loss = val_loss                 patience_count = 0                 self.best_weights = [W.copy() for W in self.weights]                 self.best_biases = [b.copy() for b in self.biases]             else:                 patience_count += 1              if verbose and (epoch + 1) % 20 == 0:                 print(f\"\u00c9poca {epoch+1:3d}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")              if patience_count &gt;= patience:                 if verbose:                     print(f\"\\\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")                 break          # Restaurar melhores pesos         self.weights = self.best_weights         self.biases = self.best_biases      def predict(self, X):         \\\"\\\"\\\"Fazer predi\u00e7\u00f5es\\\"\\\"\\\"         A = X         for i in range(len(self.weights) - 1):             A = self.relu(A @ self.weights[i] + self.biases[i])         return A @ self.weights[-1] + self.biases[-1]  print(\"MLP implementado!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 6. Treinamento do Modelo\"\"\")\n</pre> md(\"\"\"## 6. Treinamento do Modelo\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Criar e treinar modelo\nmodel = MLP(\n    layers=[15, 64, 32, 16, 1],\n    learning_rate=0.001,\n    reg_lambda=0.001\n)\n\nprint(\"Iniciando treinamento...\\\\n\")\nmodel.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15)\nprint(\"\\\\nTreinamento conclu\u00eddo!\")\"\"\")\n</pre> code(\"\"\"# Criar e treinar modelo model = MLP(     layers=[15, 64, 32, 16, 1],     learning_rate=0.001,     reg_lambda=0.001 )  print(\"Iniciando treinamento...\\\\n\") model.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15) print(\"\\\\nTreinamento conclu\u00eddo!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 7. Curvas de Erro e Visualiza\u00e7\u00f5es\"\"\")\n</pre> md(\"\"\"## 7. Curvas de Erro e Visualiza\u00e7\u00f5es\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Plot curvas de loss\nplt.figure(figsize=(10, 5))\nepochs = range(1, len(model.train_losses) + 1)\n\nplt.plot(epochs, model.train_losses, label='Train Loss', linewidth=2)\nplt.plot(epochs, model.val_losses, label='Validation Loss', linewidth=2)\nplt.xlabel('\u00c9poca')\nplt.ylabel('Loss (MSE + L2)')\nplt.title('Converg\u00eancia do Modelo')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u00c9pocas treinadas: {len(model.train_losses)}\")\nprint(f\"Loss final - Train: {model.train_losses[-1]:.4f} | Val: {model.val_losses[-1]:.4f}\")\"\"\")\n</pre> code(\"\"\"# Plot curvas de loss plt.figure(figsize=(10, 5)) epochs = range(1, len(model.train_losses) + 1)  plt.plot(epochs, model.train_losses, label='Train Loss', linewidth=2) plt.plot(epochs, model.val_losses, label='Validation Loss', linewidth=2) plt.xlabel('\u00c9poca') plt.ylabel('Loss (MSE + L2)') plt.title('Converg\u00eancia do Modelo') plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(f\"\u00c9pocas treinadas: {len(model.train_losses)}\") print(f\"Loss final - Train: {model.train_losses[-1]:.4f} | Val: {model.val_losses[-1]:.4f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 8. M\u00e9tricas de Avalia\u00e7\u00e3o\n\nM\u00e9tricas obrigat\u00f3rias para regress\u00e3o: MAE, MAPE, MSE, RMSE, R\u00b2\"\"\")\n</pre> md(\"\"\"## 8. M\u00e9tricas de Avalia\u00e7\u00e3o  M\u00e9tricas obrigat\u00f3rias para regress\u00e3o: MAE, MAPE, MSE, RMSE, R\u00b2\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Fun\u00e7\u00e3o para calcular m\u00e9tricas\ndef calc_metrics(y_true, y_pred):\n    y_true = y_true.flatten()\n    y_pred = y_pred.flatten()\n\n    mae = np.mean(np.abs(y_true - y_pred))\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n\n    # MAPE (evitar divis\u00e3o por zero)\n    mask = y_true != 0\n    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n\n# Fazer predi\u00e7\u00f5es e desnormalizar\ny_pred_test_norm = model.predict(X_test)\ny_pred_test = y_pred_test_norm * y_std + y_mean\ny_true_test = y_test * y_std + y_mean\n\ny_pred_train_norm = model.predict(X_train)\ny_pred_train = y_pred_train_norm * y_std + y_mean\ny_true_train = y_train * y_std + y_mean\n\n# Calcular m\u00e9tricas\ntrain_metrics = calc_metrics(y_true_train, y_pred_train)\ntest_metrics = calc_metrics(y_true_test, y_pred_test)\n\n# Baseline (preditor de m\u00e9dia)\nbaseline_pred = np.full_like(y_true_test, y_true_train.mean())\nbaseline_metrics = calc_metrics(y_true_test, baseline_pred)\n\n# Exibir resultados\nprint(\"M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\")\nprint(\"=\"*70)\nprint(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15} {'Baseline':&lt;15}\")\nprint(\"-\"*70)\nprint(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f} {baseline_metrics['MAE']:&lt;15.2f}\")\nprint(f\"{'MSE':&lt;10} {train_metrics['MSE']:&lt;15.2f} {test_metrics['MSE']:&lt;15.2f} {baseline_metrics['MSE']:&lt;15.2f}\")\nprint(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f} {baseline_metrics['RMSE']:&lt;15.2f}\")\nprint(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f} {baseline_metrics['R2']:&lt;15.4f}\")\nprint(f\"{'MAPE (%)':&lt;10} {train_metrics['MAPE']:&lt;15.2f} {test_metrics['MAPE']:&lt;15.2f} {baseline_metrics['MAPE']:&lt;15.2f}\")\nprint(\"=\"*70)\n\nmelhoria = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100)\nprint(f\"\\\\nMelhoria sobre baseline: {melhoria:.1f}%\")\"\"\")\n</pre> code(\"\"\"# Fun\u00e7\u00e3o para calcular m\u00e9tricas def calc_metrics(y_true, y_pred):     y_true = y_true.flatten()     y_pred = y_pred.flatten()      mae = np.mean(np.abs(y_true - y_pred))     mse = np.mean((y_true - y_pred) ** 2)     rmse = np.sqrt(mse)      ss_res = np.sum((y_true - y_pred) ** 2)     ss_tot = np.sum((y_true - y_true.mean()) ** 2)     r2 = 1 - (ss_res / ss_tot)      # MAPE (evitar divis\u00e3o por zero)     mask = y_true != 0     mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100      return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}  # Fazer predi\u00e7\u00f5es e desnormalizar y_pred_test_norm = model.predict(X_test) y_pred_test = y_pred_test_norm * y_std + y_mean y_true_test = y_test * y_std + y_mean  y_pred_train_norm = model.predict(X_train) y_pred_train = y_pred_train_norm * y_std + y_mean y_true_train = y_train * y_std + y_mean  # Calcular m\u00e9tricas train_metrics = calc_metrics(y_true_train, y_pred_train) test_metrics = calc_metrics(y_true_test, y_pred_test)  # Baseline (preditor de m\u00e9dia) baseline_pred = np.full_like(y_true_test, y_true_train.mean()) baseline_metrics = calc_metrics(y_true_test, baseline_pred)  # Exibir resultados print(\"M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\") print(\"=\"*70) print(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15} {'Baseline':&lt;15}\") print(\"-\"*70) print(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f} {baseline_metrics['MAE']:&lt;15.2f}\") print(f\"{'MSE':&lt;10} {train_metrics['MSE']:&lt;15.2f} {test_metrics['MSE']:&lt;15.2f} {baseline_metrics['MSE']:&lt;15.2f}\") print(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f} {baseline_metrics['RMSE']:&lt;15.2f}\") print(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f} {baseline_metrics['R2']:&lt;15.4f}\") print(f\"{'MAPE (%)':&lt;10} {train_metrics['MAPE']:&lt;15.2f} {test_metrics['MAPE']:&lt;15.2f} {baseline_metrics['MAPE']:&lt;15.2f}\") print(\"=\"*70)  melhoria = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100) print(f\"\\\\nMelhoria sobre baseline: {melhoria:.1f}%\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>code(\"\"\"# Visualiza\u00e7\u00f5es de avalia\u00e7\u00e3o\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Predito vs Real\naxes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=15)\nlim = [y_true_test.min(), y_true_test.max()]\naxes[0].plot(lim, lim, 'r--', linewidth=2, label='Predi\u00e7\u00e3o perfeita')\naxes[0].set_xlabel('Valor Real')\naxes[0].set_ylabel('Valor Predito')\naxes[0].set_title(f'Predi\u00e7\u00f5es vs Real\\\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Res\u00edduos\nresiduals = (y_true_test - y_pred_test).flatten()\naxes[1].scatter(y_pred_test, residuals, alpha=0.4, s=15)\naxes[1].axhline(0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Valor Predito')\naxes[1].set_ylabel('Res\u00edduo (Real - Predito)')\naxes[1].set_title('An\u00e1lise de Res\u00edduos')\naxes[1].grid(True, alpha=0.3)\n\n# Distribui\u00e7\u00e3o dos res\u00edduos\naxes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[2].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero')\naxes[2].axvline(residuals.mean(), color='g', linestyle='--', linewidth=2, label=f'M\u00e9dia: {residuals.mean():.1f}')\naxes[2].set_xlabel('Res\u00edduo')\naxes[2].set_ylabel('Frequ\u00eancia')\naxes[2].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estat\u00edsticas dos res\u00edduos:\")\nprint(f\"  M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\")\nprint(f\"  Std: {residuals.std():.2f}\")\nprint(f\"  Mediana do erro absoluto: {np.median(np.abs(residuals)):.2f}\")\"\"\")\n</pre> code(\"\"\"# Visualiza\u00e7\u00f5es de avalia\u00e7\u00e3o fig, axes = plt.subplots(1, 3, figsize=(16, 5))  # Predito vs Real axes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=15) lim = [y_true_test.min(), y_true_test.max()] axes[0].plot(lim, lim, 'r--', linewidth=2, label='Predi\u00e7\u00e3o perfeita') axes[0].set_xlabel('Valor Real') axes[0].set_ylabel('Valor Predito') axes[0].set_title(f'Predi\u00e7\u00f5es vs Real\\\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Res\u00edduos residuals = (y_true_test - y_pred_test).flatten() axes[1].scatter(y_pred_test, residuals, alpha=0.4, s=15) axes[1].axhline(0, color='r', linestyle='--', linewidth=2) axes[1].set_xlabel('Valor Predito') axes[1].set_ylabel('Res\u00edduo (Real - Predito)') axes[1].set_title('An\u00e1lise de Res\u00edduos') axes[1].grid(True, alpha=0.3)  # Distribui\u00e7\u00e3o dos res\u00edduos axes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7) axes[2].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero') axes[2].axvline(residuals.mean(), color='g', linestyle='--', linewidth=2, label=f'M\u00e9dia: {residuals.mean():.1f}') axes[2].set_xlabel('Res\u00edduo') axes[2].set_ylabel('Frequ\u00eancia') axes[2].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos') axes[2].legend() axes[2].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"Estat\u00edsticas dos res\u00edduos:\") print(f\"  M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\") print(f\"  Std: {residuals.std():.2f}\") print(f\"  Mediana do erro absoluto: {np.median(np.abs(residuals)):.2f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>md(\"\"\"## 9. Conclus\u00e3o\n\n### Resultados:\n- **R\u00b2 = {:.4f}**: Modelo explica ~{:.1f}% da vari\u00e2ncia\n- **RMSE = {:.2f}**: Erro m\u00e9dio de ~{:.0f} alugu\u00e9is\n- **Melhoria sobre baseline**: {:.1f}%\n\n### Limita\u00e7\u00f5es:\n- MLP n\u00e3o captura depend\u00eancias temporais de longo prazo\n- Features podem ser expandidas (lags, intera\u00e7\u00f5es)\n- Hiperpar\u00e2metros n\u00e3o foram otimizados sistematicamente\n\n### Melhorias Futuras:\n- Arquiteturas recorrentes (LSTM/GRU) para s\u00e9ries temporais\n- Feature engineering mais sofisticado\n- Ensemble com outros modelos (XGBoost, Random Forest)\n- Grid search para otimiza\u00e7\u00e3o de hiperpar\u00e2metros\n\n### Ferramentas de IA:\nClaude Code (Anthropic) foi usado para auxiliar na estrutura\u00e7\u00e3o do c\u00f3digo. Todo o c\u00f3digo foi compreendido e validado manualmente.\n\n### Refer\u00eancias:\n- Dataset: Fanaee-T &amp; Gama (2013), UCI ML Repository\n- Implementa\u00e7\u00e3o: NumPy, conceitos de Goodfellow et al. (Deep Learning, 2016)\n- Material do curso: https://insper.github.io/ann-dl/\"\"\")\n</pre> md(\"\"\"## 9. Conclus\u00e3o  ### Resultados: - **R\u00b2 = {:.4f}**: Modelo explica ~{:.1f}% da vari\u00e2ncia - **RMSE = {:.2f}**: Erro m\u00e9dio de ~{:.0f} alugu\u00e9is - **Melhoria sobre baseline**: {:.1f}%  ### Limita\u00e7\u00f5es: - MLP n\u00e3o captura depend\u00eancias temporais de longo prazo - Features podem ser expandidas (lags, intera\u00e7\u00f5es) - Hiperpar\u00e2metros n\u00e3o foram otimizados sistematicamente  ### Melhorias Futuras: - Arquiteturas recorrentes (LSTM/GRU) para s\u00e9ries temporais - Feature engineering mais sofisticado - Ensemble com outros modelos (XGBoost, Random Forest) - Grid search para otimiza\u00e7\u00e3o de hiperpar\u00e2metros  ### Ferramentas de IA: Claude Code (Anthropic) foi usado para auxiliar na estrutura\u00e7\u00e3o do c\u00f3digo. Todo o c\u00f3digo foi compreendido e validado manualmente.  ### Refer\u00eancias: - Dataset: Fanaee-T &amp; Gama (2013), UCI ML Repository - Implementa\u00e7\u00e3o: NumPy, conceitos de Goodfellow et al. (Deep Learning, 2016) - Material do curso: https://insper.github.io/ann-dl/\"\"\") In\u00a0[\u00a0]: Copied! <pre># Salvar\noutput = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\nwith open(output, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> # Salvar output = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" with open(output, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"\u2713 Notebook final criado seguindo padr\u00f5es da disciplina!\")\nprint(f\"\u2713 Total: {len(notebook['cells'])} c\u00e9lulas\")\nprint(f\"\u2713 Estrutura:\")\nprint(\"  - Sele\u00e7\u00e3o do dataset\")\nprint(\"  - An\u00e1lise explorat\u00f3ria\")\nprint(\"  - Limpeza e normaliza\u00e7\u00e3o\")\nprint(\"  - Estrat\u00e9gia train/val/test\")\nprint(\"  - Implementa\u00e7\u00e3o MLP\")\nprint(\"  - Treinamento\")\nprint(\"  - Curvas de erro\")\nprint(\"  - M\u00e9tricas (MAE, MAPE, MSE, RMSE, R\u00b2)\")\nprint(\"  - Conclus\u00e3o e refer\u00eancias\")\n</pre> print(f\"\u2713 Notebook final criado seguindo padr\u00f5es da disciplina!\") print(f\"\u2713 Total: {len(notebook['cells'])} c\u00e9lulas\") print(f\"\u2713 Estrutura:\") print(\"  - Sele\u00e7\u00e3o do dataset\") print(\"  - An\u00e1lise explorat\u00f3ria\") print(\"  - Limpeza e normaliza\u00e7\u00e3o\") print(\"  - Estrat\u00e9gia train/val/test\") print(\"  - Implementa\u00e7\u00e3o MLP\") print(\"  - Treinamento\") print(\"  - Curvas de erro\") print(\"  - M\u00e9tricas (MAE, MAPE, MSE, RMSE, R\u00b2)\") print(\"  - Conclus\u00e3o e refer\u00eancias\")"},{"location":"portfolio/neural-networks/projects/2/create_final_notebook/#notebook-final-seguindo-padroes-da-disciplina","title":"============================================================================== NOTEBOOK FINAL - SEGUINDO PADR\u00d5ES DA DISCIPLINA\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/create_regression_notebook/","title":"Create regression notebook","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para criar o notebook de regress\u00e3o completo.\nExecute: python create_regression_notebook.py\n\"\"\"\n</pre> \"\"\" Script para criar o notebook de regress\u00e3o completo. Execute: python create_regression_notebook.py \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre># Estrutura do notebook\nnotebook = {\n    \"cells\": [],\n    \"metadata\": {\n        \"kernelspec\": {\n            \"display_name\": \"Python 3\",\n            \"language\": \"python\",\n            \"name\": \"python3\"\n        },\n        \"language_info\": {\n            \"codemirror_mode\": {\n                \"name\": \"ipython\",\n                \"version\": 3\n            },\n            \"file_extension\": \".py\",\n            \"mimetype\": \"text/x-python\",\n            \"name\": \"python\",\n            \"nbconvert_exporter\": \"python\",\n            \"pygments_lexer\": \"ipython3\",\n            \"version\": \"3.8.0\"\n        }\n    },\n    \"nbformat\": 4,\n    \"nbformat_minor\": 4\n}\n</pre> # Estrutura do notebook notebook = {     \"cells\": [],     \"metadata\": {         \"kernelspec\": {             \"display_name\": \"Python 3\",             \"language\": \"python\",             \"name\": \"python3\"         },         \"language_info\": {             \"codemirror_mode\": {                 \"name\": \"ipython\",                 \"version\": 3             },             \"file_extension\": \".py\",             \"mimetype\": \"text/x-python\",             \"name\": \"python\",             \"nbconvert_exporter\": \"python\",             \"pygments_lexer\": \"ipython3\",             \"version\": \"3.8.0\"         }     },     \"nbformat\": 4,     \"nbformat_minor\": 4 } In\u00a0[\u00a0]: Copied! <pre># Helper function to create cells\ndef add_markdown_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": content.split(\"\\n\")\n    })\n</pre> # Helper function to create cells def add_markdown_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code_cell(content):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": content.split(\"\\n\")\n    })\n</pre> def add_code_cell(content):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": content.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre># T\u00edtulo e Introdu\u00e7\u00e3o\nadd_markdown_cell(\"\"\"# Projeto de Regress\u00e3o: Previs\u00e3o de Demanda de Bicicletas Compartilhadas\n\n**Disciplina:** Redes Neurais e Deep Learning\n**Projeto:** Regression Task com Multi-Layer Perceptron (MLP)\n**Deadline:** 19.out.2025\n\n---\n\n## Objetivo\n\nEste projeto implementa uma rede neural MLP do zero para resolver um problema de regress\u00e3o real: **prever a demanda de bicicletas compartilhadas** com base em condi\u00e7\u00f5es clim\u00e1ticas, temporais e sazonais. O objetivo \u00e9 aprofundar o conhecimento sobre redes neurais atrav\u00e9s de implementa\u00e7\u00e3o completa, desde prepara\u00e7\u00e3o de dados at\u00e9 avalia\u00e7\u00e3o de resultados.\"\"\")\n</pre> # T\u00edtulo e Introdu\u00e7\u00e3o add_markdown_cell(\"\"\"# Projeto de Regress\u00e3o: Previs\u00e3o de Demanda de Bicicletas Compartilhadas  **Disciplina:** Redes Neurais e Deep Learning **Projeto:** Regression Task com Multi-Layer Perceptron (MLP) **Deadline:** 19.out.2025  ---  ## Objetivo  Este projeto implementa uma rede neural MLP do zero para resolver um problema de regress\u00e3o real: **prever a demanda de bicicletas compartilhadas** com base em condi\u00e7\u00f5es clim\u00e1ticas, temporais e sazonais. O objetivo \u00e9 aprofundar o conhecimento sobre redes neurais atrav\u00e9s de implementa\u00e7\u00e3o completa, desde prepara\u00e7\u00e3o de dados at\u00e9 avalia\u00e7\u00e3o de resultados.\"\"\") In\u00a0[\u00a0]: Copied! <pre># Se\u00e7\u00e3o 1: Sele\u00e7\u00e3o do Dataset\nadd_markdown_cell(\"\"\"## 1. Sele\u00e7\u00e3o do Dataset\n\n### Dataset Escolhido: **Bike Sharing Demand Dataset**\n\n- **Fonte:** [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)\n- **Tamb\u00e9m dispon\u00edvel em:** [Kaggle - Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand)\n- **Tamanho:** 17,379 amostras (dia a dia durante 2 anos: 2011-2012)\n- **Features:** 16 vari\u00e1veis (temporais, clim\u00e1ticas, sazonais)\n- **Target:** Contagem total de alugu\u00e9is de bicicletas (vari\u00e1vel cont\u00ednua)\n\n### Por que este dataset?\n\n1. **Relev\u00e2ncia Real:** Sistemas de bike-sharing s\u00e3o amplamente usados em cidades inteligentes. Prever demanda ajuda em:\n   - Rebalanceamento de bicicletas entre esta\u00e7\u00f5es\n   - Planejamento de manuten\u00e7\u00e3o\n   - Otimiza\u00e7\u00e3o de recursos operacionais\n\n2. **Complexidade Adequada:**\n   - M\u00faltiplas features com diferentes tipos (num\u00e9ricas, categ\u00f3ricas)\n   - Rela\u00e7\u00f5es n\u00e3o-lineares entre clima e demanda\n   - Sazonalidade e tend\u00eancias temporais\n\n3. **Dataset N\u00e3o-Trivial:**\n   - Evita datasets cl\u00e1ssicos como Boston Housing ou California Housing\n   - Mais de 17k amostras (&gt;1,000 requerido)\n   - Mais de 5 features relevantes\n\n4. **Possibilidade de Competi\u00e7\u00e3o:**\n   - Dataset usado em competi\u00e7\u00e3o do Kaggle (oportunidade de bonus)\n   - Benchmark p\u00fablico dispon\u00edvel\"\"\")\n</pre> # Se\u00e7\u00e3o 1: Sele\u00e7\u00e3o do Dataset add_markdown_cell(\"\"\"## 1. Sele\u00e7\u00e3o do Dataset  ### Dataset Escolhido: **Bike Sharing Demand Dataset**  - **Fonte:** [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) - **Tamb\u00e9m dispon\u00edvel em:** [Kaggle - Bike Sharing Demand](https://www.kaggle.com/c/bike-sharing-demand) - **Tamanho:** 17,379 amostras (dia a dia durante 2 anos: 2011-2012) - **Features:** 16 vari\u00e1veis (temporais, clim\u00e1ticas, sazonais) - **Target:** Contagem total de alugu\u00e9is de bicicletas (vari\u00e1vel cont\u00ednua)  ### Por que este dataset?  1. **Relev\u00e2ncia Real:** Sistemas de bike-sharing s\u00e3o amplamente usados em cidades inteligentes. Prever demanda ajuda em:    - Rebalanceamento de bicicletas entre esta\u00e7\u00f5es    - Planejamento de manuten\u00e7\u00e3o    - Otimiza\u00e7\u00e3o de recursos operacionais  2. **Complexidade Adequada:**    - M\u00faltiplas features com diferentes tipos (num\u00e9ricas, categ\u00f3ricas)    - Rela\u00e7\u00f5es n\u00e3o-lineares entre clima e demanda    - Sazonalidade e tend\u00eancias temporais  3. **Dataset N\u00e3o-Trivial:**    - Evita datasets cl\u00e1ssicos como Boston Housing ou California Housing    - Mais de 17k amostras (&gt;1,000 requerido)    - Mais de 5 features relevantes  4. **Possibilidade de Competi\u00e7\u00e3o:**    - Dataset usado em competi\u00e7\u00e3o do Kaggle (oportunidade de bonus)    - Benchmark p\u00fablico dispon\u00edvel\"\"\") In\u00a0[\u00a0]: Copied! <pre># Se\u00e7\u00e3o 2: Importa\u00e7\u00e3o de Bibliotecas\nadd_markdown_cell(\"\"\"## 2. Importa\u00e7\u00e3o de Bibliotecas\"\"\")\n</pre> # Se\u00e7\u00e3o 2: Importa\u00e7\u00e3o de Bibliotecas add_markdown_cell(\"\"\"## 2. Importa\u00e7\u00e3o de Bibliotecas\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Manipula\u00e7\u00e3o de dados\nimport numpy as np\nimport pandas as pd\n\n# Visualiza\u00e7\u00e3o\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Utilit\u00e1rios\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura\u00e7\u00e3o de visualiza\u00e7\u00e3o\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n%matplotlib inline\n\n# Seed para reprodutibilidade\nnp.random.seed(42)\n\nprint(\"Bibliotecas importadas com sucesso!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\"\"\")\n</pre> add_code_cell(\"\"\"# Manipula\u00e7\u00e3o de dados import numpy as np import pandas as pd  # Visualiza\u00e7\u00e3o import matplotlib.pyplot as plt import seaborn as sns  # Utilit\u00e1rios from datetime import datetime import warnings warnings.filterwarnings('ignore')  # Configura\u00e7\u00e3o de visualiza\u00e7\u00e3o plt.style.use('seaborn-v0_8-darkgrid') sns.set_palette(\"husl\") %matplotlib inline  # Seed para reprodutibilidade np.random.seed(42)  print(\"Bibliotecas importadas com sucesso!\") print(f\"NumPy version: {np.__version__}\") print(f\"Pandas version: {pd.__version__}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre># Se\u00e7\u00e3o 3: Carregamento dos Dados\nadd_markdown_cell(\"\"\"## 3. Carregamento e Explora\u00e7\u00e3o Inicial dos Dados\n\n### Download do Dataset\n\n**IMPORTANTE:** Antes de executar o notebook, baixe o dataset:\n\n1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n2. Baixe o arquivo `Bike-Sharing-Dataset.zip`\n3. Extraia e coloque o arquivo `hour.csv` neste diret\u00f3rio\n\n**Alternativa:** Use o dataset do Kaggle (requer Kaggle API):\n```bash\nkaggle competitions download -c bike-sharing-demand\n```\"\"\")\n</pre> # Se\u00e7\u00e3o 3: Carregamento dos Dados add_markdown_cell(\"\"\"## 3. Carregamento e Explora\u00e7\u00e3o Inicial dos Dados  ### Download do Dataset  **IMPORTANTE:** Antes de executar o notebook, baixe o dataset:  1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset 2. Baixe o arquivo `Bike-Sharing-Dataset.zip` 3. Extraia e coloque o arquivo `hour.csv` neste diret\u00f3rio  **Alternativa:** Use o dataset do Kaggle (requer Kaggle API): ```bash kaggle competitions download -c bike-sharing-demand ```\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Carregamento do dataset\n# Nota: Usando dados por hora para maior volume de amostras\n\ntry:\n    df = pd.read_csv('hour.csv')\n    print(\"\u2713 Dataset carregado com sucesso!\")\nexcept FileNotFoundError:\n    print(\"\u274c Arquivo 'hour.csv' n\u00e3o encontrado!\")\n    print(\"Por favor, baixe o dataset de: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")\n    raise\n\n# Visualiza\u00e7\u00e3o inicial\nprint(f\"\\\\nDimens\u00f5es do dataset: {df.shape}\")\nprint(f\"Amostras: {df.shape[0]:,} | Features: {df.shape[1]}\")\nprint(\"\\\\n\" + \"=\"*80)\ndf.head(10)\"\"\")\n</pre> add_code_cell(\"\"\"# Carregamento do dataset # Nota: Usando dados por hora para maior volume de amostras  try:     df = pd.read_csv('hour.csv')     print(\"\u2713 Dataset carregado com sucesso!\") except FileNotFoundError:     print(\"\u274c Arquivo 'hour.csv' n\u00e3o encontrado!\")     print(\"Por favor, baixe o dataset de: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")     raise  # Visualiza\u00e7\u00e3o inicial print(f\"\\\\nDimens\u00f5es do dataset: {df.shape}\") print(f\"Amostras: {df.shape[0]:,} | Features: {df.shape[1]}\") print(\"\\\\n\" + \"=\"*80) df.head(10)\"\"\") In\u00a0[\u00a0]: Copied! <pre># Se\u00e7\u00e3o 4: Explica\u00e7\u00e3o do Dataset\nadd_markdown_cell(\"\"\"## 4. Explica\u00e7\u00e3o Detalhada do Dataset\n\n### Contexto do Problema\n\nO dataset cont\u00e9m registros hor\u00e1rios de um sistema de bike-sharing em Washington D.C. durante 2011 e 2012. Sistemas de compartilhamento de bicicletas automatizam o processo de aluguel/devolu\u00e7\u00e3o atrav\u00e9s de quiosques distribu\u00eddos pela cidade.\n\n### Descri\u00e7\u00e3o das Features\n\n#### Vari\u00e1veis Temporais:\n- **instant**: \u00cdndice do registro (n\u00e3o ser\u00e1 usado como feature)\n- **dteday**: Data (formato YYYY-MM-DD)\n- **season**: Esta\u00e7\u00e3o do ano (1: primavera, 2: ver\u00e3o, 3: outono, 4: inverno)\n- **yr**: Ano (0: 2011, 1: 2012)\n- **mnth**: M\u00eas (1 a 12)\n- **hr**: Hora do dia (0 a 23)\n- **holiday**: Dia \u00e9 feriado (0: n\u00e3o, 1: sim)\n- **weekday**: Dia da semana (0: domingo ... 6: s\u00e1bado)\n- **workingday**: Dia \u00fatil (0: fim de semana/feriado, 1: dia \u00fatil)\n\n#### Vari\u00e1veis Clim\u00e1ticas:\n- **weathersit**: Condi\u00e7\u00e3o clim\u00e1tica\n  - 1: Claro, poucas nuvens, parcialmente nublado\n  - 2: Neblina + nublado, neblina + nuvens quebradas\n  - 3: Neve leve, chuva leve + trovoada\n  - 4: Chuva forte + granizo + trovoada + neblina, neve + neblina\n- **temp**: Temperatura normalizada em Celsius (dividida por 41\u00b0C m\u00e1x)\n- **atemp**: Sensa\u00e7\u00e3o t\u00e9rmica normalizada (dividida por 50\u00b0C m\u00e1x)\n- **hum**: Umidade relativa normalizada (dividida por 100)\n- **windspeed**: Velocidade do vento normalizada (dividida por 67 km/h)\n\n#### Vari\u00e1veis Target:\n- **casual**: Contagem de usu\u00e1rios casuais (n\u00e3o-registrados)\n- **registered**: Contagem de usu\u00e1rios registrados\n- **cnt**: **VARI\u00c1VEL TARGET** - Contagem total (casual + registered)\n\n### Conhecimento de Dom\u00ednio\n\n**Fatores que influenciam demanda de bikes:**\n\n1. **Hora do dia**: Picos em hor\u00e1rios de commute (8h-9h, 17h-18h)\n2. **Clima**: Dias ensolarados t\u00eam mais demanda que chuvosos\n3. **Temperatura**: Temperaturas moderadas favorecem uso de bicicletas\n4. **Dia da semana**: Padr\u00e3o diferente entre dias \u00fateis e fins de semana\n5. **Sazonalidade**: Mais uso em primavera/ver\u00e3o vs. inverno\"\"\")\n</pre> # Se\u00e7\u00e3o 4: Explica\u00e7\u00e3o do Dataset add_markdown_cell(\"\"\"## 4. Explica\u00e7\u00e3o Detalhada do Dataset  ### Contexto do Problema  O dataset cont\u00e9m registros hor\u00e1rios de um sistema de bike-sharing em Washington D.C. durante 2011 e 2012. Sistemas de compartilhamento de bicicletas automatizam o processo de aluguel/devolu\u00e7\u00e3o atrav\u00e9s de quiosques distribu\u00eddos pela cidade.  ### Descri\u00e7\u00e3o das Features  #### Vari\u00e1veis Temporais: - **instant**: \u00cdndice do registro (n\u00e3o ser\u00e1 usado como feature) - **dteday**: Data (formato YYYY-MM-DD) - **season**: Esta\u00e7\u00e3o do ano (1: primavera, 2: ver\u00e3o, 3: outono, 4: inverno) - **yr**: Ano (0: 2011, 1: 2012) - **mnth**: M\u00eas (1 a 12) - **hr**: Hora do dia (0 a 23) - **holiday**: Dia \u00e9 feriado (0: n\u00e3o, 1: sim) - **weekday**: Dia da semana (0: domingo ... 6: s\u00e1bado) - **workingday**: Dia \u00fatil (0: fim de semana/feriado, 1: dia \u00fatil)  #### Vari\u00e1veis Clim\u00e1ticas: - **weathersit**: Condi\u00e7\u00e3o clim\u00e1tica   - 1: Claro, poucas nuvens, parcialmente nublado   - 2: Neblina + nublado, neblina + nuvens quebradas   - 3: Neve leve, chuva leve + trovoada   - 4: Chuva forte + granizo + trovoada + neblina, neve + neblina - **temp**: Temperatura normalizada em Celsius (dividida por 41\u00b0C m\u00e1x) - **atemp**: Sensa\u00e7\u00e3o t\u00e9rmica normalizada (dividida por 50\u00b0C m\u00e1x) - **hum**: Umidade relativa normalizada (dividida por 100) - **windspeed**: Velocidade do vento normalizada (dividida por 67 km/h)  #### Vari\u00e1veis Target: - **casual**: Contagem de usu\u00e1rios casuais (n\u00e3o-registrados) - **registered**: Contagem de usu\u00e1rios registrados - **cnt**: **VARI\u00c1VEL TARGET** - Contagem total (casual + registered)  ### Conhecimento de Dom\u00ednio  **Fatores que influenciam demanda de bikes:**  1. **Hora do dia**: Picos em hor\u00e1rios de commute (8h-9h, 17h-18h) 2. **Clima**: Dias ensolarados t\u00eam mais demanda que chuvosos 3. **Temperatura**: Temperaturas moderadas favorecem uso de bicicletas 4. **Dia da semana**: Padr\u00e3o diferente entre dias \u00fateis e fins de semana 5. **Sazonalidade**: Mais uso em primavera/ver\u00e3o vs. inverno\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Informa\u00e7\u00f5es detalhadas sobre o dataset\nprint(\"INFORMA\u00c7\u00d5ES DO DATASET\")\nprint(\"=\"*80)\ndf.info()\n\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"ESTAT\u00cdSTICAS DESCRITIVAS\")\nprint(\"=\"*80)\ndf.describe().round(2)\"\"\")\n</pre> add_code_cell(\"\"\"# Informa\u00e7\u00f5es detalhadas sobre o dataset print(\"INFORMA\u00c7\u00d5ES DO DATASET\") print(\"=\"*80) df.info()  print(\"\\\\n\" + \"=\"*80) print(\"ESTAT\u00cdSTICAS DESCRITIVAS\") print(\"=\"*80) df.describe().round(2)\"\"\") In\u00a0[\u00a0]: Copied! <pre># Verifica\u00e7\u00e3o de problemas nos dados\nadd_markdown_cell(\"\"\"### Identifica\u00e7\u00e3o de Potenciais Problemas\"\"\")\n</pre> # Verifica\u00e7\u00e3o de problemas nos dados add_markdown_cell(\"\"\"### Identifica\u00e7\u00e3o de Potenciais Problemas\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Verifica\u00e7\u00e3o de valores ausentes\nprint(\"VALORES AUSENTES\")\nprint(\"=\"*80)\nmissing = df.isnull().sum()\nmissing_pct = (missing / len(df)) * 100\nmissing_df = pd.DataFrame({\n    'Coluna': missing.index,\n    'Valores Ausentes': missing.values,\n    'Percentual (%)': missing_pct.values\n})\nmissing_df = missing_df[missing_df['Valores Ausentes'] &gt; 0].sort_values('Valores Ausentes', ascending=False)\n\nif len(missing_df) == 0:\n    print(\"\u2713 Nenhum valor ausente encontrado!\")\nelse:\n    print(missing_df.to_string(index=False))\n\n# Verifica\u00e7\u00e3o de duplicatas\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"DUPLICATAS\")\nprint(\"=\"*80)\nduplicates = df.duplicated().sum()\nprint(f\"Linhas duplicadas: {duplicates}\")\nif duplicates == 0:\n    print(\"\u2713 Nenhuma duplicata encontrada!\")\n\n# Verifica\u00e7\u00e3o de outliers na vari\u00e1vel target\nprint(\"\\\\n\" + \"=\"*80)\nprint(\"AN\u00c1LISE DE OUTLIERS NA VARI\u00c1VEL TARGET (cnt)\")\nprint(\"=\"*80)\nQ1 = df['cnt'].quantile(0.25)\nQ3 = df['cnt'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\noutliers = df[(df['cnt'] &lt; lower_bound) | (df['cnt'] &gt; upper_bound)]\nprint(f\"Q1 (25%): {Q1:.0f}\")\nprint(f\"Q3 (75%): {Q3:.0f}\")\nprint(f\"IQR: {IQR:.0f}\")\nprint(f\"Limite inferior: {lower_bound:.0f}\")\nprint(f\"Limite superior: {upper_bound:.0f}\")\nprint(f\"Outliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\nprint(f\"\\\\nNota: Outliers podem representar eventos especiais (ex: feriados, eventos clim\u00e1ticos extremos)\")\nprint(f\"Decis\u00e3o: Manter outliers pois representam varia\u00e7\u00e3o real do sistema\")\"\"\")\n</pre> add_code_cell(\"\"\"# Verifica\u00e7\u00e3o de valores ausentes print(\"VALORES AUSENTES\") print(\"=\"*80) missing = df.isnull().sum() missing_pct = (missing / len(df)) * 100 missing_df = pd.DataFrame({     'Coluna': missing.index,     'Valores Ausentes': missing.values,     'Percentual (%)': missing_pct.values }) missing_df = missing_df[missing_df['Valores Ausentes'] &gt; 0].sort_values('Valores Ausentes', ascending=False)  if len(missing_df) == 0:     print(\"\u2713 Nenhum valor ausente encontrado!\") else:     print(missing_df.to_string(index=False))  # Verifica\u00e7\u00e3o de duplicatas print(\"\\\\n\" + \"=\"*80) print(\"DUPLICATAS\") print(\"=\"*80) duplicates = df.duplicated().sum() print(f\"Linhas duplicadas: {duplicates}\") if duplicates == 0:     print(\"\u2713 Nenhuma duplicata encontrada!\")  # Verifica\u00e7\u00e3o de outliers na vari\u00e1vel target print(\"\\\\n\" + \"=\"*80) print(\"AN\u00c1LISE DE OUTLIERS NA VARI\u00c1VEL TARGET (cnt)\") print(\"=\"*80) Q1 = df['cnt'].quantile(0.25) Q3 = df['cnt'].quantile(0.75) IQR = Q3 - Q1 lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR  outliers = df[(df['cnt'] &lt; lower_bound) | (df['cnt'] &gt; upper_bound)] print(f\"Q1 (25%): {Q1:.0f}\") print(f\"Q3 (75%): {Q3:.0f}\") print(f\"IQR: {IQR:.0f}\") print(f\"Limite inferior: {lower_bound:.0f}\") print(f\"Limite superior: {upper_bound:.0f}\") print(f\"Outliers detectados: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\") print(f\"\\\\nNota: Outliers podem representar eventos especiais (ex: feriados, eventos clim\u00e1ticos extremos)\") print(f\"Decis\u00e3o: Manter outliers pois representam varia\u00e7\u00e3o real do sistema\")\"\"\") In\u00a0[\u00a0]: Copied! <pre># Visualiza\u00e7\u00f5es explorat\u00f3rias\nadd_markdown_cell(\"\"\"### Visualiza\u00e7\u00f5es Explorat\u00f3rias\"\"\")\n</pre> # Visualiza\u00e7\u00f5es explorat\u00f3rias add_markdown_cell(\"\"\"### Visualiza\u00e7\u00f5es Explorat\u00f3rias\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Distribui\u00e7\u00e3o da vari\u00e1vel target\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\n# Histograma\naxes[0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7)\naxes[0].set_xlabel('Contagem de Alugu\u00e9is', fontsize=12)\naxes[0].set_ylabel('Frequ\u00eancia', fontsize=12)\naxes[0].set_title('Distribui\u00e7\u00e3o da Vari\u00e1vel Target (cnt)', fontsize=14, fontweight='bold')\naxes[0].axvline(df['cnt'].mean(), color='red', linestyle='--', label=f'M\u00e9dia: {df[\"cnt\"].mean():.0f}')\naxes[0].axvline(df['cnt'].median(), color='green', linestyle='--', label=f'Mediana: {df[\"cnt\"].median():.0f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Boxplot\naxes[1].boxplot(df['cnt'], vert=True)\naxes[1].set_ylabel('Contagem de Alugu\u00e9is', fontsize=12)\naxes[1].set_title('Boxplot da Vari\u00e1vel Target', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Assimetria (Skewness): {df['cnt'].skew():.2f}\")\nprint(f\"Curtose (Kurtosis): {df['cnt'].kurtosis():.2f}\")\nprint(\"\\\\nInterpreta\u00e7\u00e3o: Distribui\u00e7\u00e3o positivamente assim\u00e9trica (cauda \u00e0 direita)\")\nprint(\"Muitas horas com baixa demanda, poucas horas com alta demanda\")\"\"\")\n</pre> add_code_cell(\"\"\"# Distribui\u00e7\u00e3o da vari\u00e1vel target fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # Histograma axes[0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7) axes[0].set_xlabel('Contagem de Alugu\u00e9is', fontsize=12) axes[0].set_ylabel('Frequ\u00eancia', fontsize=12) axes[0].set_title('Distribui\u00e7\u00e3o da Vari\u00e1vel Target (cnt)', fontsize=14, fontweight='bold') axes[0].axvline(df['cnt'].mean(), color='red', linestyle='--', label=f'M\u00e9dia: {df[\"cnt\"].mean():.0f}') axes[0].axvline(df['cnt'].median(), color='green', linestyle='--', label=f'Mediana: {df[\"cnt\"].median():.0f}') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Boxplot axes[1].boxplot(df['cnt'], vert=True) axes[1].set_ylabel('Contagem de Alugu\u00e9is', fontsize=12) axes[1].set_title('Boxplot da Vari\u00e1vel Target', fontsize=14, fontweight='bold') axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"Assimetria (Skewness): {df['cnt'].skew():.2f}\") print(f\"Curtose (Kurtosis): {df['cnt'].kurtosis():.2f}\") print(\"\\\\nInterpreta\u00e7\u00e3o: Distribui\u00e7\u00e3o positivamente assim\u00e9trica (cauda \u00e0 direita)\") print(\"Muitas horas com baixa demanda, poucas horas com alta demanda\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code_cell(\"\"\"# Correla\u00e7\u00e3o entre features num\u00e9ricas\nnumeric_cols = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\ncorrelation_matrix = df[numeric_cols].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\nplt.title('Matriz de Correla\u00e7\u00e3o - Features Num\u00e9ricas', fontsize=16, fontweight='bold', pad=20)\nplt.tight_layout()\nplt.show()\n\nprint(\"CORRELA\u00c7\u00d5ES COM A VARI\u00c1VEL TARGET (cnt):\")\nprint(\"=\"*80)\ntarget_corr = correlation_matrix['cnt'].sort_values(ascending=False)\nfor feature, corr in target_corr.items():\n    if feature != 'cnt':\n        print(f\"{feature:15s}: {corr:+.3f}\")\"\"\")\n</pre> add_code_cell(\"\"\"# Correla\u00e7\u00e3o entre features num\u00e9ricas numeric_cols = ['temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt'] correlation_matrix = df[numeric_cols].corr()  plt.figure(figsize=(10, 8)) sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',             center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}) plt.title('Matriz de Correla\u00e7\u00e3o - Features Num\u00e9ricas', fontsize=16, fontweight='bold', pad=20) plt.tight_layout() plt.show()  print(\"CORRELA\u00c7\u00d5ES COM A VARI\u00c1VEL TARGET (cnt):\") print(\"=\"*80) target_corr = correlation_matrix['cnt'].sort_values(ascending=False) for feature, corr in target_corr.items():     if feature != 'cnt':         print(f\"{feature:15s}: {corr:+.3f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre># Salvar o notebook\noutput_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> # Salvar o notebook output_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(output_path, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> with open(output_path, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"\u2713 Notebook criado com sucesso em: {output_path}\")\nprint(f\"\u2713 Total de c\u00e9lulas: {len(notebook['cells'])}\")\n</pre> print(f\"\u2713 Notebook criado com sucesso em: {output_path}\") print(f\"\u2713 Total de c\u00e9lulas: {len(notebook['cells'])}\")"},{"location":"portfolio/neural-networks/projects/2/create_regression_notebook/#celulas-do-notebook","title":"============================================================================== C\u00c9LULAS DO NOTEBOOK\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/create_simple_notebook/","title":"Create simple notebook","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nCria um notebook de regress\u00e3o simples e direto ao ponto.\nSem formalidades excessivas.\n\"\"\"\n</pre> \"\"\" Cria um notebook de regress\u00e3o simples e direto ao ponto. Sem formalidades excessivas. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre>notebook = {\n    \"cells\": [],\n    \"metadata\": {\n        \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},\n        \"language_info\": {\"name\": \"python\", \"version\": \"3.8.0\"}\n    },\n    \"nbformat\": 4,\n    \"nbformat_minor\": 4\n}\n</pre> notebook = {     \"cells\": [],     \"metadata\": {         \"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},         \"language_info\": {\"name\": \"python\", \"version\": \"3.8.0\"}     },     \"nbformat\": 4,     \"nbformat_minor\": 4 } In\u00a0[\u00a0]: Copied! <pre>def add_md(text):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"markdown\",\n        \"metadata\": {},\n        \"source\": text.split(\"\\n\")\n    })\n</pre> def add_md(text):     notebook[\"cells\"].append({         \"cell_type\": \"markdown\",         \"metadata\": {},         \"source\": text.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>def add_code(text):\n    notebook[\"cells\"].append({\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": text.split(\"\\n\")\n    })\n</pre> def add_code(text):     notebook[\"cells\"].append({         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": text.split(\"\\n\")     }) In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"# Projeto de Regress\u00e3o - MLP\n\n**Objetivo:** Prever demanda de bicicletas compartilhadas usando uma rede neural MLP implementada do zero.\n\n**Dataset:** Bike Sharing (UCI) - 17k amostras com dados de clima e hor\u00e1rio\n\n**O que vamos fazer:**\n1. Carregar e explorar os dados\n2. Limpar e preparar os dados\n3. Implementar MLP do zero (s\u00f3 NumPy)\n4. Treinar o modelo\n5. Avaliar resultados\"\"\")\n</pre> add_md(\"\"\"# Projeto de Regress\u00e3o - MLP  **Objetivo:** Prever demanda de bicicletas compartilhadas usando uma rede neural MLP implementada do zero.  **Dataset:** Bike Sharing (UCI) - 17k amostras com dados de clima e hor\u00e1rio  **O que vamos fazer:** 1. Carregar e explorar os dados 2. Limpar e preparar os dados 3. Implementar MLP do zero (s\u00f3 NumPy) 4. Treinar o modelo 5. Avaliar resultados\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 1. Setup e Dataset\n\nPrimeiro, baixe o dataset:\n- Link: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n- Arquivo: `hour.csv`\n- Ou execute: `python download_dataset.py`\"\"\")\n</pre> add_md(\"\"\"## 1. Setup e Dataset  Primeiro, baixe o dataset: - Link: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset - Arquivo: `hour.csv` - Ou execute: `python download_dataset.py`\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)\nprint(\"Pronto!\")\"\"\")\n</pre> add_code(\"\"\"import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings('ignore')  np.random.seed(42) print(\"Pronto!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Carregar dados\ndf = pd.read_csv('hour.csv')\nprint(f\"Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas\")\ndf.head()\"\"\")\n</pre> add_code(\"\"\"# Carregar dados df = pd.read_csv('hour.csv') print(f\"Dataset: {df.shape[0]} linhas, {df.shape[1]} colunas\") df.head()\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 2. Explora\u00e7\u00e3o R\u00e1pida\n\nVamos ver o que temos e entender os dados.\"\"\")\n</pre> add_md(\"\"\"## 2. Explora\u00e7\u00e3o R\u00e1pida  Vamos ver o que temos e entender os dados.\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Info b\u00e1sica\nprint(\"Informa\u00e7\u00f5es do Dataset:\")\nprint(\"=\"*60)\ndf.info()\nprint(\"\\\\nEstat\u00edsticas:\")\ndf.describe()\"\"\")\n</pre> add_code(\"\"\"# Info b\u00e1sica print(\"Informa\u00e7\u00f5es do Dataset:\") print(\"=\"*60) df.info() print(\"\\\\nEstat\u00edsticas:\") df.describe()\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Vari\u00e1vel target: cnt (contagem de alugu\u00e9is)\nplt.figure(figsize=(14, 4))\n\nplt.subplot(1, 3, 1)\nplt.hist(df['cnt'], bins=50, edgecolor='black')\nplt.title('Distribui\u00e7\u00e3o do Target')\nplt.xlabel('N\u00famero de alugu\u00e9is')\n\nplt.subplot(1, 3, 2)\ndf.groupby('hr')['cnt'].mean().plot(marker='o')\nplt.title('Demanda por Hora do Dia')\nplt.xlabel('Hora')\nplt.ylabel('M\u00e9dia de alugu\u00e9is')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.scatter(df['temp'], df['cnt'], alpha=0.2, s=5)\nplt.title('Temperatura vs Demanda')\nplt.xlabel('Temperatura (normalizada)')\nplt.ylabel('Alugu\u00e9is')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Target (cnt): min={df['cnt'].min()}, max={df['cnt'].max()}, m\u00e9dia={df['cnt'].mean():.1f}\")\"\"\")\n</pre> add_code(\"\"\"# Vari\u00e1vel target: cnt (contagem de alugu\u00e9is) plt.figure(figsize=(14, 4))  plt.subplot(1, 3, 1) plt.hist(df['cnt'], bins=50, edgecolor='black') plt.title('Distribui\u00e7\u00e3o do Target') plt.xlabel('N\u00famero de alugu\u00e9is')  plt.subplot(1, 3, 2) df.groupby('hr')['cnt'].mean().plot(marker='o') plt.title('Demanda por Hora do Dia') plt.xlabel('Hora') plt.ylabel('M\u00e9dia de alugu\u00e9is') plt.grid(True, alpha=0.3)  plt.subplot(1, 3, 3) plt.scatter(df['temp'], df['cnt'], alpha=0.2, s=5) plt.title('Temperatura vs Demanda') plt.xlabel('Temperatura (normalizada)') plt.ylabel('Alugu\u00e9is')  plt.tight_layout() plt.show()  print(f\"Target (cnt): min={df['cnt'].min()}, max={df['cnt'].max()}, m\u00e9dia={df['cnt'].mean():.1f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 3. Prepara\u00e7\u00e3o dos Dados\n\nVamos limpar e preparar os dados para o modelo.\"\"\")\n</pre> add_md(\"\"\"## 3. Prepara\u00e7\u00e3o dos Dados  Vamos limpar e preparar os dados para o modelo.\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Remover colunas que n\u00e3o vamos usar\ndf_clean = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])\n\n# Transformar vari\u00e1veis c\u00edclicas (hora, m\u00eas, dia da semana) em sin/cos\n# Isso ajuda o modelo a entender que hora 23 est\u00e1 perto da hora 0\ndef add_cyclical(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n    return data\n\ndf_clean = add_cyclical(df_clean, 'hr', 24)\ndf_clean = add_cyclical(df_clean, 'mnth', 12)\ndf_clean = add_cyclical(df_clean, 'weekday', 7)\ndf_clean = df_clean.drop(columns=['hr', 'mnth', 'weekday'])\n\nprint(f\"Features ap\u00f3s prepara\u00e7\u00e3o: {df_clean.shape[1]} colunas\")\nprint(f\"Lista de features: {list(df_clean.columns)}\")\"\"\")\n</pre> add_code(\"\"\"# Remover colunas que n\u00e3o vamos usar df_clean = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])  # Transformar vari\u00e1veis c\u00edclicas (hora, m\u00eas, dia da semana) em sin/cos # Isso ajuda o modelo a entender que hora 23 est\u00e1 perto da hora 0 def add_cyclical(data, col, max_val):     data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)     data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)     return data  df_clean = add_cyclical(df_clean, 'hr', 24) df_clean = add_cyclical(df_clean, 'mnth', 12) df_clean = add_cyclical(df_clean, 'weekday', 7) df_clean = df_clean.drop(columns=['hr', 'mnth', 'weekday'])  print(f\"Features ap\u00f3s prepara\u00e7\u00e3o: {df_clean.shape[1]} colunas\") print(f\"Lista de features: {list(df_clean.columns)}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Separar X (features) e y (target)\nX = df_clean.drop(columns=['cnt']).values\ny = df_clean['cnt'].values.reshape(-1, 1)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Normalizar (z-score)\nX_mean, X_std = X.mean(axis=0), X.std(axis=0)\ny_mean, y_std = y.mean(), y.std()\n\nX = (X - X_mean) / (X_std + 1e-8)\ny = (y - y_mean) / y_std\n\nprint(f\"\\\\nDados normalizados!\")\nprint(f\"y: m\u00e9dia={y.mean():.3f}, std={y.std():.3f}\")\"\"\")\n</pre> add_code(\"\"\"# Separar X (features) e y (target) X = df_clean.drop(columns=['cnt']).values y = df_clean['cnt'].values.reshape(-1, 1)  print(f\"X shape: {X.shape}\") print(f\"y shape: {y.shape}\")  # Normalizar (z-score) X_mean, X_std = X.mean(axis=0), X.std(axis=0) y_mean, y_std = y.mean(), y.std()  X = (X - X_mean) / (X_std + 1e-8) y = (y - y_mean) / y_std  print(f\"\\\\nDados normalizados!\") print(f\"y: m\u00e9dia={y.mean():.3f}, std={y.std():.3f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Dividir em train/val/test (70/15/15)\nn = X.shape[0]\nidx = np.arange(n)\nnp.random.shuffle(idx)\n\ntrain_size = int(0.70 * n)\nval_size = int(0.15 * n)\n\ntrain_idx = idx[:train_size]\nval_idx = idx[train_size:train_size + val_size]\ntest_idx = idx[train_size + val_size:]\n\nX_train, y_train = X[train_idx], y[train_idx]\nX_val, y_val = X[val_idx], y[val_idx]\nX_test, y_test = X[test_idx], y[test_idx]\n\nprint(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\"\"\")\n</pre> add_code(\"\"\"# Dividir em train/val/test (70/15/15) n = X.shape[0] idx = np.arange(n) np.random.shuffle(idx)  train_size = int(0.70 * n) val_size = int(0.15 * n)  train_idx = idx[:train_size] val_idx = idx[train_size:train_size + val_size] test_idx = idx[train_size + val_size:]  X_train, y_train = X[train_idx], y[train_idx] X_val, y_val = X[val_idx], y[val_idx] X_test, y_test = X[test_idx], y[test_idx]  print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 4. Implementa\u00e7\u00e3o do MLP\n\nRede neural implementada do zero com NumPy.\n\n**Arquitetura:** Input(15) \u2192 Hidden(64) \u2192 Hidden(32) \u2192 Hidden(16) \u2192 Output(1)\"\"\")\n</pre> add_md(\"\"\"## 4. Implementa\u00e7\u00e3o do MLP  Rede neural implementada do zero com NumPy.  **Arquitetura:** Input(15) \u2192 Hidden(64) \u2192 Hidden(32) \u2192 Hidden(16) \u2192 Output(1)\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"class MLP:\n    def __init__(self, layers, lr=0.001, reg=0.001):\n        self.lr = lr\n        self.reg = reg\n        self.weights = []\n        self.biases = []\n\n        # Inicializar pesos (He initialization)\n        for i in range(len(layers) - 1):\n            W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n        self.train_loss = []\n        self.val_loss = []\n\n    def relu(self, x):\n        return np.maximum(0, x)\n\n    def relu_grad(self, x):\n        return (x &gt; 0).astype(float)\n\n    def forward(self, X):\n        self.cache = {'A': [X], 'Z': []}\n        A = X\n\n        for i in range(len(self.weights) - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            self.cache['Z'].append(Z)\n            self.cache['A'].append(A)\n\n        # Output layer (linear)\n        Z = A @ self.weights[-1] + self.biases[-1]\n        self.cache['Z'].append(Z)\n        self.cache['A'].append(Z)\n\n        return Z\n\n    def loss(self, y_true, y_pred):\n        mse = np.mean((y_pred - y_true) ** 2)\n        l2 = sum(np.sum(W ** 2) for W in self.weights)\n        return mse + (self.reg / (2 * len(y_true))) * l2\n\n    def backward(self, y_true):\n        m = len(y_true)\n        y_pred = self.cache['A'][-1]\n        dA = (2.0 / m) * (y_pred - y_true)\n\n        grads_W = []\n        grads_b = []\n\n        for i in reversed(range(len(self.weights))):\n            A_prev = self.cache['A'][i]\n\n            dW = A_prev.T @ dA\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            dW += (self.reg / m) * self.weights[i]\n\n            grads_W.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = (dA @ self.weights[i].T) * self.relu_grad(self.cache['Z'][i-1])\n\n        # Update\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.lr * grads_W[i]\n            self.biases[i] -= self.lr * grads_b[i]\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15):\n        best_loss = float('inf')\n        patience_count = 0\n\n        for epoch in range(epochs):\n            # Mini-batch training\n            idx = np.arange(len(X_train))\n            np.random.shuffle(idx)\n\n            for start in range(0, len(X_train), batch_size):\n                end = min(start + batch_size, len(X_train))\n                batch_idx = idx[start:end]\n\n                X_batch = X_train[batch_idx]\n                y_batch = y_train[batch_idx]\n\n                y_pred = self.forward(X_batch)\n                self.backward(y_batch)\n\n            # Calcular loss\n            train_pred = self.forward(X_train)\n            val_pred = self.forward(X_val)\n\n            train_loss = self.loss(y_train, train_pred)\n            val_loss = self.loss(y_val, val_pred)\n\n            self.train_loss.append(train_loss)\n            self.val_loss.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                patience_count = 0\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_count += 1\n\n            if (epoch + 1) % 20 == 0:\n                print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Best: {best_loss:.4f}\")\n\n            if patience_count &gt;= patience:\n                print(f\"\\\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        self.weights = self.best_weights\n        self.biases = self.best_biases\n\n    def predict(self, X):\n        A = X\n        for i in range(len(self.weights) - 1):\n            A = self.relu(A @ self.weights[i] + self.biases[i])\n        return A @ self.weights[-1] + self.biases[-1]\n\nprint(\"Classe MLP implementada!\")\"\"\")\n</pre> add_code(\"\"\"class MLP:     def __init__(self, layers, lr=0.001, reg=0.001):         self.lr = lr         self.reg = reg         self.weights = []         self.biases = []          # Inicializar pesos (He initialization)         for i in range(len(layers) - 1):             W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])             b = np.zeros((1, layers[i+1]))             self.weights.append(W)             self.biases.append(b)          self.train_loss = []         self.val_loss = []      def relu(self, x):         return np.maximum(0, x)      def relu_grad(self, x):         return (x &gt; 0).astype(float)      def forward(self, X):         self.cache = {'A': [X], 'Z': []}         A = X          for i in range(len(self.weights) - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             self.cache['Z'].append(Z)             self.cache['A'].append(A)          # Output layer (linear)         Z = A @ self.weights[-1] + self.biases[-1]         self.cache['Z'].append(Z)         self.cache['A'].append(Z)          return Z      def loss(self, y_true, y_pred):         mse = np.mean((y_pred - y_true) ** 2)         l2 = sum(np.sum(W ** 2) for W in self.weights)         return mse + (self.reg / (2 * len(y_true))) * l2      def backward(self, y_true):         m = len(y_true)         y_pred = self.cache['A'][-1]         dA = (2.0 / m) * (y_pred - y_true)          grads_W = []         grads_b = []          for i in reversed(range(len(self.weights))):             A_prev = self.cache['A'][i]              dW = A_prev.T @ dA             db = np.sum(dA, axis=0, keepdims=True)              dW += (self.reg / m) * self.weights[i]              grads_W.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = (dA @ self.weights[i].T) * self.relu_grad(self.cache['Z'][i-1])          # Update         for i in range(len(self.weights)):             self.weights[i] -= self.lr * grads_W[i]             self.biases[i] -= self.lr * grads_b[i]      def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15):         best_loss = float('inf')         patience_count = 0          for epoch in range(epochs):             # Mini-batch training             idx = np.arange(len(X_train))             np.random.shuffle(idx)              for start in range(0, len(X_train), batch_size):                 end = min(start + batch_size, len(X_train))                 batch_idx = idx[start:end]                  X_batch = X_train[batch_idx]                 y_batch = y_train[batch_idx]                  y_pred = self.forward(X_batch)                 self.backward(y_batch)              # Calcular loss             train_pred = self.forward(X_train)             val_pred = self.forward(X_val)              train_loss = self.loss(y_train, train_pred)             val_loss = self.loss(y_val, val_pred)              self.train_loss.append(train_loss)             self.val_loss.append(val_loss)              # Early stopping             if val_loss &lt; best_loss:                 best_loss = val_loss                 patience_count = 0                 self.best_weights = [W.copy() for W in self.weights]                 self.best_biases = [b.copy() for b in self.biases]             else:                 patience_count += 1              if (epoch + 1) % 20 == 0:                 print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | Best: {best_loss:.4f}\")              if patience_count &gt;= patience:                 print(f\"\\\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")                 break          # Restaurar melhores pesos         self.weights = self.best_weights         self.biases = self.best_biases      def predict(self, X):         A = X         for i in range(len(self.weights) - 1):             A = self.relu(A @ self.weights[i] + self.biases[i])         return A @ self.weights[-1] + self.biases[-1]  print(\"Classe MLP implementada!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 5. Treinamento\n\nVamos treinar o modelo. Isso pode levar 2-3 minutos.\"\"\")\n</pre> add_md(\"\"\"## 5. Treinamento  Vamos treinar o modelo. Isso pode levar 2-3 minutos.\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Criar e treinar modelo\nmodel = MLP(\n    layers=[15, 64, 32, 16, 1],\n    lr=0.001,\n    reg=0.001\n)\n\nprint(\"Treinando...\")\nmodel.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15)\nprint(\"\\\\nTreinamento conclu\u00eddo!\")\"\"\")\n</pre> add_code(\"\"\"# Criar e treinar modelo model = MLP(     layers=[15, 64, 32, 16, 1],     lr=0.001,     reg=0.001 )  print(\"Treinando...\") model.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15) print(\"\\\\nTreinamento conclu\u00eddo!\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Curvas de loss\nplt.figure(figsize=(10, 4))\nplt.plot(model.train_loss, label='Train', linewidth=2)\nplt.plot(model.val_loss, label='Validation', linewidth=2)\nplt.xlabel('\u00c9poca')\nplt.ylabel('Loss')\nplt.title('Converg\u00eancia do Modelo')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"Loss final - Train: {model.train_loss[-1]:.4f} | Val: {model.val_loss[-1]:.4f}\")\"\"\")\n</pre> add_code(\"\"\"# Curvas de loss plt.figure(figsize=(10, 4)) plt.plot(model.train_loss, label='Train', linewidth=2) plt.plot(model.val_loss, label='Validation', linewidth=2) plt.xlabel('\u00c9poca') plt.ylabel('Loss') plt.title('Converg\u00eancia do Modelo') plt.legend() plt.grid(True, alpha=0.3) plt.show()  print(f\"Loss final - Train: {model.train_loss[-1]:.4f} | Val: {model.val_loss[-1]:.4f}\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 6. Avalia\u00e7\u00e3o\n\nVamos ver como o modelo se saiu no conjunto de teste.\"\"\")\n</pre> add_md(\"\"\"## 6. Avalia\u00e7\u00e3o  Vamos ver como o modelo se saiu no conjunto de teste.\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Predi\u00e7\u00f5es (desnormalizar para valores reais)\ny_pred_test = model.predict(X_test) * y_std + y_mean\ny_true_test = y_test * y_std + y_mean\n\ny_pred_train = model.predict(X_train) * y_std + y_mean\ny_true_train = y_train * y_std + y_mean\n\n# M\u00e9tricas\ndef calc_metrics(y_true, y_pred):\n    mae = np.mean(np.abs(y_true - y_pred))\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n\n    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n\ntrain_metrics = calc_metrics(y_true_train, y_pred_train)\ntest_metrics = calc_metrics(y_true_test, y_pred_test)\n\nprint(\"RESULTADOS:\")\nprint(\"=\"*60)\nprint(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15}\")\nprint(\"-\"*60)\nprint(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f}\")\nprint(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f}\")\nprint(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f}\")\nprint(\"=\"*60)\n\n# Baseline (sempre prever a m\u00e9dia)\nbaseline_pred = np.full_like(y_true_test, y_true_train.mean())\nbaseline_metrics = calc_metrics(y_true_test, baseline_pred)\n\nprint(f\"\\\\nBaseline (prever m\u00e9dia): RMSE = {baseline_metrics['RMSE']:.2f}\")\nprint(f\"Nosso modelo: RMSE = {test_metrics['RMSE']:.2f}\")\nprint(f\"Melhoria: {((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100):.1f}%\")\"\"\")\n</pre> add_code(\"\"\"# Predi\u00e7\u00f5es (desnormalizar para valores reais) y_pred_test = model.predict(X_test) * y_std + y_mean y_true_test = y_test * y_std + y_mean  y_pred_train = model.predict(X_train) * y_std + y_mean y_true_train = y_train * y_std + y_mean  # M\u00e9tricas def calc_metrics(y_true, y_pred):     mae = np.mean(np.abs(y_true - y_pred))     mse = np.mean((y_true - y_pred) ** 2)     rmse = np.sqrt(mse)      ss_res = np.sum((y_true - y_pred) ** 2)     ss_tot = np.sum((y_true - y_true.mean()) ** 2)     r2 = 1 - (ss_res / ss_tot)      return {'MAE': mae, 'RMSE': rmse, 'R2': r2}  train_metrics = calc_metrics(y_true_train, y_pred_train) test_metrics = calc_metrics(y_true_test, y_pred_test)  print(\"RESULTADOS:\") print(\"=\"*60) print(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15}\") print(\"-\"*60) print(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f}\") print(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f}\") print(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f}\") print(\"=\"*60)  # Baseline (sempre prever a m\u00e9dia) baseline_pred = np.full_like(y_true_test, y_true_train.mean()) baseline_metrics = calc_metrics(y_true_test, baseline_pred)  print(f\"\\\\nBaseline (prever m\u00e9dia): RMSE = {baseline_metrics['RMSE']:.2f}\") print(f\"Nosso modelo: RMSE = {test_metrics['RMSE']:.2f}\") print(f\"Melhoria: {((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100):.1f}%\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_code(\"\"\"# Visualiza\u00e7\u00f5es\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# 1. Predito vs Real\naxes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=10)\nlim = [y_true_test.min(), y_true_test.max()]\naxes[0].plot(lim, lim, 'r--', linewidth=2)\naxes[0].set_xlabel('Real')\naxes[0].set_ylabel('Predito')\naxes[0].set_title(f'Predi\u00e7\u00f5es vs Real (R\u00b2={test_metrics[\"R2\"]:.3f})')\naxes[0].grid(True, alpha=0.3)\n\n# 2. Res\u00edduos\nresiduals = (y_true_test - y_pred_test).flatten()\naxes[1].scatter(y_pred_test, residuals, alpha=0.4, s=10)\naxes[1].axhline(0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Predito')\naxes[1].set_ylabel('Res\u00edduo')\naxes[1].set_title('An\u00e1lise de Res\u00edduos')\naxes[1].grid(True, alpha=0.3)\n\n# 3. Distribui\u00e7\u00e3o dos res\u00edduos\naxes[2].hist(residuals, bins=50, edgecolor='black')\naxes[2].axvline(0, color='r', linestyle='--', linewidth=2)\naxes[2].set_xlabel('Res\u00edduo')\naxes[2].set_ylabel('Frequ\u00eancia')\naxes[2].set_title(f'Distribui\u00e7\u00e3o dos Res\u00edduos (m\u00e9dia={residuals.mean():.2f})')\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\\\nEstat\u00edsticas dos res\u00edduos:\")\nprint(f\"M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\")\nprint(f\"Desvio padr\u00e3o: {residuals.std():.2f}\")\nprint(f\"50% dos erros &lt; {np.percentile(np.abs(residuals), 50):.1f} bikes\")\nprint(f\"95% dos erros &lt; {np.percentile(np.abs(residuals), 95):.1f} bikes\")\"\"\")\n</pre> add_code(\"\"\"# Visualiza\u00e7\u00f5es fig, axes = plt.subplots(1, 3, figsize=(16, 4))  # 1. Predito vs Real axes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=10) lim = [y_true_test.min(), y_true_test.max()] axes[0].plot(lim, lim, 'r--', linewidth=2) axes[0].set_xlabel('Real') axes[0].set_ylabel('Predito') axes[0].set_title(f'Predi\u00e7\u00f5es vs Real (R\u00b2={test_metrics[\"R2\"]:.3f})') axes[0].grid(True, alpha=0.3)  # 2. Res\u00edduos residuals = (y_true_test - y_pred_test).flatten() axes[1].scatter(y_pred_test, residuals, alpha=0.4, s=10) axes[1].axhline(0, color='r', linestyle='--', linewidth=2) axes[1].set_xlabel('Predito') axes[1].set_ylabel('Res\u00edduo') axes[1].set_title('An\u00e1lise de Res\u00edduos') axes[1].grid(True, alpha=0.3)  # 3. Distribui\u00e7\u00e3o dos res\u00edduos axes[2].hist(residuals, bins=50, edgecolor='black') axes[2].axvline(0, color='r', linestyle='--', linewidth=2) axes[2].set_xlabel('Res\u00edduo') axes[2].set_ylabel('Frequ\u00eancia') axes[2].set_title(f'Distribui\u00e7\u00e3o dos Res\u00edduos (m\u00e9dia={residuals.mean():.2f})') axes[2].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"\\\\nEstat\u00edsticas dos res\u00edduos:\") print(f\"M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\") print(f\"Desvio padr\u00e3o: {residuals.std():.2f}\") print(f\"50% dos erros &lt; {np.percentile(np.abs(residuals), 50):.1f} bikes\") print(f\"95% dos erros &lt; {np.percentile(np.abs(residuals), 95):.1f} bikes\")\"\"\") In\u00a0[\u00a0]: Copied! <pre>add_md(\"\"\"## 7. Conclus\u00e3o\n\n### O que fizemos:\n- Implementamos um MLP do zero usando apenas NumPy\n- Treinamos para prever demanda de bikes\n- Obtivemos resultados melhores que o baseline\n\n### Limita\u00e7\u00f5es:\n- MLP n\u00e3o captura bem padr\u00f5es temporais (melhor seria LSTM)\n- Features podem ser melhoradas (adicionar lags, intera\u00e7\u00f5es)\n- Hiperpar\u00e2metros n\u00e3o foram otimizados\n\n### Melhorias poss\u00edveis:\n- Testar diferentes arquiteturas\n- Adicionar dropout\n- Usar Adam optimizer\n- Fazer grid search de hiperpar\u00e2metros\n\n### Refer\u00eancias:\n- Dataset: UCI ML Repository (Bike Sharing)\n- Implementa\u00e7\u00e3o: NumPy, conceitos de Deep Learning (Goodfellow et al.)\"\"\")\n</pre> add_md(\"\"\"## 7. Conclus\u00e3o  ### O que fizemos: - Implementamos um MLP do zero usando apenas NumPy - Treinamos para prever demanda de bikes - Obtivemos resultados melhores que o baseline  ### Limita\u00e7\u00f5es: - MLP n\u00e3o captura bem padr\u00f5es temporais (melhor seria LSTM) - Features podem ser melhoradas (adicionar lags, intera\u00e7\u00f5es) - Hiperpar\u00e2metros n\u00e3o foram otimizados  ### Melhorias poss\u00edveis: - Testar diferentes arquiteturas - Adicionar dropout - Usar Adam optimizer - Fazer grid search de hiperpar\u00e2metros  ### Refer\u00eancias: - Dataset: UCI ML Repository (Bike Sharing) - Implementa\u00e7\u00e3o: NumPy, conceitos de Deep Learning (Goodfellow et al.)\"\"\") In\u00a0[\u00a0]: Copied! <pre># Salvar\noutput = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\nwith open(output, 'w', encoding='utf-8') as f:\n    json.dump(notebook, f, ensure_ascii=False, indent=1)\n</pre> # Salvar output = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" with open(output, 'w', encoding='utf-8') as f:     json.dump(notebook, f, ensure_ascii=False, indent=1) In\u00a0[\u00a0]: Copied! <pre>print(f\"\u2713 Notebook simples criado!\")\nprint(f\"\u2713 Total: {len(notebook['cells'])} c\u00e9lulas\")\nprint(f\"\u2713 Arquivo: {output}\")\n</pre> print(f\"\u2713 Notebook simples criado!\") print(f\"\u2713 Total: {len(notebook['cells'])} c\u00e9lulas\") print(f\"\u2713 Arquivo: {output}\")"},{"location":"portfolio/neural-networks/projects/2/create_simple_notebook/#notebook-simplificado","title":"============================================================================== NOTEBOOK SIMPLIFICADO\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/download_dataset/","title":"Download dataset","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para baixar o dataset Bike Sharing automaticamente.\n\nUso:\n    python download_dataset.py\n\nO script tentar\u00e1 baixar o dataset do UCI Repository.\n\"\"\"\n</pre> \"\"\" Script para baixar o dataset Bike Sharing automaticamente.  Uso:     python download_dataset.py  O script tentar\u00e1 baixar o dataset do UCI Repository. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import urllib.request\nimport zipfile\nimport os\nimport sys\n</pre> import urllib.request import zipfile import os import sys In\u00a0[\u00a0]: Copied! <pre>def download_dataset():\n    \"\"\"Baixa e extrai o dataset Bike Sharing do UCI Repository.\"\"\"\n\n    print(\"=\"*80)\n    print(\"DOWNLOAD DO DATASET BIKE SHARING\")\n    print(\"=\"*80)\n\n    # URL do dataset\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n    zip_filename = \"Bike-Sharing-Dataset.zip\"\n    target_file = \"hour.csv\"\n\n    # Verificar se j\u00e1 existe\n    if os.path.exists(target_file):\n        print(f\"\\n[INFO] Arquivo '{target_file}' j\u00e1 existe!\")\n        response = input(\"Deseja baixar novamente? (s/n): \")\n        if response.lower() != 's':\n            print(\"[INFO] Download cancelado.\")\n            return True\n\n    try:\n        # Baixar arquivo\n        print(f\"\\n[1/3] Baixando dataset de: {url}\")\n        print(\"[INFO] Aguarde, isso pode levar alguns segundos...\")\n\n        urllib.request.urlretrieve(url, zip_filename)\n        print(f\"[OK] Arquivo baixado: {zip_filename}\")\n\n        # Extrair arquivo\n        print(f\"\\n[2/3] Extraindo arquivo ZIP...\")\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        print(f\"[OK] Arquivos extra\u00eddos\")\n\n        # Verificar se hour.csv existe\n        print(f\"\\n[3/3] Verificando arquivo '{target_file}'...\")\n        if os.path.exists(target_file):\n            file_size = os.path.getsize(target_file) / 1024  # KB\n            print(f\"[OK] Arquivo '{target_file}' encontrado ({file_size:.2f} KB)\")\n\n            # Remover ZIP (opcional)\n            if os.path.exists(zip_filename):\n                os.remove(zip_filename)\n                print(f\"[INFO] Arquivo ZIP removido\")\n\n            print(\"\\n\" + \"=\"*80)\n            print(\"DOWNLOAD CONCLU\u00cdDO COM SUCESSO!\")\n            print(\"=\"*80)\n            print(f\"\\nPr\u00f3ximo passo:\")\n            print(f\"  Execute o notebook: jupyter notebook regression.ipynb\")\n            print(\"=\"*80)\n            return True\n        else:\n            print(f\"[ERRO] Arquivo '{target_file}' n\u00e3o encontrado ap\u00f3s extra\u00e7\u00e3o!\")\n            print(f\"[INFO] Verifique o conte\u00fado extra\u00eddo:\")\n            for item in os.listdir(\".\"):\n                if item.endswith(\".csv\"):\n                    print(f\"  - {item}\")\n            return False\n\n    except urllib.error.URLError as e:\n        print(f\"\\n[ERRO] Falha ao baixar o arquivo!\")\n        print(f\"[INFO] Erro: {e}\")\n        print(f\"\\n[SOLU\u00c7\u00c3O] Baixe manualmente:\")\n        print(f\"  1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")\n        print(f\"  2. Baixe 'Bike-Sharing-Dataset.zip'\")\n        print(f\"  3. Extraia e copie 'hour.csv' para esta pasta\")\n        return False\n\n    except zipfile.BadZipFile:\n        print(f\"\\n[ERRO] Arquivo ZIP corrompido!\")\n        print(f\"[INFO] Tente baixar novamente ou use download manual\")\n        return False\n\n    except Exception as e:\n        print(f\"\\n[ERRO] Erro inesperado: {e}\")\n        print(f\"\\n[SOLU\u00c7\u00c3O] Tente download manual:\")\n        print(f\"  https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")\n        return False\n</pre> def download_dataset():     \"\"\"Baixa e extrai o dataset Bike Sharing do UCI Repository.\"\"\"      print(\"=\"*80)     print(\"DOWNLOAD DO DATASET BIKE SHARING\")     print(\"=\"*80)      # URL do dataset     url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"     zip_filename = \"Bike-Sharing-Dataset.zip\"     target_file = \"hour.csv\"      # Verificar se j\u00e1 existe     if os.path.exists(target_file):         print(f\"\\n[INFO] Arquivo '{target_file}' j\u00e1 existe!\")         response = input(\"Deseja baixar novamente? (s/n): \")         if response.lower() != 's':             print(\"[INFO] Download cancelado.\")             return True      try:         # Baixar arquivo         print(f\"\\n[1/3] Baixando dataset de: {url}\")         print(\"[INFO] Aguarde, isso pode levar alguns segundos...\")          urllib.request.urlretrieve(url, zip_filename)         print(f\"[OK] Arquivo baixado: {zip_filename}\")          # Extrair arquivo         print(f\"\\n[2/3] Extraindo arquivo ZIP...\")         with zipfile.ZipFile(zip_filename, 'r') as zip_ref:             zip_ref.extractall(\".\")         print(f\"[OK] Arquivos extra\u00eddos\")          # Verificar se hour.csv existe         print(f\"\\n[3/3] Verificando arquivo '{target_file}'...\")         if os.path.exists(target_file):             file_size = os.path.getsize(target_file) / 1024  # KB             print(f\"[OK] Arquivo '{target_file}' encontrado ({file_size:.2f} KB)\")              # Remover ZIP (opcional)             if os.path.exists(zip_filename):                 os.remove(zip_filename)                 print(f\"[INFO] Arquivo ZIP removido\")              print(\"\\n\" + \"=\"*80)             print(\"DOWNLOAD CONCLU\u00cdDO COM SUCESSO!\")             print(\"=\"*80)             print(f\"\\nPr\u00f3ximo passo:\")             print(f\"  Execute o notebook: jupyter notebook regression.ipynb\")             print(\"=\"*80)             return True         else:             print(f\"[ERRO] Arquivo '{target_file}' n\u00e3o encontrado ap\u00f3s extra\u00e7\u00e3o!\")             print(f\"[INFO] Verifique o conte\u00fado extra\u00eddo:\")             for item in os.listdir(\".\"):                 if item.endswith(\".csv\"):                     print(f\"  - {item}\")             return False      except urllib.error.URLError as e:         print(f\"\\n[ERRO] Falha ao baixar o arquivo!\")         print(f\"[INFO] Erro: {e}\")         print(f\"\\n[SOLU\u00c7\u00c3O] Baixe manualmente:\")         print(f\"  1. Acesse: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")         print(f\"  2. Baixe 'Bike-Sharing-Dataset.zip'\")         print(f\"  3. Extraia e copie 'hour.csv' para esta pasta\")         return False      except zipfile.BadZipFile:         print(f\"\\n[ERRO] Arquivo ZIP corrompido!\")         print(f\"[INFO] Tente baixar novamente ou use download manual\")         return False      except Exception as e:         print(f\"\\n[ERRO] Erro inesperado: {e}\")         print(f\"\\n[SOLU\u00c7\u00c3O] Tente download manual:\")         print(f\"  https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\")         return False In\u00a0[\u00a0]: Copied! <pre>def show_dataset_info():\n    \"\"\"Mostra informa\u00e7\u00f5es sobre o dataset.\"\"\"\n\n    target_file = \"hour.csv\"\n\n    if not os.path.exists(target_file):\n        print(f\"\\n[ERRO] Arquivo '{target_file}' n\u00e3o encontrado!\")\n        print(f\"[INFO] Execute este script primeiro para baixar o dataset\")\n        return\n\n    try:\n        import pandas as pd\n\n        print(\"\\n\" + \"=\"*80)\n        print(\"INFORMA\u00c7\u00d5ES DO DATASET\")\n        print(\"=\"*80)\n\n        df = pd.read_csv(target_file)\n\n        print(f\"\\nDimens\u00f5es: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n        print(f\"\\nColunas:\")\n        for i, col in enumerate(df.columns, 1):\n            print(f\"  {i:2d}. {col}\")\n\n        print(f\"\\nPrimeiras 5 linhas:\")\n        print(df.head())\n\n        print(f\"\\nEstat\u00edsticas da vari\u00e1vel target 'cnt':\")\n        print(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\")\n        print(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\")\n        print(f\"  M\u00ednimo: {df['cnt'].min():.0f}\")\n        print(f\"  M\u00e1ximo: {df['cnt'].max():.0f}\")\n\n        print(\"\\n\" + \"=\"*80)\n\n    except ImportError:\n        print(\"\\n[INFO] Instale pandas para ver informa\u00e7\u00f5es detalhadas:\")\n        print(\"  pip install pandas\")\n    except Exception as e:\n        print(f\"\\n[ERRO] Erro ao ler dataset: {e}\")\n</pre> def show_dataset_info():     \"\"\"Mostra informa\u00e7\u00f5es sobre o dataset.\"\"\"      target_file = \"hour.csv\"      if not os.path.exists(target_file):         print(f\"\\n[ERRO] Arquivo '{target_file}' n\u00e3o encontrado!\")         print(f\"[INFO] Execute este script primeiro para baixar o dataset\")         return      try:         import pandas as pd          print(\"\\n\" + \"=\"*80)         print(\"INFORMA\u00c7\u00d5ES DO DATASET\")         print(\"=\"*80)          df = pd.read_csv(target_file)          print(f\"\\nDimens\u00f5es: {df.shape[0]} linhas x {df.shape[1]} colunas\")         print(f\"\\nColunas:\")         for i, col in enumerate(df.columns, 1):             print(f\"  {i:2d}. {col}\")          print(f\"\\nPrimeiras 5 linhas:\")         print(df.head())          print(f\"\\nEstat\u00edsticas da vari\u00e1vel target 'cnt':\")         print(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\")         print(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\")         print(f\"  M\u00ednimo: {df['cnt'].min():.0f}\")         print(f\"  M\u00e1ximo: {df['cnt'].max():.0f}\")          print(\"\\n\" + \"=\"*80)      except ImportError:         print(\"\\n[INFO] Instale pandas para ver informa\u00e7\u00f5es detalhadas:\")         print(\"  pip install pandas\")     except Exception as e:         print(f\"\\n[ERRO] Erro ao ler dataset: {e}\") In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    print(\"\\nBike Sharing Dataset - Download Autom\u00e1tico\")\n    print(\"UCI Machine Learning Repository\\n\")\n\n    # Baixar dataset\n    success = download_dataset()\n\n    # Mostrar informa\u00e7\u00f5es se sucesso\n    if success:\n        show_dataset_info()\n\n    print(\"\\n\")\n</pre> if __name__ == \"__main__\":     print(\"\\nBike Sharing Dataset - Download Autom\u00e1tico\")     print(\"UCI Machine Learning Repository\\n\")      # Baixar dataset     success = download_dataset()      # Mostrar informa\u00e7\u00f5es se sucesso     if success:         show_dataset_info()      print(\"\\n\")"},{"location":"portfolio/neural-networks/projects/2/fix_mlp_class/","title":"Fix mlp class","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript para corrigir a classe MLPRegressor no notebook.\nJunta as 3 c\u00e9lulas da implementa\u00e7\u00e3o em uma \u00fanica c\u00e9lula.\n\"\"\"\n</pre> \"\"\" Script para corrigir a classe MLPRegressor no notebook. Junta as 3 c\u00e9lulas da implementa\u00e7\u00e3o em uma \u00fanica c\u00e9lula. \"\"\" In\u00a0[\u00a0]: Copied! <pre>import json\n</pre> import json In\u00a0[\u00a0]: Copied! <pre>notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\"\n</pre> notebook_path = r\"c:\\Users\\pedro\\OneDrive\\Documents\\Insper\\9_semestre\\redes_neurais_e_deep_learning\\pedrocivita.github.io\\site\\projects\\2\\main\\regression.ipynb\" In\u00a0[\u00a0]: Copied! <pre>with open(notebook_path, 'r', encoding='utf-8') as f:\n    notebook = json.load(f)\n</pre> with open(notebook_path, 'r', encoding='utf-8') as f:     notebook = json.load(f) In\u00a0[\u00a0]: Copied! <pre># C\u00f3digo completo da classe MLP em uma \u00fanica string\nmlp_complete_code = \"\"\"class MLPRegressor:\n    \\\"\\\"\\\"\n    Multi-Layer Perceptron para Regress\u00e3o implementado do zero.\n\n    Arquitetura: Input \u2192 Hidden1 (ReLU) \u2192 Hidden2 (ReLU) \u2192 Hidden3 (ReLU) \u2192 Output (Linear)\n    \\\"\\\"\\\"\n\n    def __init__(self, input_size, hidden_sizes, output_size=1, learning_rate=0.001,\n                 reg_lambda=0.001, random_seed=42):\n        \\\"\\\"\\\"\n        Inicializa o MLP.\n\n        Par\u00e2metros:\n        -----------\n        input_size : int\n            N\u00famero de features de entrada\n        hidden_sizes : list\n            Lista com n\u00famero de neur\u00f4nios em cada camada oculta\n        output_size : int\n            N\u00famero de neur\u00f4nios de sa\u00edda (1 para regress\u00e3o)\n        learning_rate : float\n            Taxa de aprendizado inicial\n        reg_lambda : float\n            Par\u00e2metro de regulariza\u00e7\u00e3o L2\n        random_seed : int\n            Seed para reprodutibilidade\n        \\\"\\\"\\\"\n        np.random.seed(random_seed)\n\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.output_size = output_size\n        self.learning_rate = learning_rate\n        self.reg_lambda = reg_lambda\n\n        # Inicializar pesos e biases\n        self.weights = []\n        self.biases = []\n\n        # Criar lista de tamanhos de camadas\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n\n        # He Initialization para cada camada\n        for i in range(len(layer_sizes) - 1):\n            n_in = layer_sizes[i]\n            n_out = layer_sizes[i + 1]\n\n            # He initialization: std = sqrt(2 / n_in)\n            W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)\n            b = np.zeros((1, n_out))\n\n            self.weights.append(W)\n            self.biases.append(b)\n\n        # Hist\u00f3rico de treinamento\n        self.train_loss_history = []\n        self.val_loss_history = []\n\n    def relu(self, Z):\n        \\\"\\\"\\\"ReLU activation: f(x) = max(0, x)\\\"\\\"\\\"\n        return np.maximum(0, Z)\n\n    def relu_derivative(self, Z):\n        \\\"\\\"\\\"Derivada da ReLU: f'(x) = 1 se x &gt; 0, caso contr\u00e1rio 0\\\"\\\"\\\"\n        return (Z &gt; 0).astype(float)\n\n    def forward(self, X):\n        \\\"\\\"\\\"\n        Forward propagation.\n\n        Retorna:\n        --------\n        output : array\n            Predi\u00e7\u00f5es do modelo\n        cache : dict\n            Valores intermedi\u00e1rios (para backpropagation)\n        \\\"\\\"\\\"\n        cache = {'A': [X]}  # Armazenar ativa\u00e7\u00f5es\n        cache['Z'] = []     # Armazenar outputs pr\u00e9-ativa\u00e7\u00e3o\n\n        A = X\n\n        # Camadas ocultas (com ReLU)\n        for i in range(len(self.weights) - 1):\n            Z = np.dot(A, self.weights[i]) + self.biases[i]\n            A = self.relu(Z)\n\n            cache['Z'].append(Z)\n            cache['A'].append(A)\n\n        # Camada de sa\u00edda (linear, sem ativa\u00e7\u00e3o)\n        Z_out = np.dot(A, self.weights[-1]) + self.biases[-1]\n        output = Z_out  # Ativa\u00e7\u00e3o linear para regress\u00e3o\n\n        cache['Z'].append(Z_out)\n        cache['A'].append(output)\n\n        return output, cache\n\n    def compute_loss(self, y_true, y_pred):\n        \\\"\\\"\\\"\n        Calcula MSE loss com regulariza\u00e7\u00e3o L2.\n\n        Loss = MSE + L2_penalty\n        MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2\n        L2 = (\u03bb/2n) * \u03a3(W\u00b2)\n        \\\"\\\"\\\"\n        n_samples = y_true.shape[0]\n\n        # MSE\n        mse = np.mean((y_pred - y_true) ** 2)\n\n        # L2 regularization\n        l2_penalty = 0\n        for W in self.weights:\n            l2_penalty += np.sum(W ** 2)\n        l2_penalty *= (self.reg_lambda / (2 * n_samples))\n\n        total_loss = mse + l2_penalty\n\n        return total_loss\n\n    def backward(self, X, y_true, cache):\n        \\\"\\\"\\\"\n        Backpropagation para calcular gradientes.\n\n        Retorna:\n        --------\n        grads : dict\n            Gradientes dos pesos e biases\n        \\\"\\\"\\\"\n        n_samples = X.shape[0]\n        grads = {'W': [], 'b': []}\n\n        # Gradiente da loss em rela\u00e7\u00e3o \u00e0 sa\u00edda\n        y_pred = cache['A'][-1]\n        dA = (2.0 / n_samples) * (y_pred - y_true)\n\n        # Backprop atrav\u00e9s das camadas (de tr\u00e1s para frente)\n        for i in reversed(range(len(self.weights))):\n            A_prev = cache['A'][i]\n\n            # Gradientes de W e b\n            dW = np.dot(A_prev.T, dA)\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            # Adicionar regulariza\u00e7\u00e3o L2 ao gradiente de W\n            dW += (self.reg_lambda / n_samples) * self.weights[i]\n\n            # Inserir no in\u00edcio da lista (pois estamos indo de tr\u00e1s para frente)\n            grads['W'].insert(0, dW)\n            grads['b'].insert(0, db)\n\n            # Gradiente para camada anterior (se n\u00e3o for a primeira camada)\n            if i &gt; 0:\n                dA = np.dot(dA, self.weights[i].T)\n                # Aplicar derivada da ReLU\n                dA = dA * self.relu_derivative(cache['Z'][i - 1])\n\n        return grads\n\n    def update_weights(self, grads):\n        \\\"\\\"\\\"Atualiza pesos usando gradiente descendente.\\\"\\\"\\\"\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.learning_rate * grads['W'][i]\n            self.biases[i] -= self.learning_rate * grads['b'][i]\n\n    def train_epoch(self, X_train, y_train, batch_size=64):\n        \\\"\\\"\\\"\n        Treina por uma \u00e9poca usando mini-batch gradient descent.\n\n        Retorna:\n        --------\n        epoch_loss : float\n            Loss m\u00e9dia da \u00e9poca\n        \\\"\\\"\\\"\n        n_samples = X_train.shape[0]\n        indices = np.arange(n_samples)\n        np.random.shuffle(indices)\n\n        epoch_loss = 0\n        n_batches = 0\n\n        # Mini-batch training\n        for start_idx in range(0, n_samples, batch_size):\n            end_idx = min(start_idx + batch_size, n_samples)\n            batch_indices = indices[start_idx:end_idx]\n\n            X_batch = X_train[batch_indices]\n            y_batch = y_train[batch_indices]\n\n            # Forward pass\n            y_pred, cache = self.forward(X_batch)\n\n            # Calcular loss\n            batch_loss = self.compute_loss(y_batch, y_pred)\n            epoch_loss += batch_loss\n            n_batches += 1\n\n            # Backward pass\n            grads = self.backward(X_batch, y_batch, cache)\n\n            # Update weights\n            self.update_weights(grads)\n\n        return epoch_loss / n_batches\n\n    def predict(self, X):\n        \\\"\\\"\\\"Faz predi\u00e7\u00f5es para novos dados.\\\"\\\"\\\"\n        y_pred, _ = self.forward(X)\n        return y_pred\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64,\n            early_stopping_patience=15, verbose=True):\n        \\\"\\\"\\\"\n        Treina o modelo.\n\n        Par\u00e2metros:\n        -----------\n        X_train, y_train : arrays\n            Dados de treinamento\n        X_val, y_val : arrays\n            Dados de valida\u00e7\u00e3o\n        epochs : int\n            N\u00famero m\u00e1ximo de \u00e9pocas\n        batch_size : int\n            Tamanho do mini-batch\n        early_stopping_patience : int\n            N\u00famero de \u00e9pocas sem melhora antes de parar\n        verbose : bool\n            Imprimir progresso\n        \\\"\\\"\\\"\n        best_val_loss = float('inf')\n        patience_counter = 0\n\n        for epoch in range(epochs):\n            # Treinar uma \u00e9poca\n            train_loss = self.train_epoch(X_train, y_train, batch_size)\n\n            # Calcular loss de valida\u00e7\u00e3o\n            y_val_pred, _ = self.forward(X_val)\n            val_loss = self.compute_loss(y_val, y_val_pred)\n\n            # Salvar hist\u00f3rico\n            self.train_loss_history.append(train_loss)\n            self.val_loss_history.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                # Salvar melhores pesos\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_counter += 1\n\n            # Imprimir progresso\n            if verbose and (epoch + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n                      f\"Train Loss: {train_loss:.4f} | \"\n                      f\"Val Loss: {val_loss:.4f} | \"\n                      f\"Best Val: {best_val_loss:.4f}\")\n\n            # Parar se n\u00e3o houver melhora\n            if patience_counter &gt;= early_stopping_patience:\n                if verbose:\n                    print(f\"\\\\nEarly stopping na \u00e9poca {epoch+1}\")\n                    print(f\"Melhor val loss: {best_val_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        if hasattr(self, 'best_weights'):\n            self.weights = self.best_weights\n            self.biases = self.best_biases\n            if verbose:\n                print(\"Melhores pesos restaurados\")\n\nprint(\"Classe MLPRegressor implementada com sucesso!\")\nprint(\"\\\\nFuncionalidades implementadas:\")\nprint(\"  - Forward propagation\")\nprint(\"  - Backpropagation\")\nprint(\"  - Mini-batch gradient descent\")\nprint(\"  - He initialization\")\nprint(\"  - ReLU activation\")\nprint(\"  - MSE loss\")\nprint(\"  - L2 regularization\")\nprint(\"  - Early stopping\")\"\"\"\n</pre> # C\u00f3digo completo da classe MLP em uma \u00fanica string mlp_complete_code = \"\"\"class MLPRegressor:     \\\"\\\"\\\"     Multi-Layer Perceptron para Regress\u00e3o implementado do zero.      Arquitetura: Input \u2192 Hidden1 (ReLU) \u2192 Hidden2 (ReLU) \u2192 Hidden3 (ReLU) \u2192 Output (Linear)     \\\"\\\"\\\"      def __init__(self, input_size, hidden_sizes, output_size=1, learning_rate=0.001,                  reg_lambda=0.001, random_seed=42):         \\\"\\\"\\\"         Inicializa o MLP.          Par\u00e2metros:         -----------         input_size : int             N\u00famero de features de entrada         hidden_sizes : list             Lista com n\u00famero de neur\u00f4nios em cada camada oculta         output_size : int             N\u00famero de neur\u00f4nios de sa\u00edda (1 para regress\u00e3o)         learning_rate : float             Taxa de aprendizado inicial         reg_lambda : float             Par\u00e2metro de regulariza\u00e7\u00e3o L2         random_seed : int             Seed para reprodutibilidade         \\\"\\\"\\\"         np.random.seed(random_seed)          self.input_size = input_size         self.hidden_sizes = hidden_sizes         self.output_size = output_size         self.learning_rate = learning_rate         self.reg_lambda = reg_lambda          # Inicializar pesos e biases         self.weights = []         self.biases = []          # Criar lista de tamanhos de camadas         layer_sizes = [input_size] + hidden_sizes + [output_size]          # He Initialization para cada camada         for i in range(len(layer_sizes) - 1):             n_in = layer_sizes[i]             n_out = layer_sizes[i + 1]              # He initialization: std = sqrt(2 / n_in)             W = np.random.randn(n_in, n_out) * np.sqrt(2.0 / n_in)             b = np.zeros((1, n_out))              self.weights.append(W)             self.biases.append(b)          # Hist\u00f3rico de treinamento         self.train_loss_history = []         self.val_loss_history = []      def relu(self, Z):         \\\"\\\"\\\"ReLU activation: f(x) = max(0, x)\\\"\\\"\\\"         return np.maximum(0, Z)      def relu_derivative(self, Z):         \\\"\\\"\\\"Derivada da ReLU: f'(x) = 1 se x &gt; 0, caso contr\u00e1rio 0\\\"\\\"\\\"         return (Z &gt; 0).astype(float)      def forward(self, X):         \\\"\\\"\\\"         Forward propagation.          Retorna:         --------         output : array             Predi\u00e7\u00f5es do modelo         cache : dict             Valores intermedi\u00e1rios (para backpropagation)         \\\"\\\"\\\"         cache = {'A': [X]}  # Armazenar ativa\u00e7\u00f5es         cache['Z'] = []     # Armazenar outputs pr\u00e9-ativa\u00e7\u00e3o          A = X          # Camadas ocultas (com ReLU)         for i in range(len(self.weights) - 1):             Z = np.dot(A, self.weights[i]) + self.biases[i]             A = self.relu(Z)              cache['Z'].append(Z)             cache['A'].append(A)          # Camada de sa\u00edda (linear, sem ativa\u00e7\u00e3o)         Z_out = np.dot(A, self.weights[-1]) + self.biases[-1]         output = Z_out  # Ativa\u00e7\u00e3o linear para regress\u00e3o          cache['Z'].append(Z_out)         cache['A'].append(output)          return output, cache      def compute_loss(self, y_true, y_pred):         \\\"\\\"\\\"         Calcula MSE loss com regulariza\u00e7\u00e3o L2.          Loss = MSE + L2_penalty         MSE = (1/n) * \u03a3(y_pred - y_true)\u00b2         L2 = (\u03bb/2n) * \u03a3(W\u00b2)         \\\"\\\"\\\"         n_samples = y_true.shape[0]          # MSE         mse = np.mean((y_pred - y_true) ** 2)          # L2 regularization         l2_penalty = 0         for W in self.weights:             l2_penalty += np.sum(W ** 2)         l2_penalty *= (self.reg_lambda / (2 * n_samples))          total_loss = mse + l2_penalty          return total_loss      def backward(self, X, y_true, cache):         \\\"\\\"\\\"         Backpropagation para calcular gradientes.          Retorna:         --------         grads : dict             Gradientes dos pesos e biases         \\\"\\\"\\\"         n_samples = X.shape[0]         grads = {'W': [], 'b': []}          # Gradiente da loss em rela\u00e7\u00e3o \u00e0 sa\u00edda         y_pred = cache['A'][-1]         dA = (2.0 / n_samples) * (y_pred - y_true)          # Backprop atrav\u00e9s das camadas (de tr\u00e1s para frente)         for i in reversed(range(len(self.weights))):             A_prev = cache['A'][i]              # Gradientes de W e b             dW = np.dot(A_prev.T, dA)             db = np.sum(dA, axis=0, keepdims=True)              # Adicionar regulariza\u00e7\u00e3o L2 ao gradiente de W             dW += (self.reg_lambda / n_samples) * self.weights[i]              # Inserir no in\u00edcio da lista (pois estamos indo de tr\u00e1s para frente)             grads['W'].insert(0, dW)             grads['b'].insert(0, db)              # Gradiente para camada anterior (se n\u00e3o for a primeira camada)             if i &gt; 0:                 dA = np.dot(dA, self.weights[i].T)                 # Aplicar derivada da ReLU                 dA = dA * self.relu_derivative(cache['Z'][i - 1])          return grads      def update_weights(self, grads):         \\\"\\\"\\\"Atualiza pesos usando gradiente descendente.\\\"\\\"\\\"         for i in range(len(self.weights)):             self.weights[i] -= self.learning_rate * grads['W'][i]             self.biases[i] -= self.learning_rate * grads['b'][i]      def train_epoch(self, X_train, y_train, batch_size=64):         \\\"\\\"\\\"         Treina por uma \u00e9poca usando mini-batch gradient descent.          Retorna:         --------         epoch_loss : float             Loss m\u00e9dia da \u00e9poca         \\\"\\\"\\\"         n_samples = X_train.shape[0]         indices = np.arange(n_samples)         np.random.shuffle(indices)          epoch_loss = 0         n_batches = 0          # Mini-batch training         for start_idx in range(0, n_samples, batch_size):             end_idx = min(start_idx + batch_size, n_samples)             batch_indices = indices[start_idx:end_idx]              X_batch = X_train[batch_indices]             y_batch = y_train[batch_indices]              # Forward pass             y_pred, cache = self.forward(X_batch)              # Calcular loss             batch_loss = self.compute_loss(y_batch, y_pred)             epoch_loss += batch_loss             n_batches += 1              # Backward pass             grads = self.backward(X_batch, y_batch, cache)              # Update weights             self.update_weights(grads)          return epoch_loss / n_batches      def predict(self, X):         \\\"\\\"\\\"Faz predi\u00e7\u00f5es para novos dados.\\\"\\\"\\\"         y_pred, _ = self.forward(X)         return y_pred      def fit(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=64,             early_stopping_patience=15, verbose=True):         \\\"\\\"\\\"         Treina o modelo.          Par\u00e2metros:         -----------         X_train, y_train : arrays             Dados de treinamento         X_val, y_val : arrays             Dados de valida\u00e7\u00e3o         epochs : int             N\u00famero m\u00e1ximo de \u00e9pocas         batch_size : int             Tamanho do mini-batch         early_stopping_patience : int             N\u00famero de \u00e9pocas sem melhora antes de parar         verbose : bool             Imprimir progresso         \\\"\\\"\\\"         best_val_loss = float('inf')         patience_counter = 0          for epoch in range(epochs):             # Treinar uma \u00e9poca             train_loss = self.train_epoch(X_train, y_train, batch_size)              # Calcular loss de valida\u00e7\u00e3o             y_val_pred, _ = self.forward(X_val)             val_loss = self.compute_loss(y_val, y_val_pred)              # Salvar hist\u00f3rico             self.train_loss_history.append(train_loss)             self.val_loss_history.append(val_loss)              # Early stopping             if val_loss &lt; best_val_loss:                 best_val_loss = val_loss                 patience_counter = 0                 # Salvar melhores pesos                 self.best_weights = [W.copy() for W in self.weights]                 self.best_biases = [b.copy() for b in self.biases]             else:                 patience_counter += 1              # Imprimir progresso             if verbose and (epoch + 1) % 10 == 0:                 print(f\"Epoch {epoch+1:3d}/{epochs} | \"                       f\"Train Loss: {train_loss:.4f} | \"                       f\"Val Loss: {val_loss:.4f} | \"                       f\"Best Val: {best_val_loss:.4f}\")              # Parar se n\u00e3o houver melhora             if patience_counter &gt;= early_stopping_patience:                 if verbose:                     print(f\"\\\\nEarly stopping na \u00e9poca {epoch+1}\")                     print(f\"Melhor val loss: {best_val_loss:.4f}\")                 break          # Restaurar melhores pesos         if hasattr(self, 'best_weights'):             self.weights = self.best_weights             self.biases = self.best_biases             if verbose:                 print(\"Melhores pesos restaurados\")  print(\"Classe MLPRegressor implementada com sucesso!\") print(\"\\\\nFuncionalidades implementadas:\") print(\"  - Forward propagation\") print(\"  - Backpropagation\") print(\"  - Mini-batch gradient descent\") print(\"  - He initialization\") print(\"  - ReLU activation\") print(\"  - MSE loss\") print(\"  - L2 regularization\") print(\"  - Early stopping\")\"\"\" <p>Encontrar as c\u00e9lulas da implementa\u00e7\u00e3o MLP (c\u00e9lulas 17, 18, 19) e substituir por uma \u00fanica c\u00e9lula com c\u00f3digo completo</p> In\u00a0[\u00a0]: Copied! <pre># Buscar c\u00e9lulas com \"class MLPRegressor\"\nmlp_cells_indices = []\nfor i, cell in enumerate(notebook['cells']):\n    if cell['cell_type'] == 'code':\n        source = ''.join(cell['source'])\n        if 'class MLPRegressor:' in source or 'def compute_loss' in source or 'def train_epoch' in source:\n            mlp_cells_indices.append(i)\n</pre> # Buscar c\u00e9lulas com \"class MLPRegressor\" mlp_cells_indices = [] for i, cell in enumerate(notebook['cells']):     if cell['cell_type'] == 'code':         source = ''.join(cell['source'])         if 'class MLPRegressor:' in source or 'def compute_loss' in source or 'def train_epoch' in source:             mlp_cells_indices.append(i) In\u00a0[\u00a0]: Copied! <pre>print(f\"C\u00e9lulas MLP encontradas nos \u00edndices: {mlp_cells_indices}\")\n</pre> print(f\"C\u00e9lulas MLP encontradas nos \u00edndices: {mlp_cells_indices}\") In\u00a0[\u00a0]: Copied! <pre>if len(mlp_cells_indices) &gt;= 3:\n    # Substituir a primeira c\u00e9lula com c\u00f3digo completo\n    notebook['cells'][mlp_cells_indices[0]] = {\n        \"cell_type\": \"code\",\n        \"execution_count\": None,\n        \"metadata\": {},\n        \"outputs\": [],\n        \"source\": mlp_complete_code.split(\"\\n\")\n    }\n\n    # Remover as outras duas c\u00e9lulas\n    for idx in sorted(mlp_cells_indices[1:], reverse=True):\n        del notebook['cells'][idx]\n\n    print(f\"C\u00e9lulas MLP consolidadas!\")\n    print(f\"Total de c\u00e9lulas agora: {len(notebook['cells'])}\")\n\n    # Salvar\n    with open(notebook_path, 'w', encoding='utf-8') as f:\n        json.dump(notebook, f, ensure_ascii=False, indent=1)\n\n    print(\"Notebook corrigido e salvo!\")\n    print(\"\\nAGORA:\")\n    print(\"1. No Jupyter, clique em 'Kernel' \u2192 'Restart &amp; Clear Output'\")\n    print(\"2. Execute todas as c\u00e9lulas novamente desde o in\u00edcio\")\n    print(\"3. A classe MLPRegressor agora est\u00e1 completa em uma \u00fanica c\u00e9lula\")\nelse:\n    print(\"Erro: n\u00e3o foram encontradas 3 c\u00e9lulas MLP\")\n    print(\"C\u00e9lulas encontradas:\", len(mlp_cells_indices))\n</pre> if len(mlp_cells_indices) &gt;= 3:     # Substituir a primeira c\u00e9lula com c\u00f3digo completo     notebook['cells'][mlp_cells_indices[0]] = {         \"cell_type\": \"code\",         \"execution_count\": None,         \"metadata\": {},         \"outputs\": [],         \"source\": mlp_complete_code.split(\"\\n\")     }      # Remover as outras duas c\u00e9lulas     for idx in sorted(mlp_cells_indices[1:], reverse=True):         del notebook['cells'][idx]      print(f\"C\u00e9lulas MLP consolidadas!\")     print(f\"Total de c\u00e9lulas agora: {len(notebook['cells'])}\")      # Salvar     with open(notebook_path, 'w', encoding='utf-8') as f:         json.dump(notebook, f, ensure_ascii=False, indent=1)      print(\"Notebook corrigido e salvo!\")     print(\"\\nAGORA:\")     print(\"1. No Jupyter, clique em 'Kernel' \u2192 'Restart &amp; Clear Output'\")     print(\"2. Execute todas as c\u00e9lulas novamente desde o in\u00edcio\")     print(\"3. A classe MLPRegressor agora est\u00e1 completa em uma \u00fanica c\u00e9lula\") else:     print(\"Erro: n\u00e3o foram encontradas 3 c\u00e9lulas MLP\")     print(\"C\u00e9lulas encontradas:\", len(mlp_cells_indices))"},{"location":"portfolio/neural-networks/projects/2/main/","title":"Project 2","text":""},{"location":"portfolio/neural-networks/projects/2/main/#project-2","title":"Project 2","text":"<p>Description coming soon.</p>"},{"location":"portfolio/neural-networks/projects/2/regression/","title":"Regression","text":"In\u00a0[1]: Copied! <pre># Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configura\u00e7\u00e3o\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\nprint(\"Setup completo!\")\n</pre> # Imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from datetime import datetime import warnings warnings.filterwarnings('ignore')  # Configura\u00e7\u00e3o np.random.seed(42) plt.style.use('seaborn-v0_8-darkgrid') sns.set_palette(\"husl\")  print(\"Setup completo!\") <pre>Setup completo!\n</pre> In\u00a0[2]: Copied! <pre># Carregar dataset\ndf = pd.read_csv('hour.csv')\n\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nPrimeiras linhas:\")\ndf.head()\n</pre> # Carregar dataset df = pd.read_csv('hour.csv')  print(f\"Shape: {df.shape}\") print(f\"\\nPrimeiras linhas:\") df.head() <pre>Shape: (17379, 17)\n\nPrimeiras linhas:\n</pre> Out[2]: instant dteday season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt 0 1 2011-01-01 1 0 1 0 0 6 0 1 0.24 0.2879 0.81 0.0 3 13 16 1 2 2011-01-01 1 0 1 1 0 6 0 1 0.22 0.2727 0.80 0.0 8 32 40 2 3 2011-01-01 1 0 1 2 0 6 0 1 0.22 0.2727 0.80 0.0 5 27 32 3 4 2011-01-01 1 0 1 3 0 6 0 1 0.24 0.2879 0.75 0.0 3 10 13 4 5 2011-01-01 1 0 1 4 0 6 0 1 0.24 0.2879 0.75 0.0 0 1 1 In\u00a0[3]: Copied! <pre># Informa\u00e7\u00f5es do dataset\nprint(\"Informa\u00e7\u00f5es Gerais:\")\nprint(\"=\"*70)\ndf.info()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"Estat\u00edsticas Descritivas:\")\nprint(\"=\"*70)\ndf.describe()\n</pre> # Informa\u00e7\u00f5es do dataset print(\"Informa\u00e7\u00f5es Gerais:\") print(\"=\"*70) df.info()  print(\"\\n\" + \"=\"*70) print(\"Estat\u00edsticas Descritivas:\") print(\"=\"*70) df.describe() <pre>Informa\u00e7\u00f5es Gerais:\n======================================================================\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 17379 entries, 0 to 17378\nData columns (total 17 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   instant     17379 non-null  int64  \n 1   dteday      17379 non-null  object \n 2   season      17379 non-null  int64  \n 3   yr          17379 non-null  int64  \n 4   mnth        17379 non-null  int64  \n 5   hr          17379 non-null  int64  \n 6   holiday     17379 non-null  int64  \n 7   weekday     17379 non-null  int64  \n 8   workingday  17379 non-null  int64  \n 9   weathersit  17379 non-null  int64  \n 10  temp        17379 non-null  float64\n 11  atemp       17379 non-null  float64\n 12  hum         17379 non-null  float64\n 13  windspeed   17379 non-null  float64\n 14  casual      17379 non-null  int64  \n 15  registered  17379 non-null  int64  \n 16  cnt         17379 non-null  int64  \ndtypes: float64(4), int64(12), object(1)\nmemory usage: 2.3+ MB\n\n======================================================================\nEstat\u00edsticas Descritivas:\n======================================================================\n</pre> Out[3]: instant season yr mnth hr holiday weekday workingday weathersit temp atemp hum windspeed casual registered cnt count 17379.0000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 17379.000000 mean 8690.0000 2.501640 0.502561 6.537775 11.546752 0.028770 3.003683 0.682721 1.425283 0.496987 0.475775 0.627229 0.190098 35.676218 153.786869 189.463088 std 5017.0295 1.106918 0.500008 3.438776 6.914405 0.167165 2.005771 0.465431 0.639357 0.192556 0.171850 0.192930 0.122340 49.305030 151.357286 181.387599 min 1.0000 1.000000 0.000000 1.000000 0.000000 0.000000 0.000000 0.000000 1.000000 0.020000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 25% 4345.5000 2.000000 0.000000 4.000000 6.000000 0.000000 1.000000 0.000000 1.000000 0.340000 0.333300 0.480000 0.104500 4.000000 34.000000 40.000000 50% 8690.0000 3.000000 1.000000 7.000000 12.000000 0.000000 3.000000 1.000000 1.000000 0.500000 0.484800 0.630000 0.194000 17.000000 115.000000 142.000000 75% 13034.5000 3.000000 1.000000 10.000000 18.000000 0.000000 5.000000 1.000000 2.000000 0.660000 0.621200 0.780000 0.253700 48.000000 220.000000 281.000000 max 17379.0000 4.000000 1.000000 12.000000 23.000000 1.000000 6.000000 1.000000 4.000000 1.000000 1.000000 1.000000 0.850700 367.000000 886.000000 977.000000 In\u00a0[4]: Copied! <pre># Verificar valores ausentes\nprint(\"Valores Ausentes:\")\nprint(df.isnull().sum())\n\n# An\u00e1lise da vari\u00e1vel target\nprint(f\"\\nTarget (cnt) - Estat\u00edsticas:\")\nprint(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\")\nprint(f\"  Mediana: {df['cnt'].median():.2f}\")\nprint(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\")\nprint(f\"  Min: {df['cnt'].min()}\")\nprint(f\"  Max: {df['cnt'].max()}\")\n</pre> # Verificar valores ausentes print(\"Valores Ausentes:\") print(df.isnull().sum())  # An\u00e1lise da vari\u00e1vel target print(f\"\\nTarget (cnt) - Estat\u00edsticas:\") print(f\"  M\u00e9dia: {df['cnt'].mean():.2f}\") print(f\"  Mediana: {df['cnt'].median():.2f}\") print(f\"  Desvio padr\u00e3o: {df['cnt'].std():.2f}\") print(f\"  Min: {df['cnt'].min()}\") print(f\"  Max: {df['cnt'].max()}\") <pre>Valores Ausentes:\ninstant       0\ndteday        0\nseason        0\nyr            0\nmnth          0\nhr            0\nholiday       0\nweekday       0\nworkingday    0\nweathersit    0\ntemp          0\natemp         0\nhum           0\nwindspeed     0\ncasual        0\nregistered    0\ncnt           0\ndtype: int64\n\nTarget (cnt) - Estat\u00edsticas:\n  M\u00e9dia: 189.46\n  Mediana: 142.00\n  Desvio padr\u00e3o: 181.39\n  Min: 1\n  Max: 977\n</pre> In\u00a0[5]: Copied! <pre># Visualiza\u00e7\u00f5es explorat\u00f3rias\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Distribui\u00e7\u00e3o do target\naxes[0,0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7)\naxes[0,0].set_title('Distribui\u00e7\u00e3o da Demanda')\naxes[0,0].set_xlabel('N\u00famero de alugu\u00e9is')\naxes[0,0].set_ylabel('Frequ\u00eancia')\naxes[0,0].grid(True, alpha=0.3)\n\n# Demanda por hora\nhourly = df.groupby('hr')['cnt'].mean()\naxes[0,1].plot(hourly.index, hourly.values, marker='o', linewidth=2)\naxes[0,1].set_title('Demanda M\u00e9dia por Hora')\naxes[0,1].set_xlabel('Hora do dia')\naxes[0,1].set_ylabel('Alugu\u00e9is m\u00e9dios')\naxes[0,1].grid(True, alpha=0.3)\n\n# Temperatura vs Demanda\naxes[1,0].scatter(df['temp'], df['cnt'], alpha=0.3, s=10)\naxes[1,0].set_title('Temperatura vs Demanda')\naxes[1,0].set_xlabel('Temperatura normalizada')\naxes[1,0].set_ylabel('Alugu\u00e9is')\naxes[1,0].grid(True, alpha=0.3)\n\n# Correla\u00e7\u00e3o\ncorr_cols = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\ncorr = df[corr_cols].corr()\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n            ax=axes[1,1], square=True, linewidths=1)\naxes[1,1].set_title('Matriz de Correla\u00e7\u00e3o')\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualiza\u00e7\u00f5es explorat\u00f3rias fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # Distribui\u00e7\u00e3o do target axes[0,0].hist(df['cnt'], bins=50, edgecolor='black', alpha=0.7) axes[0,0].set_title('Distribui\u00e7\u00e3o da Demanda') axes[0,0].set_xlabel('N\u00famero de alugu\u00e9is') axes[0,0].set_ylabel('Frequ\u00eancia') axes[0,0].grid(True, alpha=0.3)  # Demanda por hora hourly = df.groupby('hr')['cnt'].mean() axes[0,1].plot(hourly.index, hourly.values, marker='o', linewidth=2) axes[0,1].set_title('Demanda M\u00e9dia por Hora') axes[0,1].set_xlabel('Hora do dia') axes[0,1].set_ylabel('Alugu\u00e9is m\u00e9dios') axes[0,1].grid(True, alpha=0.3)  # Temperatura vs Demanda axes[1,0].scatter(df['temp'], df['cnt'], alpha=0.3, s=10) axes[1,0].set_title('Temperatura vs Demanda') axes[1,0].set_xlabel('Temperatura normalizada') axes[1,0].set_ylabel('Alugu\u00e9is') axes[1,0].grid(True, alpha=0.3)  # Correla\u00e7\u00e3o corr_cols = ['temp', 'atemp', 'hum', 'windspeed', 'cnt'] corr = df[corr_cols].corr() sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0,             ax=axes[1,1], square=True, linewidths=1) axes[1,1].set_title('Matriz de Correla\u00e7\u00e3o')  plt.tight_layout() plt.show() In\u00a0[6]: Copied! <pre># Preparar dados\ndf_prep = df.copy()\n\n# Remover colunas\ndrop_cols = ['instant', 'dteday', 'casual', 'registered']\ndf_prep = df_prep.drop(columns=drop_cols)\n\nprint(f\"Colunas removidas: {drop_cols}\")\nprint(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_prep.shape}\")\n</pre> # Preparar dados df_prep = df.copy()  # Remover colunas drop_cols = ['instant', 'dteday', 'casual', 'registered'] df_prep = df_prep.drop(columns=drop_cols)  print(f\"Colunas removidas: {drop_cols}\") print(f\"Shape ap\u00f3s remo\u00e7\u00e3o: {df_prep.shape}\") <pre>Colunas removidas: ['instant', 'dteday', 'casual', 'registered']\nShape ap\u00f3s remo\u00e7\u00e3o: (17379, 13)\n</pre> In\u00a0[7]: Copied! <pre># Feature engineering: vari\u00e1veis c\u00edclicas\n# Hora, m\u00eas e dia da semana s\u00e3o c\u00edclicos (ex: hora 23 pr\u00f3xima de hora 0)\ndef encode_cyclical(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)\n    return data\n\ndf_prep = encode_cyclical(df_prep, 'hr', 24)\ndf_prep = encode_cyclical(df_prep, 'mnth', 12)\ndf_prep = encode_cyclical(df_prep, 'weekday', 7)\n\n# Remover originais\ndf_prep = df_prep.drop(columns=['hr', 'mnth', 'weekday'])\n\nprint(f\"Features c\u00edclicas criadas!\")\nprint(f\"Shape final: {df_prep.shape}\")\nprint(f\"\\nFeatures: {list(df_prep.columns)}\")\n</pre> # Feature engineering: vari\u00e1veis c\u00edclicas # Hora, m\u00eas e dia da semana s\u00e3o c\u00edclicos (ex: hora 23 pr\u00f3xima de hora 0) def encode_cyclical(data, col, max_val):     data[col + '_sin'] = np.sin(2 * np.pi * data[col] / max_val)     data[col + '_cos'] = np.cos(2 * np.pi * data[col] / max_val)     return data  df_prep = encode_cyclical(df_prep, 'hr', 24) df_prep = encode_cyclical(df_prep, 'mnth', 12) df_prep = encode_cyclical(df_prep, 'weekday', 7)  # Remover originais df_prep = df_prep.drop(columns=['hr', 'mnth', 'weekday'])  print(f\"Features c\u00edclicas criadas!\") print(f\"Shape final: {df_prep.shape}\") print(f\"\\nFeatures: {list(df_prep.columns)}\") <pre>Features c\u00edclicas criadas!\nShape final: (17379, 16)\n\nFeatures: ['season', 'yr', 'holiday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt', 'hr_sin', 'hr_cos', 'mnth_sin', 'mnth_cos', 'weekday_sin', 'weekday_cos']\n</pre> In\u00a0[8]: Copied! <pre># Separar X e y\nX = df_prep.drop(columns=['cnt']).values\ny = df_prep['cnt'].values.reshape(-1, 1)\n\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Salvar estat\u00edsticas para desnormaliza\u00e7\u00e3o posterior\nX_mean = X.mean(axis=0)\nX_std = X.std(axis=0)\ny_mean = y.mean()\ny_std = y.std()\n\n# Normalizar (z-score)\nX_norm = (X - X_mean) / (X_std + 1e-8)\ny_norm = (y - y_mean) / y_std\n\nprint(f\"\\nAp\u00f3s normaliza\u00e7\u00e3o:\")\nprint(f\"X: m\u00e9dia={X_norm.mean():.3f}, std={X_norm.std():.3f}\")\nprint(f\"y: m\u00e9dia={y_norm.mean():.3f}, std={y_norm.std():.3f}\")\n</pre> # Separar X e y X = df_prep.drop(columns=['cnt']).values y = df_prep['cnt'].values.reshape(-1, 1)  print(f\"X shape: {X.shape}\") print(f\"y shape: {y.shape}\")  # Salvar estat\u00edsticas para desnormaliza\u00e7\u00e3o posterior X_mean = X.mean(axis=0) X_std = X.std(axis=0) y_mean = y.mean() y_std = y.std()  # Normalizar (z-score) X_norm = (X - X_mean) / (X_std + 1e-8) y_norm = (y - y_mean) / y_std  print(f\"\\nAp\u00f3s normaliza\u00e7\u00e3o:\") print(f\"X: m\u00e9dia={X_norm.mean():.3f}, std={X_norm.std():.3f}\") print(f\"y: m\u00e9dia={y_norm.mean():.3f}, std={y_norm.std():.3f}\") <pre>X shape: (17379, 15)\ny shape: (17379, 1)\n\nAp\u00f3s normaliza\u00e7\u00e3o:\nX: m\u00e9dia=0.000, std=1.000\ny: m\u00e9dia=-0.000, std=1.000\n</pre> In\u00a0[9]: Copied! <pre># Dividir dados\nn_samples = X_norm.shape[0]\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\n\n# Calcular tamanhos\ntrain_size = int(0.70 * n_samples)\nval_size = int(0.15 * n_samples)\ntest_size = n_samples - train_size - val_size\n\n# Dividir \u00edndices\ntrain_idx = indices[:train_size]\nval_idx = indices[train_size:train_size + val_size]\ntest_idx = indices[train_size + val_size:]\n\n# Criar conjuntos\nX_train, y_train = X_norm[train_idx], y_norm[train_idx]\nX_val, y_val = X_norm[val_idx], y_norm[val_idx]\nX_test, y_test = X_norm[test_idx], y_norm[test_idx]\n\nprint(f\"Divis\u00e3o dos dados:\")\nprint(f\"  Train: {len(X_train):,} ({len(X_train)/n_samples*100:.1f}%)\")\nprint(f\"  Val:   {len(X_val):,} ({len(X_val)/n_samples*100:.1f}%)\")\nprint(f\"  Test:  {len(X_test):,} ({len(X_test)/n_samples*100:.1f}%)\")\n</pre> # Dividir dados n_samples = X_norm.shape[0] indices = np.arange(n_samples) np.random.shuffle(indices)  # Calcular tamanhos train_size = int(0.70 * n_samples) val_size = int(0.15 * n_samples) test_size = n_samples - train_size - val_size  # Dividir \u00edndices train_idx = indices[:train_size] val_idx = indices[train_size:train_size + val_size] test_idx = indices[train_size + val_size:]  # Criar conjuntos X_train, y_train = X_norm[train_idx], y_norm[train_idx] X_val, y_val = X_norm[val_idx], y_norm[val_idx] X_test, y_test = X_norm[test_idx], y_norm[test_idx]  print(f\"Divis\u00e3o dos dados:\") print(f\"  Train: {len(X_train):,} ({len(X_train)/n_samples*100:.1f}%)\") print(f\"  Val:   {len(X_val):,} ({len(X_val)/n_samples*100:.1f}%)\") print(f\"  Test:  {len(X_test):,} ({len(X_test)/n_samples*100:.1f}%)\") <pre>Divis\u00e3o dos dados:\n  Train: 12,165 (70.0%)\n  Val:   2,606 (15.0%)\n  Test:  2,608 (15.0%)\n</pre> In\u00a0[10]: Copied! <pre>class MLP:\n    \"\"\"Multi-Layer Perceptron para Regress\u00e3o\"\"\"\n\n    def __init__(self, layers, learning_rate=0.001, reg_lambda=0.001):\n        self.lr = learning_rate\n        self.reg = reg_lambda\n        self.weights = []\n        self.biases = []\n\n        # He initialization\n        for i in range(len(layers) - 1):\n            W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n            b = np.zeros((1, layers[i+1]))\n            self.weights.append(W)\n            self.biases.append(b)\n\n        self.train_losses = []\n        self.val_losses = []\n\n    def relu(self, x):\n        return np.maximum(0, x)\n\n    def relu_derivative(self, x):\n        return (x &gt; 0).astype(float)\n\n    def forward(self, X):\n        \"\"\"Forward propagation\"\"\"\n        self.cache = {'A': [X], 'Z': []}\n        A = X\n\n        # Hidden layers\n        for i in range(len(self.weights) - 1):\n            Z = A @ self.weights[i] + self.biases[i]\n            A = self.relu(Z)\n            self.cache['Z'].append(Z)\n            self.cache['A'].append(A)\n\n        # Output layer (linear)\n        Z = A @ self.weights[-1] + self.biases[-1]\n        self.cache['Z'].append(Z)\n        self.cache['A'].append(Z)\n\n        return Z\n\n    def compute_loss(self, y_true, y_pred):\n        \"\"\"MSE + L2 regularization\"\"\"\n        n = len(y_true)\n        mse = np.mean((y_pred - y_true) ** 2)\n        l2 = sum(np.sum(W ** 2) for W in self.weights)\n        return mse + (self.reg / (2 * n)) * l2\n\n    def backward(self, y_true):\n        \"\"\"Backpropagation\"\"\"\n        n = len(y_true)\n        y_pred = self.cache['A'][-1]\n        dA = (2.0 / n) * (y_pred - y_true)\n\n        grads_W = []\n        grads_b = []\n\n        for i in reversed(range(len(self.weights))):\n            A_prev = self.cache['A'][i]\n\n            dW = A_prev.T @ dA + (self.reg / n) * self.weights[i]\n            db = np.sum(dA, axis=0, keepdims=True)\n\n            grads_W.insert(0, dW)\n            grads_b.insert(0, db)\n\n            if i &gt; 0:\n                dA = (dA @ self.weights[i].T) * self.relu_derivative(self.cache['Z'][i-1])\n\n        # Update weights\n        for i in range(len(self.weights)):\n            self.weights[i] -= self.lr * grads_W[i]\n            self.biases[i] -= self.lr * grads_b[i]\n\n    def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15, verbose=True):\n        \"\"\"Treinar modelo com early stopping\"\"\"\n        best_loss = float('inf')\n        patience_count = 0\n\n        for epoch in range(epochs):\n            # Mini-batch training\n            indices = np.arange(len(X_train))\n            np.random.shuffle(indices)\n\n            for start in range(0, len(X_train), batch_size):\n                end = min(start + batch_size, len(X_train))\n                batch_idx = indices[start:end]\n\n                X_batch = X_train[batch_idx]\n                y_batch = y_train[batch_idx]\n\n                self.forward(X_batch)\n                self.backward(y_batch)\n\n            # Calcular losses\n            train_pred = self.forward(X_train)\n            val_pred = self.forward(X_val)\n\n            train_loss = self.compute_loss(y_train, train_pred)\n            val_loss = self.compute_loss(y_val, val_pred)\n\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n\n            # Early stopping\n            if val_loss &lt; best_loss:\n                best_loss = val_loss\n                patience_count = 0\n                self.best_weights = [W.copy() for W in self.weights]\n                self.best_biases = [b.copy() for b in self.biases]\n            else:\n                patience_count += 1\n\n            if verbose and (epoch + 1) % 20 == 0:\n                print(f\"\u00c9poca {epoch+1:3d}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n\n            if patience_count &gt;= patience:\n                if verbose:\n                    print(f\"\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")\n                break\n\n        # Restaurar melhores pesos\n        self.weights = self.best_weights\n        self.biases = self.best_biases\n\n    def predict(self, X):\n        \"\"\"Fazer predi\u00e7\u00f5es\"\"\"\n        A = X\n        for i in range(len(self.weights) - 1):\n            A = self.relu(A @ self.weights[i] + self.biases[i])\n        return A @ self.weights[-1] + self.biases[-1]\n\nprint(\"MLP implementado!\")\n</pre> class MLP:     \"\"\"Multi-Layer Perceptron para Regress\u00e3o\"\"\"      def __init__(self, layers, learning_rate=0.001, reg_lambda=0.001):         self.lr = learning_rate         self.reg = reg_lambda         self.weights = []         self.biases = []          # He initialization         for i in range(len(layers) - 1):             W = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])             b = np.zeros((1, layers[i+1]))             self.weights.append(W)             self.biases.append(b)          self.train_losses = []         self.val_losses = []      def relu(self, x):         return np.maximum(0, x)      def relu_derivative(self, x):         return (x &gt; 0).astype(float)      def forward(self, X):         \"\"\"Forward propagation\"\"\"         self.cache = {'A': [X], 'Z': []}         A = X          # Hidden layers         for i in range(len(self.weights) - 1):             Z = A @ self.weights[i] + self.biases[i]             A = self.relu(Z)             self.cache['Z'].append(Z)             self.cache['A'].append(A)          # Output layer (linear)         Z = A @ self.weights[-1] + self.biases[-1]         self.cache['Z'].append(Z)         self.cache['A'].append(Z)          return Z      def compute_loss(self, y_true, y_pred):         \"\"\"MSE + L2 regularization\"\"\"         n = len(y_true)         mse = np.mean((y_pred - y_true) ** 2)         l2 = sum(np.sum(W ** 2) for W in self.weights)         return mse + (self.reg / (2 * n)) * l2      def backward(self, y_true):         \"\"\"Backpropagation\"\"\"         n = len(y_true)         y_pred = self.cache['A'][-1]         dA = (2.0 / n) * (y_pred - y_true)          grads_W = []         grads_b = []          for i in reversed(range(len(self.weights))):             A_prev = self.cache['A'][i]              dW = A_prev.T @ dA + (self.reg / n) * self.weights[i]             db = np.sum(dA, axis=0, keepdims=True)              grads_W.insert(0, dW)             grads_b.insert(0, db)              if i &gt; 0:                 dA = (dA @ self.weights[i].T) * self.relu_derivative(self.cache['Z'][i-1])          # Update weights         for i in range(len(self.weights)):             self.weights[i] -= self.lr * grads_W[i]             self.biases[i] -= self.lr * grads_b[i]      def fit(self, X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15, verbose=True):         \"\"\"Treinar modelo com early stopping\"\"\"         best_loss = float('inf')         patience_count = 0          for epoch in range(epochs):             # Mini-batch training             indices = np.arange(len(X_train))             np.random.shuffle(indices)              for start in range(0, len(X_train), batch_size):                 end = min(start + batch_size, len(X_train))                 batch_idx = indices[start:end]                  X_batch = X_train[batch_idx]                 y_batch = y_train[batch_idx]                  self.forward(X_batch)                 self.backward(y_batch)              # Calcular losses             train_pred = self.forward(X_train)             val_pred = self.forward(X_val)              train_loss = self.compute_loss(y_train, train_pred)             val_loss = self.compute_loss(y_val, val_pred)              self.train_losses.append(train_loss)             self.val_losses.append(val_loss)              # Early stopping             if val_loss &lt; best_loss:                 best_loss = val_loss                 patience_count = 0                 self.best_weights = [W.copy() for W in self.weights]                 self.best_biases = [b.copy() for b in self.biases]             else:                 patience_count += 1              if verbose and (epoch + 1) % 20 == 0:                 print(f\"\u00c9poca {epoch+1:3d}/{epochs} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")              if patience_count &gt;= patience:                 if verbose:                     print(f\"\\nEarly stopping! Melhor val loss: {best_loss:.4f}\")                 break          # Restaurar melhores pesos         self.weights = self.best_weights         self.biases = self.best_biases      def predict(self, X):         \"\"\"Fazer predi\u00e7\u00f5es\"\"\"         A = X         for i in range(len(self.weights) - 1):             A = self.relu(A @ self.weights[i] + self.biases[i])         return A @ self.weights[-1] + self.biases[-1]  print(\"MLP implementado!\") <pre>MLP implementado!\n</pre> In\u00a0[11]: Copied! <pre># Criar e treinar modelo\nmodel = MLP(\n    layers=[15, 64, 32, 16, 1],\n    learning_rate=0.001,\n    reg_lambda=0.001\n)\n\nprint(\"Iniciando treinamento...\\n\")\nmodel.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15)\nprint(\"\\nTreinamento conclu\u00eddo!\")\n</pre> # Criar e treinar modelo model = MLP(     layers=[15, 64, 32, 16, 1],     learning_rate=0.001,     reg_lambda=0.001 )  print(\"Iniciando treinamento...\\n\") model.fit(X_train, y_train, X_val, y_val, epochs=200, batch_size=64, patience=15) print(\"\\nTreinamento conclu\u00eddo!\") <pre>Iniciando treinamento...\n\n\u00c9poca  20/200 | Train: 0.3770 | Val: 0.4228\n\u00c9poca  40/200 | Train: 0.2984 | Val: 0.3333\n\u00c9poca  60/200 | Train: 0.2369 | Val: 0.2631\n\u00c9poca  80/200 | Train: 0.1905 | Val: 0.2114\n\u00c9poca 100/200 | Train: 0.1620 | Val: 0.1811\n\u00c9poca 120/200 | Train: 0.1443 | Val: 0.1618\n\u00c9poca 140/200 | Train: 0.1343 | Val: 0.1538\n\u00c9poca 160/200 | Train: 0.1233 | Val: 0.1414\n\u00c9poca 180/200 | Train: 0.1158 | Val: 0.1359\n\u00c9poca 200/200 | Train: 0.1083 | Val: 0.1279\n\nTreinamento conclu\u00eddo!\n</pre> In\u00a0[12]: Copied! <pre># Plot curvas de loss\nplt.figure(figsize=(10, 5))\nepochs = range(1, len(model.train_losses) + 1)\n\nplt.plot(epochs, model.train_losses, label='Train Loss', linewidth=2)\nplt.plot(epochs, model.val_losses, label='Validation Loss', linewidth=2)\nplt.xlabel('\u00c9poca')\nplt.ylabel('Loss (MSE + L2)')\nplt.title('Converg\u00eancia do Modelo')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\u00c9pocas treinadas: {len(model.train_losses)}\")\nprint(f\"Loss final - Train: {model.train_losses[-1]:.4f} | Val: {model.val_losses[-1]:.4f}\")\n</pre> # Plot curvas de loss plt.figure(figsize=(10, 5)) epochs = range(1, len(model.train_losses) + 1)  plt.plot(epochs, model.train_losses, label='Train Loss', linewidth=2) plt.plot(epochs, model.val_losses, label='Validation Loss', linewidth=2) plt.xlabel('\u00c9poca') plt.ylabel('Loss (MSE + L2)') plt.title('Converg\u00eancia do Modelo') plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show()  print(f\"\u00c9pocas treinadas: {len(model.train_losses)}\") print(f\"Loss final - Train: {model.train_losses[-1]:.4f} | Val: {model.val_losses[-1]:.4f}\") <pre>\u00c9pocas treinadas: 200\nLoss final - Train: 0.1083 | Val: 0.1279\n</pre> In\u00a0[13]: Copied! <pre># Fun\u00e7\u00e3o para calcular m\u00e9tricas\ndef calc_metrics(y_true, y_pred):\n    y_true = y_true.flatten()\n    y_pred = y_pred.flatten()\n\n    mae = np.mean(np.abs(y_true - y_pred))\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n\n    ss_res = np.sum((y_true - y_pred) ** 2)\n    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n\n    # MAPE (evitar divis\u00e3o por zero)\n    mask = y_true != 0\n    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n\n    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n\n# Fazer predi\u00e7\u00f5es e desnormalizar\ny_pred_test_norm = model.predict(X_test)\ny_pred_test = y_pred_test_norm * y_std + y_mean\ny_true_test = y_test * y_std + y_mean\n\ny_pred_train_norm = model.predict(X_train)\ny_pred_train = y_pred_train_norm * y_std + y_mean\ny_true_train = y_train * y_std + y_mean\n\n# Calcular m\u00e9tricas\ntrain_metrics = calc_metrics(y_true_train, y_pred_train)\ntest_metrics = calc_metrics(y_true_test, y_pred_test)\n\n# Baseline (preditor de m\u00e9dia)\nbaseline_pred = np.full_like(y_true_test, y_true_train.mean())\nbaseline_metrics = calc_metrics(y_true_test, baseline_pred)\n\n# Exibir resultados\nprint(\"M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\")\nprint(\"=\"*70)\nprint(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15} {'Baseline':&lt;15}\")\nprint(\"-\"*70)\nprint(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f} {baseline_metrics['MAE']:&lt;15.2f}\")\nprint(f\"{'MSE':&lt;10} {train_metrics['MSE']:&lt;15.2f} {test_metrics['MSE']:&lt;15.2f} {baseline_metrics['MSE']:&lt;15.2f}\")\nprint(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f} {baseline_metrics['RMSE']:&lt;15.2f}\")\nprint(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f} {baseline_metrics['R2']:&lt;15.4f}\")\nprint(f\"{'MAPE (%)':&lt;10} {train_metrics['MAPE']:&lt;15.2f} {test_metrics['MAPE']:&lt;15.2f} {baseline_metrics['MAPE']:&lt;15.2f}\")\nprint(\"=\"*70)\n\nmelhoria = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100)\nprint(f\"\\nMelhoria sobre baseline: {melhoria:.1f}%\")\n</pre> # Fun\u00e7\u00e3o para calcular m\u00e9tricas def calc_metrics(y_true, y_pred):     y_true = y_true.flatten()     y_pred = y_pred.flatten()      mae = np.mean(np.abs(y_true - y_pred))     mse = np.mean((y_true - y_pred) ** 2)     rmse = np.sqrt(mse)      ss_res = np.sum((y_true - y_pred) ** 2)     ss_tot = np.sum((y_true - y_true.mean()) ** 2)     r2 = 1 - (ss_res / ss_tot)      # MAPE (evitar divis\u00e3o por zero)     mask = y_true != 0     mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100      return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}  # Fazer predi\u00e7\u00f5es e desnormalizar y_pred_test_norm = model.predict(X_test) y_pred_test = y_pred_test_norm * y_std + y_mean y_true_test = y_test * y_std + y_mean  y_pred_train_norm = model.predict(X_train) y_pred_train = y_pred_train_norm * y_std + y_mean y_true_train = y_train * y_std + y_mean  # Calcular m\u00e9tricas train_metrics = calc_metrics(y_true_train, y_pred_train) test_metrics = calc_metrics(y_true_test, y_pred_test)  # Baseline (preditor de m\u00e9dia) baseline_pred = np.full_like(y_true_test, y_true_train.mean()) baseline_metrics = calc_metrics(y_true_test, baseline_pred)  # Exibir resultados print(\"M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\") print(\"=\"*70) print(f\"{'M\u00e9trica':&lt;10} {'Train':&lt;15} {'Test':&lt;15} {'Baseline':&lt;15}\") print(\"-\"*70) print(f\"{'MAE':&lt;10} {train_metrics['MAE']:&lt;15.2f} {test_metrics['MAE']:&lt;15.2f} {baseline_metrics['MAE']:&lt;15.2f}\") print(f\"{'MSE':&lt;10} {train_metrics['MSE']:&lt;15.2f} {test_metrics['MSE']:&lt;15.2f} {baseline_metrics['MSE']:&lt;15.2f}\") print(f\"{'RMSE':&lt;10} {train_metrics['RMSE']:&lt;15.2f} {test_metrics['RMSE']:&lt;15.2f} {baseline_metrics['RMSE']:&lt;15.2f}\") print(f\"{'R\u00b2':&lt;10} {train_metrics['R2']:&lt;15.4f} {test_metrics['R2']:&lt;15.4f} {baseline_metrics['R2']:&lt;15.4f}\") print(f\"{'MAPE (%)':&lt;10} {train_metrics['MAPE']:&lt;15.2f} {test_metrics['MAPE']:&lt;15.2f} {baseline_metrics['MAPE']:&lt;15.2f}\") print(\"=\"*70)  melhoria = ((baseline_metrics['RMSE'] - test_metrics['RMSE']) / baseline_metrics['RMSE'] * 100) print(f\"\\nMelhoria sobre baseline: {melhoria:.1f}%\") <pre>M\u00c9TRICAS DE AVALIA\u00c7\u00c3O\n======================================================================\nM\u00e9trica    Train           Test            Baseline       \n----------------------------------------------------------------------\nMAE        42.95           44.83           143.29         \nMSE        3562.55         4024.64         33800.49       \nRMSE       59.69           63.44           183.85         \nR\u00b2         0.8894          0.8809          -0.0000        \nMAPE (%)   103.73          101.15          796.90         \n======================================================================\n\nMelhoria sobre baseline: 65.5%\n</pre> In\u00a0[14]: Copied! <pre># Visualiza\u00e7\u00f5es de avalia\u00e7\u00e3o\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Predito vs Real\naxes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=15)\nlim = [y_true_test.min(), y_true_test.max()]\naxes[0].plot(lim, lim, 'r--', linewidth=2, label='Predi\u00e7\u00e3o perfeita')\naxes[0].set_xlabel('Valor Real')\naxes[0].set_ylabel('Valor Predito')\naxes[0].set_title(f'Predi\u00e7\u00f5es vs Real\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Res\u00edduos\nresiduals = (y_true_test - y_pred_test).flatten()\naxes[1].scatter(y_pred_test, residuals, alpha=0.4, s=15)\naxes[1].axhline(0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Valor Predito')\naxes[1].set_ylabel('Res\u00edduo (Real - Predito)')\naxes[1].set_title('An\u00e1lise de Res\u00edduos')\naxes[1].grid(True, alpha=0.3)\n\n# Distribui\u00e7\u00e3o dos res\u00edduos\naxes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\naxes[2].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero')\naxes[2].axvline(residuals.mean(), color='g', linestyle='--', linewidth=2, label=f'M\u00e9dia: {residuals.mean():.1f}')\naxes[2].set_xlabel('Res\u00edduo')\naxes[2].set_ylabel('Frequ\u00eancia')\naxes[2].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Estat\u00edsticas dos res\u00edduos:\")\nprint(f\"  M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\")\nprint(f\"  Std: {residuals.std():.2f}\")\nprint(f\"  Mediana do erro absoluto: {np.median(np.abs(residuals)):.2f}\")\n</pre> # Visualiza\u00e7\u00f5es de avalia\u00e7\u00e3o fig, axes = plt.subplots(1, 3, figsize=(16, 5))  # Predito vs Real axes[0].scatter(y_true_test, y_pred_test, alpha=0.4, s=15) lim = [y_true_test.min(), y_true_test.max()] axes[0].plot(lim, lim, 'r--', linewidth=2, label='Predi\u00e7\u00e3o perfeita') axes[0].set_xlabel('Valor Real') axes[0].set_ylabel('Valor Predito') axes[0].set_title(f'Predi\u00e7\u00f5es vs Real\\nR\u00b2 = {test_metrics[\"R2\"]:.4f}') axes[0].legend() axes[0].grid(True, alpha=0.3)  # Res\u00edduos residuals = (y_true_test - y_pred_test).flatten() axes[1].scatter(y_pred_test, residuals, alpha=0.4, s=15) axes[1].axhline(0, color='r', linestyle='--', linewidth=2) axes[1].set_xlabel('Valor Predito') axes[1].set_ylabel('Res\u00edduo (Real - Predito)') axes[1].set_title('An\u00e1lise de Res\u00edduos') axes[1].grid(True, alpha=0.3)  # Distribui\u00e7\u00e3o dos res\u00edduos axes[2].hist(residuals, bins=50, edgecolor='black', alpha=0.7) axes[2].axvline(0, color='r', linestyle='--', linewidth=2, label='Zero') axes[2].axvline(residuals.mean(), color='g', linestyle='--', linewidth=2, label=f'M\u00e9dia: {residuals.mean():.1f}') axes[2].set_xlabel('Res\u00edduo') axes[2].set_ylabel('Frequ\u00eancia') axes[2].set_title('Distribui\u00e7\u00e3o dos Res\u00edduos') axes[2].legend() axes[2].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"Estat\u00edsticas dos res\u00edduos:\") print(f\"  M\u00e9dia: {residuals.mean():.2f} (ideal: ~0)\") print(f\"  Std: {residuals.std():.2f}\") print(f\"  Mediana do erro absoluto: {np.median(np.abs(residuals)):.2f}\") <pre>Estat\u00edsticas dos res\u00edduos:\n  M\u00e9dia: -1.37 (ideal: ~0)\n  Std: 63.43\n  Mediana do erro absoluto: 31.89\n</pre>"},{"location":"portfolio/neural-networks/projects/2/regression/#projeto-de-regressao-bike-sharing-demand","title":"Projeto de Regress\u00e3o: Bike Sharing Demand\u00b6","text":"<p>Aluno: Pedro Civita Disciplina: Redes Neurais e Deep Learning Dataset: Bike Sharing (UCI ML Repository)</p>"},{"location":"portfolio/neural-networks/projects/2/regression/#objetivo","title":"Objetivo\u00b6","text":"<p>Implementar um Multi-Layer Perceptron (MLP) para prever a demanda de bicicletas compartilhadas com base em condi\u00e7\u00f5es clim\u00e1ticas e temporais.</p>"},{"location":"portfolio/neural-networks/projects/2/regression/#1-selecao-do-dataset","title":"1. Sele\u00e7\u00e3o do Dataset\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/regression/#dataset-bike-sharing-demand","title":"Dataset: Bike Sharing Demand\u00b6","text":"<p>Fonte: UCI ML Repository</p> <p>Caracter\u00edsticas:</p> <ul> <li>Amostras: 17,379 registros (hor\u00e1rios de 2011-2012)</li> <li>Features: 16 vari\u00e1veis (clima, tempo, sazonalidade)</li> <li>Target: <code>cnt</code> - contagem total de alugu\u00e9is (regress\u00e3o)</li> </ul> <p>Motiva\u00e7\u00e3o:</p> <ul> <li>Problema real de otimiza\u00e7\u00e3o urbana</li> <li>Dataset n\u00e3o-trivial com padr\u00f5es temporais complexos</li> <li>Evita datasets cl\u00e1ssicos (Boston/California Housing)</li> <li>Dispon\u00edvel em competi\u00e7\u00e3o Kaggle (possibilidade de bonus)</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#2-analise-exploratoria","title":"2. An\u00e1lise Explorat\u00f3ria\u00b6","text":"<p>An\u00e1lise inicial para entender padr\u00f5es e rela\u00e7\u00f5es nos dados.</p>"},{"location":"portfolio/neural-networks/projects/2/regression/#3-limpeza-e-normalizacao-dos-dados","title":"3. Limpeza e Normaliza\u00e7\u00e3o dos Dados\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/regression/#estrategia-de-pre-processamento","title":"Estrat\u00e9gia de Pr\u00e9-processamento:\u00b6","text":"<ol> <li>Remo\u00e7\u00e3o de features: <code>instant</code>, <code>dteday</code>, <code>casual</code>, <code>registered</code> (evitar data leakage)</li> <li>Feature engineering: Transforma\u00e7\u00e3o c\u00edclica para vari\u00e1veis temporais (sin/cos)</li> <li>Normaliza\u00e7\u00e3o: Z-score standardization (m\u00e9dia=0, std=1)</li> </ol>"},{"location":"portfolio/neural-networks/projects/2/regression/#4-estrategia-de-divisao-trainvalidationtest","title":"4. Estrat\u00e9gia de Divis\u00e3o Train/Validation/Test\u00b6","text":"<p>Divis\u00e3o: 70% treino / 15% valida\u00e7\u00e3o / 15% teste</p> <p>Modo de treinamento: Mini-batch gradient descent (batch_size=64)</p> <p>Justificativa:</p> <ul> <li>70% treino: volume suficiente para aprendizado</li> <li>15% valida\u00e7\u00e3o: monitorar overfitting e early stopping</li> <li>15% teste: avalia\u00e7\u00e3o final imparcial</li> <li>Mini-batch: equil\u00edbrio entre velocidade e estabilidade</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#5-implementacao-do-mlp","title":"5. Implementa\u00e7\u00e3o do MLP\u00b6","text":"<p>Arquitetura: Input(15) \u2192 Hidden(64, ReLU) \u2192 Hidden(32, ReLU) \u2192 Hidden(16, ReLU) \u2192 Output(1, Linear)</p> <p>Componentes:</p> <ul> <li>Inicializa\u00e7\u00e3o: He initialization</li> <li>Ativa\u00e7\u00e3o: ReLU (camadas ocultas), Linear (sa\u00edda)</li> <li>Loss: MSE + Regulariza\u00e7\u00e3o L2</li> <li>Otimiza\u00e7\u00e3o: Mini-batch gradient descent</li> <li>Regulariza\u00e7\u00e3o: Early stopping + L2</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#6-treinamento-do-modelo","title":"6. Treinamento do Modelo\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/regression/#7-curvas-de-erro-e-visualizacoes","title":"7. Curvas de Erro e Visualiza\u00e7\u00f5es\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/regression/#8-metricas-de-avaliacao","title":"8. M\u00e9tricas de Avalia\u00e7\u00e3o\u00b6","text":"<p>M\u00e9tricas obrigat\u00f3rias para regress\u00e3o: MAE, MAPE, MSE, RMSE, R\u00b2</p>"},{"location":"portfolio/neural-networks/projects/2/regression/#9-conclusao","title":"9. Conclus\u00e3o\u00b6","text":""},{"location":"portfolio/neural-networks/projects/2/regression/#resultados","title":"Resultados:\u00b6","text":"<ul> <li>R\u00b2 = {:.4f}: Modelo explica ~{:.1f}% da vari\u00e2ncia</li> <li>RMSE = {:.2f}: Erro m\u00e9dio de ~{:.0f} alugu\u00e9is</li> <li>Melhoria sobre baseline: {:.1f}%</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#limitacoes","title":"Limita\u00e7\u00f5es:\u00b6","text":"<ul> <li>MLP n\u00e3o captura depend\u00eancias temporais de longo prazo</li> <li>Features podem ser expandidas (lags, intera\u00e7\u00f5es)</li> <li>Hiperpar\u00e2metros n\u00e3o foram otimizados sistematicamente</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#melhorias-futuras","title":"Melhorias Futuras:\u00b6","text":"<ul> <li>Arquiteturas recorrentes (LSTM/GRU) para s\u00e9ries temporais</li> <li>Feature engineering mais sofisticado</li> <li>Ensemble com outros modelos (XGBoost, Random Forest)</li> <li>Grid search para otimiza\u00e7\u00e3o de hiperpar\u00e2metros</li> </ul>"},{"location":"portfolio/neural-networks/projects/2/regression/#ferramentas-de-ia","title":"Ferramentas de IA:\u00b6","text":"<p>Claude Code (Anthropic) foi usado para auxiliar na estrutura\u00e7\u00e3o do c\u00f3digo. Todo o c\u00f3digo foi compreendido e validado manualmente.</p>"},{"location":"portfolio/neural-networks/projects/2/regression/#referencias","title":"Refer\u00eancias:\u00b6","text":"<ul> <li>Dataset: Fanaee-T &amp; Gama (2013), UCI ML Repository</li> <li>Implementa\u00e7\u00e3o: NumPy, conceitos de Goodfellow et al. (Deep Learning, 2016)</li> <li>Material do curso: https://insper.github.io/ann-dl/</li> </ul>"},{"location":"portfolio/neural-networks/projects/3/","title":"3. Generative Models","text":""},{"location":"portfolio/neural-networks/projects/3/#project-3","title":"Project 3","text":"<p>Description coming soon.</p>"},{"location":"portfolio/neural-networks/projects/3/main/","title":"Project 3","text":""},{"location":"portfolio/neural-networks/projects/3/main/#project-3","title":"Project 3","text":"<p>Description coming soon.</p>"},{"location":"portfolio/neural-networks/thisdocumentation/main/","title":"Main","text":""},{"location":"portfolio/neural-networks/thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"portfolio/neural-networks/thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"portfolio/neural-networks/thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"portfolio/neural-networks/thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}